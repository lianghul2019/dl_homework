{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Processing Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import essential libraries\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Libraries for NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda\n",
    "\n",
    "# questions: how to load data from data source & how to use cuda? & how to use instance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading data from data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all the files now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# read zip files\n",
    "from zipfile import ZipFile\n",
    "file_name = \"11-785-s20-hw1p2.zip\"\n",
    "\n",
    "# opening the zip file in READ mode \n",
    "with ZipFile(file_name, 'r') as zp: \n",
    "    # extracting all the files \n",
    "    print('Extracting all the files now...') \n",
    "    zp.extractall() \n",
    "    print('Done!') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dev.npy', 'dev_labels.npy', 'hw1p2_sample_submission.csv', 'test.npy', 'train.npy', 'train_labels.npy']\n"
     ]
    }
   ],
   "source": [
    "# check zipfile object\n",
    "print(zp.namelist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all data from files\n",
    "dev = np.load(\"dev.npy\")\n",
    "dev_labels = np.load(\"dev_labels.npy\")\n",
    "\n",
    "test = np.load(\"test.npy\")\n",
    "\n",
    "train = np.load(\"train.npy\")\n",
    "train_labels = np.load(\"train_labels.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training samples:  (24500,)\n",
      "The frames in utterance 1:  (477, 40)\n"
     ]
    }
   ],
   "source": [
    "# print out and check the shape of training set\n",
    "print(\"The number of training samples: \", train.shape)\n",
    "print(\"The frames in utterance 1: \",train[0].shape)\n",
    "train_temp = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creat Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, data, labels, context_len, train_mode = True):\n",
    "        self.labels = labels\n",
    "        self.context_len = context_len\n",
    "        self.data = data\n",
    "        # padding all the utterance\n",
    "        self.length_list = [0]\n",
    "        count = 0\n",
    "        self.train_mode = train_mode\n",
    "        for i in range(data.shape[0]):\n",
    "            count += self.data[i].shape[0]\n",
    "            self.data[i] = np.pad(self.data[i],((context_len,context_len),(0,0)), 'constant', constant_values=0)\n",
    "            self.length_list.append(count)\n",
    "    def __len__(self):\n",
    "        # len should be total frames\n",
    "        return self.length_list[-1]\n",
    "\n",
    "    def binary_search(self, target):\n",
    "        low = 0\n",
    "        high = len(self.length_list) - 1\n",
    "        while (low < high - 1):\n",
    "            mid = (low + high) // 2\n",
    "            if target > self.length_list[mid]:\n",
    "                low = mid\n",
    "            elif target < self.length_list[mid]:\n",
    "                high = mid\n",
    "            elif target == self.length_list[mid]:\n",
    "                # corner case\n",
    "                low = mid\n",
    "                break\n",
    "        # low -- u_idx\n",
    "        # target - f_idx\n",
    "        return low, target - self.length_list[low]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # convert it to a regular python int.\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # how to get the utterance binary search\n",
    "            frames = np.zeros((len(idx), (2 * self.context_len + 1) * 40))\n",
    "            if self.train_mode:\n",
    "                labels = np.zeros(len(idx))\n",
    "    \n",
    "            # idx should list, np.array\n",
    "            for i in range(len(idx)):\n",
    "                u_idx, f_idx = self.binary_search(idx[i])\n",
    "\n",
    "                # change frames and labels\n",
    "                frames[i] = self.data[u_idx][f_idx:f_idx + 2 * self.context_len + 1].reshape(1, -1)\n",
    "                if self.train_mode:\n",
    "                    labels[i] = self.labels[u_idx][f_idx]\n",
    "        else:\n",
    "            u_idx, f_idx = self.binary_search(idx)\n",
    "            \n",
    "            # change frames and labels\n",
    "            frames = self.data[u_idx][f_idx:f_idx + 2 * self.context_len + 1].reshape(1, -1).squeeze()\n",
    "            if self.train_mode:\n",
    "                labels = self.labels[u_idx][f_idx]\n",
    "            else:\n",
    "                labels = []\n",
    "        # return tensor\n",
    "        return {\"frames\": frames, \"labels\": labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataLoader\n",
    "PyTorch provides convenient wrappers in torch.utils.data to load, shuffle, batch, and iterate over data. You should use it whenever possible because it has several advantages:  \n",
    "1. Bulit-in logic for shuffling and batching. (Don't reinvent wheels.)  \n",
    "2. Asynchronous data loading. (Load and preprocess data for the next batch while training the current batch.)\n",
    "3. On-the-fly data preprocessing. (No need to store the whole preprocessed dataset.)    \n",
    "\n",
    "The basic usage involves two classes: Dataset and Dataloader.  \n",
    "1. The abstract class Dataset represents a dataset whose samples can be randomly accessed via its __getitem__ method. We can use one of the provided implementation like TensorDataset, or inherit Dataset ourselves and implement loading and preprocessing in __getitem__.  \n",
    "2. Dataloader turns a Dataset into an iterator of mini-batches, handling shuffling, batching and multiprocessing under the hood.  \n",
    "Refer to the documentation for more usage and details: https://pytorch.org/docs/stable/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1, read and load the data, use Dataset() function\n",
    "\n",
    "# step 2, create classes to preprocess the data, or cat the data\n",
    "\n",
    "# step 3, use DataLoader to shuffle, customize batch size\n",
    "\n",
    "# step 4, once training, use DataLoader object (iterator)\n",
    "## looping with two for loops, for epoch & for i in dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construct the Network and Define the model\n",
    "define the network object\n",
    "\n",
    "To reach the baseline, we use following steps:\n",
    "Baseline model:\n",
    "* Layers -> [input_size, 2048, 1024, output_size]\n",
    "* ReLU activations\n",
    "* Context size k = 12 frames on both sides\n",
    "* Adam optimizer, with the default learning rate 1e-3\n",
    "* Zero padding of k frames on both sides of each utterance\n",
    "This should get you to around the cutoffs for B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note: How much time would the model take to run an epoc\n",
    "Depends on network size and context and batch size. Could vary from a few minutes to 20 mins.\n",
    "I would advise that if an epoch is taking you an hour on training data or something similar, there is something wrong with the way you are making batches (dataset, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Description\n",
    "The utterance, context and the sliding window. The sliding window will help differentiate the phoe in a utterance\n",
    "but considering the fact that the length of the sliding window, we also want to create the context for the model.\n",
    "the dimension of data (3, Y_x , 40 ) -- means there are three utterance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "context_len = 15\n",
    "input_size = (2 * context_len + 1) * 40\n",
    "hiddens = [2048, 1024, 512, 256, 256]\n",
    "output_size  = 138\n",
    "\n",
    "# Load data into pre-defined datasets.\n",
    "# Training Data\n",
    "train_dataset = Dataset(train, train_labels, context_len)\n",
    "\n",
    "# Dev Data\n",
    "dev_dataset = Dataset(dev, dev_labels, context_len)\n",
    "\n",
    "# Test Data\n",
    "test_dataset = Dataset(test, None, context_len, train_mode = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(507, 40)\n",
      "[-377.28824994 -297.82954848 -329.87172842 ...    0.            0.\n",
      "    0.        ]\n",
      "110\n",
      "(18, 368)\n"
     ]
    }
   ],
   "source": [
    "print(train[0].shape)\n",
    "print((train_dataset[15388712]['frames']))\n",
    "print((train_dataset[15388712]['labels']))\n",
    "print(train_dataset.binary_search(9001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train the model for several epochs, and validate performance on validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model and optimizer, criterion for training\n",
    "# define the network\n",
    "# model = MLP(input_size, output_size, hiddens)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr = 0.001)\n",
    "# device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, data1 in enumerate(test_loader): \n",
    "            \n",
    "            data = data1['frames'].float().to(device)\n",
    "            target = data1['labels'].to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frames': array([-575.51613546, -390.27229869, -449.75549712, ...,  168.07380721,\n",
       "         140.17706286,  177.80356023]), 'labels': 24}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, train_dataset, criterion, optimizer, batch_size):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # change of the learning rate\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
    "    \n",
    "    # print current lr\n",
    "    print(\"Current lr is : \" , optimizer.param_groups[0]['lr'])\n",
    "    for data1 in train_loader:   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data1[\"frames\"].float().to(device)\n",
    "        target = data1[\"labels\"].to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # backward is only applicable after the forward \n",
    "        # call, so if you want to call backward again, \n",
    "        # it should follow the forward call.\n",
    "        loss.backward()\n",
    "        # optimizer step update params after backward\n",
    "        # based on current gradient, and that is why we\n",
    "        # should call optimizer.zero_grad() each time\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    return model, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# # MLP model construction\n",
    "class MLP1(nn.Module):\n",
    "    def __init__(self,input_size, output_size, hiddens):\n",
    "        super(MLP1, self).__init__()\n",
    "        # remember to add the hiddens into nn.ModuleList()\n",
    "        # it depends on the way you define a network\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size,hiddens[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddens[0],hiddens[1]),\n",
    "            nn.BatchNorm1d(num_features = hiddens[1], momentum = 0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddens[1],hiddens[2]),\n",
    "            nn.BatchNorm1d(num_features = hiddens[2], momentum = 0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddens[2],hiddens[3]),\n",
    "            nn.BatchNorm1d(num_features = hiddens[3], momentum = 0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddens[3],hiddens[4]),\n",
    "            nn.BatchNorm1d(num_features = hiddens[4], momentum = 0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddens[4],hiddens[5]),\n",
    "            nn.BatchNorm1d(num_features = hiddens[5], momentum = 0.1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddens[5],output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self,X):\n",
    "#         for i in self.layers:\n",
    "#             print(\"X shape is :\", X.shape)\n",
    "        X = self.layers(X)\n",
    "        self.output = X\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP1(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=1240, out_features=2048, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "    (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (9): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (12): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (15): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): Linear(in_features=256, out_features=138, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Init the model to find good training method\n",
    "input_size = 1240\n",
    "output_size = 138\n",
    "hiddens = [2048,1024,512,512,256,256]\n",
    "model1 = MLP1(input_size, output_size, hiddens)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters())\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model1.to(device)\n",
    "print(model1)\n",
    "\n",
    "# Model settings\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "batch_size = 256\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr is :  0.001\n",
      "Training Loss:  2.0475332946997704 Time:  521.7506194114685 s\n",
      "Testing Loss:  1.7618870750295392\n",
      "Testing Accuracy:  52.10539243248362 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.7391971089971494 Time:  521.7650563716888 s\n",
      "Testing Loss:  1.6719749860691302\n",
      "Testing Accuracy:  54.379760770364406 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.6417104922991188 Time:  523.4633958339691 s\n",
      "Testing Loss:  1.57223868162343\n",
      "Testing Accuracy:  56.93422664670127 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.5834612108368804 Time:  528.5029680728912 s\n",
      "Testing Loss:  1.5012852238208958\n",
      "Testing Accuracy:  58.700631514154324 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.5434074055687859 Time:  524.707864522934 s\n",
      "Testing Loss:  1.4854004507940828\n",
      "Testing Accuracy:  59.14008724009967 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.5132051880607575 Time:  522.5344154834747 s\n",
      "Testing Loss:  1.4607314886694605\n",
      "Testing Accuracy:  59.741416556679425 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.4886671865995613 Time:  528.1746144294739 s\n",
      "Testing Loss:  1.4630095529059568\n",
      "Testing Accuracy:  59.70605294775656 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.4685857611558573 Time:  521.4747996330261 s\n",
      "Testing Loss:  1.4448857439964107\n",
      "Testing Accuracy:  60.142549375884094 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.4512635719325397 Time:  524.8813092708588 s\n",
      "Testing Loss:  1.4302465295701317\n",
      "Testing Accuracy:  60.55522345657822 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.4364888274933199 Time:  524.2276136875153 s\n",
      "Testing Loss:  1.4275150024868322\n",
      "Testing Accuracy:  60.52918163578146 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.4232155528365291 Time:  523.3292183876038 s\n",
      "Testing Loss:  1.4061363661492412\n",
      "Testing Accuracy:  61.271225563598264 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.4112021356417999 Time:  524.9433214664459 s\n",
      "Testing Loss:  1.399392163956707\n",
      "Testing Accuracy:  61.47512118324564 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.4003715900619023 Time:  535.5698335170746 s\n",
      "Testing Loss:  1.398903246381969\n",
      "Testing Accuracy:  61.472753744991394 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3907490113736063 Time:  524.1792900562286 s\n",
      "Testing Loss:  1.4013469375669956\n",
      "Testing Accuracy:  61.25184216289159 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3816717816103563 Time:  522.3864579200745 s\n",
      "Testing Loss:  1.389397066346172\n",
      "Testing Accuracy:  61.647648246024175 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3735070752805625 Time:  522.9591083526611 s\n",
      "Testing Loss:  1.387334863130342\n",
      "Testing Accuracy:  61.87270284506893 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3657475646486135 Time:  521.1322355270386 s\n",
      "Testing Loss:  1.3815995730346802\n",
      "Testing Accuracy:  61.96015009558532 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3585186098686557 Time:  521.6449861526489 s\n",
      "Testing Loss:  1.376447540137804\n",
      "Testing Accuracy:  62.15043294527075 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.351734153434691 Time:  521.0690910816193 s\n",
      "Testing Loss:  1.377257039770484\n",
      "Testing Accuracy:  62.1749951171586 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.34537799300204 Time:  522.5034513473511 s\n",
      "Testing Loss:  1.366493466011051\n",
      "Testing Accuracy:  62.46707781177682 %\n"
     ]
    }
   ],
   "source": [
    "# Start the training process and inspect changes of parmas\n",
    "for i in range(20):\n",
    "\n",
    "    model1,_ = train_epoch(model1, train_dataset, criterion, optimizer, batch_size)\n",
    "    # evaluate the model\n",
    "    eval_loss,_  = test_model(model1,dev_loader, criterion)\n",
    "    scheduler.step(eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "{'factor': 0.1, 'min_lrs': [0], 'patience': 10, 'verbose': False, 'cooldown': 0, 'cooldown_counter': 0, 'mode': 'min', 'threshold': 0.0001, 'threshold_mode': 'rel', 'best': 1.366493466011051, 'num_bad_epochs': 0, 'mode_worse': inf, 'eps': 1e-08, 'last_epoch': 19}\n"
     ]
    }
   ],
   "source": [
    "# inspect training params\n",
    "print(optimizer.param_groups[0]['lr'])\n",
    "print(scheduler.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr is :  0.001\n",
      "Training Loss:  1.3392689984417037 Time:  521.8033604621887 s\n",
      "Testing Loss:  1.366871906884692\n",
      "Testing Accuracy:  62.44488307814322 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3337235155165559 Time:  521.2782769203186 s\n",
      "Testing Loss:  1.3699928627772764\n",
      "Testing Accuracy:  62.337608532247465 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3281757631321394 Time:  525.1525976657867 s\n",
      "Testing Loss:  1.362134625390172\n",
      "Testing Accuracy:  62.66949378251528 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3227056846645544 Time:  528.6545436382294 s\n",
      "Testing Loss:  1.3644574579528788\n",
      "Testing Accuracy:  62.45257725246953 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.317948128487257 Time:  532.6425511837006 s\n",
      "Testing Loss:  1.3692855260588905\n",
      "Testing Accuracy:  62.35447652980901 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3133519661624258 Time:  522.7350435256958 s\n",
      "Testing Loss:  1.3598318353746877\n",
      "Testing Accuracy:  62.66949378251528 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.308750661598647 Time:  521.6880841255188 s\n",
      "Testing Loss:  1.3611853859076897\n",
      "Testing Accuracy:  62.62880343752034 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3042744945439073 Time:  523.1481840610504 s\n",
      "Testing Loss:  1.355149413667845\n",
      "Testing Accuracy:  62.86909842032683 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.3001059190904074 Time:  524.5158016681671 s\n",
      "Testing Loss:  1.362150047471126\n",
      "Testing Accuracy:  62.67585627282358 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.2962299014510907 Time:  525.1082420349121 s\n",
      "Testing Loss:  1.3555970501380437\n",
      "Testing Accuracy:  62.93079977982824 %\n"
     ]
    }
   ],
   "source": [
    "# Continue training to get better performance\n",
    "for i in range(10):\n",
    "    #################### change the learning rate rule. #####################\n",
    "    model1,_ = train_epoch(model1, train_dataset, criterion, optimizer, batch_size)\n",
    "    ########################################################################\n",
    "    # evaluate the model\n",
    "    eval_loss,_  = test_model(model1,dev_loader, criterion)\n",
    "    scheduler.step(eval_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr is :  0.001\n",
      "Training Loss:  1.2924058641059992 Time:  522.4022789001465 s\n",
      "Testing Loss:  1.3546084003014998\n",
      "Testing Accuracy:  62.88596641788836 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.2883698746579109 Time:  523.164671421051 s\n",
      "Testing Loss:  1.3575674547277616\n",
      "Testing Accuracy:  62.83580631987642 %\n",
      "Current lr is :  0.001\n",
      "Training Loss:  1.2849656797348963 Time:  524.7252948284149 s\n",
      "Testing Loss:  1.355825785028212\n",
      "Testing Accuracy:  62.90638557283128 %\n"
     ]
    }
   ],
   "source": [
    "# Continue training...\n",
    "for i in range(3):\n",
    "    #################### change the learning rate rule. #####################\n",
    "    model1,_ = train_epoch(model1, train_dataset, criterion, optimizer, batch_size)\n",
    "    ########################################################################\n",
    "    # evaluate the model\n",
    "    eval_loss,_  = test_model(model1,dev_loader, criterion)\n",
    "    scheduler.step(eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lr is :  0.0001\n",
      "Training Loss:  1.2112064915954062 Time:  526.798223733902 s\n",
      "Testing Loss:  1.3167077033582961\n",
      "Testing Accuracy:  64.0867015074663 %\n"
     ]
    }
   ],
   "source": [
    "# Change LR manually to get better performance\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.0001)\n",
    "model1,_ = train_epoch(model1, train_dataset, criterion, optimizer, batch_size)\n",
    "\n",
    "##############################ZZ##########################################\n",
    "# evaluate the model\n",
    "eval_loss,_  = test_model(model1,dev_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model dicts for future learning\n",
    "torch.save(model1.state_dict(), 'mode1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: load and resume learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = MLP1(input_size,output_size, hiddens)\n",
    "# model1.load_state_dict(torch.load('mode1.pth'))\n",
    "# model1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dev Dataset\n",
    "\n",
    "After defining all the procedures, then in this part, generate labels for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.load(\"test.npy\")\n",
    "test_dataset = Dataset(test, None, context_len,  False)\n",
    "# makeing predictions and construct labels and stored results into files\n",
    "import pandas as pd\n",
    "\n",
    "def predict(model, test_dataset, filename = None):\n",
    "#     model.eval()\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size)\n",
    "    y_predicts = []\n",
    "    for data1 in test_loader:  \n",
    "#         print(data1)\n",
    "        data = data1['frames'].float().to(device)\n",
    "\n",
    "        outputs = model(data)\n",
    "\n",
    "        # make predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_predicts += predicted.tolist()\n",
    "            \n",
    "    # if filename, then output the predicted labels\n",
    "    if filename:\n",
    "        # before writing files, put onto cpu\n",
    "        pred = np.array(y_predicts)\n",
    "        columns = np.arange(len(pred))\n",
    "        # write file\n",
    "        df = pd.DataFrame({\"id\" :columns, \"label\" : pred})\n",
    "        df.to_csv(filename, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the result...\n",
    "predicted = predict(model1, test_dataset, filename=\"submission1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
