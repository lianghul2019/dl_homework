{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "hw2p2_version_with_data_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcEPSIta5A4N",
        "colab_type": "text"
      },
      "source": [
        "# Recitation - 6\n",
        "___\n",
        "\n",
        "* Custom Dataset & DataLoader\n",
        "* Torchvision ImageFolder Dataset\n",
        "* Residual Block\n",
        "* CNN model with Residual Block\n",
        "* Loss Functions (Center Loss and Triplet Loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ielKcATp5A4Q",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1IUD3G05A4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision import  transforms\n",
        "import torch\n",
        "import torchvision   \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrdv1G925A4X",
        "colab_type": "text"
      },
      "source": [
        "## Custom DataSet with DataLoader\n",
        "___\n",
        "We have used a subset of the data given for the Face Classification and Verification problem in Part 2 of the homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_8L60jm5A4b",
        "colab_type": "text"
      },
      "source": [
        "#### Parse the given directory to accumulate all the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvxQ4ystKBhi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0b6ee19f-9f46-4f2c-e2a0-d4399e3bcc50"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGUUv8JmKiGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read data from my drive\n",
        "\n",
        "\n",
        "with ZipFile(\"/content/drive/My Drive/11-785hw2p2-s20/test_classification.zip\") as file1:\n",
        "    file1.extractall()\n",
        "\n",
        "with ZipFile(\"/content/drive/My Drive/11-785hw2p2-s20/test_verification.zip\") as file2:\n",
        "    file2.extractall()\n",
        "\n",
        "with ZipFile(\"/content/drive/My Drive/11-785hw2p2-s20/validation_verification.zip\") as file3:\n",
        "    file3.extractall()\n",
        "\n",
        "with ZipFile(\"/content/drive/My Drive/11-785hw2p2-s20/validation_classification.zip\") as file4:\n",
        "    file4.extractall()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6eTiLFkN3GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with ZipFile(\"/content/drive/My Drive/11-785hw2p2-s20/train_data.zip\") as file:\n",
        "    file.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEo3DCVq5A40",
        "colab_type": "text"
      },
      "source": [
        "## Torchvision DataSet and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VyLcH6R5A41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading data\n",
        "train_dataTransform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    #transforms.RandomRotation(10),\n",
        "    #transforms.RandomAffine(5),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "val_dataTransform = torchvision.transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(root='/content/train_data/medium/', \n",
        "                                                       transform=train_dataTransform)\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(root='/content/test_classification/', \n",
        "                                                       transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "val_dataset = torchvision.datasets.ImageFolder(root='/content/validation_classification/medium/', \n",
        "                                                       transform=val_dataTransform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfEwoAJs5A44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
        "validation_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfpmSAH55A47",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1e6d18d4-3d3f-47dc-db4d-accd3fe68225"
      },
      "source": [
        "print(train_loader.__len__(), len(train_dataset.classes))\n",
        "validation_loader.__len__(), len(val_dataset.classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25693 2300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(144, 2300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahw2yrk85A49",
        "colab_type": "text"
      },
      "source": [
        "## Residual Block\n",
        "\n",
        "Resnet: https://arxiv.org/pdf/1512.03385.pdf\n",
        "\n",
        "Here is a basic usage of shortcut in Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HEgbFPy5A4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BottleNeck(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channel, first_two_channel, last_channel, stride_list= [1,1,1]):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        self.first_two_channel = first_two_channel\n",
        "        self.last_channel = last_channel\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channel, first_two_channel, kernel_size=1, stride=stride_list[0], padding=0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(first_two_channel)\n",
        "        self.activation = nn.ReLU(first_two_channel)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(first_two_channel, first_two_channel, kernel_size=3, stride=stride_list[1], padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(first_two_channel)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(first_two_channel, last_channel, kernel_size =1, stride = stride_list[2], padding=0, bias= False)\n",
        "        self.bn3 = nn.BatchNorm2d(last_channel)\n",
        "\n",
        "        self.shortcut = nn.Conv2d(in_channel, in_channel, kernel_size=1, stride=stride_list[0],bias=False)\n",
        "        self.bn = nn.BatchNorm2d(last_channel)\n",
        "\n",
        "        self.layers = [self.conv1, self.bn1,self.activation, self.conv2, self.bn2,self.activation, self.conv3, self.bn3]\n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        out = self.layers(x)\n",
        "        shortcut = self.shortcut(x)\n",
        "        size = shortcut.size()\n",
        "        self.padding = torch.zeros(size[0],self.last_channel - size[1], size[2],size[3]).to(device)\n",
        "        shortcut = torch.cat((shortcut, self.padding),1).to(device)\n",
        "        shortcut = self.bn(shortcut)\n",
        "        # self.shortcut = F.batch_norm(self.shortcut)\n",
        "        # check the size of shortcut\n",
        "        out += shortcut\n",
        "\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqmGgPvL5A5A",
        "colab_type": "text"
      },
      "source": [
        "## CNN Model with Residual Block "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCkodQy75A5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, num_feats, hidden_sizes1, hidden_sizes2, reps, num_classes, feat_dim=1000):\n",
        "        super(Network, self).__init__()\n",
        "        # self.layers = []\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_feats,64*4 ,kernel_size=3,stride=1,padding=1, bias=False)\n",
        "        # self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
        "        self.layers = [self.conv1]\n",
        "        for j in range(3):\n",
        "            # append the network according to different situation\n",
        "            # j = 0 represents that is the first block\n",
        "            if j == 0:\n",
        "                self.layers.append(BottleNeck(64*4, 64, 64*4))   \n",
        "            else:\n",
        "                self.layers.append(BottleNeck(64*4, 64, 64*4))   \n",
        "        for i in range(len(reps)):\n",
        "            for j in range(reps[i]):\n",
        "                # append the network according to different situation\n",
        "                # j = 0 represents that is the first block\n",
        "                if i == 0 and j == 0:\n",
        "                    print(\"i == 0 and j == 0\")\n",
        "                    self.layers.append(BottleNeck(64*4, hidden_sizes1[i], hidden_sizes2[i],stride_list=[2,1,1]))\n",
        "                elif (i != 0 and j == 0):\n",
        "                    print(\"i != 0 and j == 0\" )\n",
        "                    self.layers.append(BottleNeck(hidden_sizes2[i-1],hidden_sizes1[i], hidden_sizes2[i], stride_list=[2,1,1]))\n",
        "                elif  j != 0:\n",
        "                    print(\"i != 0 and j != 0\")\n",
        "                    self.layers.append(BottleNeck(hidden_sizes2[i], hidden_sizes1[i], hidden_sizes2[i]))\n",
        "\n",
        "        # self.conv2d = nn.Conv2d(hidden_sizes2[-1], hidden_sizes2[-1], kernel_size=2)\n",
        "        # self.layers.append(self.conv2d)\n",
        "        self.layers = nn.Sequential(*self.layers)\n",
        "        # self.last_bn = nn.BatchNorm2d(hidden_sizes2[-1])\n",
        "        # print out the shape\n",
        "        self.linear_label = nn.Linear(hidden_sizes2[-1], num_classes, bias=False)\n",
        "        # For creating the embedding to be passed into the Center Loss criterion\n",
        "        self.linear_closs = nn.Linear(hidden_sizes2[-1], feat_dim, bias=False)\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.relu_closs = nn.ReLU(inplace=True)\n",
        "    \n",
        "    def forward(self, x, evalMode=False):\n",
        "        output = x\n",
        "        output = self.layers(output)       \n",
        "        # output = self.last_bn(output)\n",
        "        # output = self.relu_closs(output)\n",
        "\n",
        "        output = F.avg_pool2d(output,4)\n",
        "        output = output.reshape(output.shape[0], output.shape[1])\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        label_output = self.linear_label(output)\n",
        "        label_output = label_output/torch.norm(self.linear_label.weight, dim=1)\n",
        "        \n",
        "        # Create the feature embedding for the Center Loss\n",
        "        closs_output = self.linear_closs(output)\n",
        "        closs_output = self.relu_closs(closs_output)\n",
        "\n",
        "        return closs_output, label_output\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enlAsRud5A5C",
        "colab_type": "text"
      },
      "source": [
        "### Training & Testing Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWhr-NdQ5A5F",
        "colab_type": "text"
      },
      "source": [
        "#### Dataset, DataLoader and Constant Declarations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xENDCp55A5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adb76453-d2ab-4114-98bf-8a0dbf8120dc"
      },
      "source": [
        "numEpochs = 50\n",
        "print(\"Num of epochs is: \", numEpochs)\n",
        "num_feats = 3\n",
        "\n",
        "learningRate = 1e-2\n",
        "weightDecay = 5e-5\n",
        "\n",
        "##############  modify this  ####################\n",
        "# hidden_sizes = [32, 64, 128, 128, 128, 256]\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n",
        "#################################################\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num of epochs is:  50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apWnMBYfsSx3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1bad88a-aabe-4e6f-80ae-6f829fb3efb4"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VliBjYtY5A5N",
        "colab_type": "text"
      },
      "source": [
        "## Center Loss\n",
        "___\n",
        "The following piece of code for Center Loss has been pulled and modified based on the code from the GitHub Repo: https://github.com/KaiyangZhou/pytorch-center-loss\n",
        "    \n",
        "<b>Reference:</b>\n",
        "<i>Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.</i>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SweYBND15A5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CenterLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        num_classes (int): number of classes.\n",
        "        feat_dim (int): feature dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, feat_dim, device=torch.device('cpu')):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_dim = feat_dim\n",
        "        self.device = device\n",
        "        \n",
        "        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).to(self.device))\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\n",
        "            labels: ground truth labels with shape (batch_size).\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
        "        distmat.addmm_(1, -2, x, self.centers.t())\n",
        "\n",
        "        classes = torch.arange(self.num_classes).long().to(self.device)\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
        "\n",
        "        dist = []\n",
        "        for i in range(batch_size):\n",
        "            value = distmat[i][mask[i]]\n",
        "            value = value.clamp(min=1e-12, max=1e+12) # for numerical stability\n",
        "            dist.append(value)\n",
        "        dist = torch.cat(dist)\n",
        "        loss = dist.mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISzfrg259DeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "def train_closs(optimizer_closs, optimizer_label, model, data_loader, test_loader, task='Classification'):\n",
        "    \n",
        "    model.train()\n",
        "    lr = learningRate\n",
        "    # optimizer_label = torch.optim.Adam(model.parameters(), amsgrad=True, lr=0.01)\n",
        "    # optimizer_closs = torch.optim.Adam(criterion_closs.parameters(), amsgrad=True, lr=0.5)\n",
        "    scheduler_label = torch.optim.lr_scheduler.StepLR(optimizer_label, 1, gamma=0.9)\n",
        "    scheduler_closs = torch.optim.lr_scheduler.StepLR(optimizer_closs, 1, gamma=0.9)\n",
        "    for epoch in range(numEpochs):\n",
        "        print(\"Current running lr is: \",optimizer_label.param_groups[0]['lr'])\n",
        "        \n",
        "        avg_loss = 0.0\n",
        "        epoch_start = time.time()\n",
        "        # if epoch > 7:\n",
        "        #     lr = lr/5\n",
        "        #     lr_cent = lr_cent/5\n",
        "        #     optimizer_label = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weightDecay, momentum=0.9)\n",
        "        #     optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)\n",
        "        # elif epoch > 10 and epoch <= 15:\n",
        "        #     lr = lr/2\n",
        "        #     lr_cent = lr_cent/2\n",
        "        #     optimizer_label = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weightDecay, momentum=0.9)\n",
        "        #     optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)\n",
        "\n",
        "\n",
        "        for batch_num, (feats, labels) in enumerate(data_loader):\n",
        "            feats, labels = feats.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer_label.zero_grad()\n",
        "            optimizer_closs.zero_grad()\n",
        "            \n",
        "            feature, outputs = model(feats)\n",
        "            # print(\"feature size\", feature.size())\n",
        "            # print(\"outputs size\", outputs.size())\n",
        "            # print(\"label size\", labels.size())\n",
        "            l_loss = criterion_label(outputs, labels.long())\n",
        "            c_loss = criterion_closs(feature, labels.long())\n",
        "            loss = l_loss + closs_weight * c_loss\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            optimizer_label.step()\n",
        "            # by doing so, weight_cent would not impact on the learning of centers\n",
        "            for param in criterion_closs.parameters():\n",
        "                param.grad.data *= (1. / closs_weight)\n",
        "            optimizer_closs.step()\n",
        "            \n",
        "            avg_loss += loss.item()\n",
        "\n",
        "            if batch_num % 50 == 49:\n",
        "                temp = time.time()\n",
        "                print('Epoch: {}\\tBatch: {}\\tAvg-Loss: {:.4f}'.format(epoch+1, batch_num+1, avg_loss/50))\n",
        "                avg_loss = 0.0    \n",
        "                print(\"Time for 50 batch \", temp - epoch_start)\n",
        "            \n",
        "            # clear the cuda cache\n",
        "            torch.cuda.empty_cache()\n",
        "            del feats\n",
        "            del labels\n",
        "            del loss\n",
        "\n",
        "        if task == 'Classification':\n",
        "            val_loss, val_acc, v_closs, v_lloss,v_criterion = test_classify_closs(model, test_loader)\n",
        "            # train_loss, train_acc, _, _, _= test_classify_closs(model, data_loader)\n",
        "            print('#########################    Val Loss: {:.4f}\\tVal Accuracy: {:.4f}  ########################'.\n",
        "                  format(val_loss, val_acc))\n",
        "            # use val loss to change lr\n",
        "            scheduler_label.step()\n",
        "            scheduler_closs.step()\n",
        "        else:\n",
        "            test_verify(model, test_loader)\n",
        "        model_save_name = 'classifier_trial2' + str(epoch + 1)+ '.pt'\n",
        "        path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "        torch.save({\n",
        "            # 'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': (optimizer_closs.state_dict(),optimizer_label.state_dict()),\n",
        "            # 'loss': loss,\n",
        "            }, path)\n",
        "        print(\"Running time for epoch \" + str(epoch + 1) + \" is \" + str((epoch_start - time.time())/60))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_classify_closs(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
        "        feats, labels = feats.to(device), labels.to(device)\n",
        "        feature, outputs = model(feats)\n",
        "        \n",
        "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
        "        pred_labels = pred_labels.view(-1)\n",
        "        l_loss = criterion_label(outputs, labels.long())\n",
        "        c_loss = criterion_closs(feature, labels.long())\n",
        "        loss = l_loss + closs_weight * c_loss\n",
        "        \n",
        "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "        total += len(labels)\n",
        "        test_loss.extend([loss.item()]*feats.size()[0])\n",
        "        del feats\n",
        "        del labels\n",
        "\n",
        "    model.train()\n",
        "    return np.mean(test_loss), accuracy/total, c_loss, l_loss, criterion_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIigD_a55A5S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "2436c4ae-b74f-4b45-9ed8-5f0ea36652a0"
      },
      "source": [
        "closs_weight = 1\n",
        "lr_cent = 0.5\n",
        "feat_dim = 100\n",
        "\n",
        "num_feats = 3\n",
        "hidden_sizes1 = [128, 256, 512]\n",
        "hidden_sizes2 = [512, 1024, 2048]\n",
        "reps = [4,6,3]\n",
        "# network = Network(num_feats, hidden_sizes, num_classes, feat_dim)\n",
        "\n",
        "# network.apply(init_weights)\n",
        "\n",
        "network = Network(num_feats, hidden_sizes1, hidden_sizes2, reps, num_classes, feat_dim)\n",
        "\n",
        "# network = Resnet50(num_feats, num_classes, feat_dim)\n",
        "criterion_label = nn.CrossEntropyLoss()\n",
        "criterion_closs = CenterLoss(num_classes, feat_dim, device)\n",
        "# optimizer_label = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "# optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr=lr_cent)        \n",
        "optimizer_label = torch.optim.SGD(network.parameters(), lr=0.01)\n",
        "optimizer_closs = torch.optim.SGD(criterion_closs.parameters(), lr = 0.5)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i == 0 and j == 0\n",
            "i != 0 and j != 0\n",
            "i != 0 and j != 0\n",
            "i != 0 and j != 0\n",
            "i != 0 and j == 0\n",
            "i != 0 and j != 0\n",
            "i != 0 and j != 0\n",
            "i != 0 and j != 0\n",
            "i != 0 and j != 0\n",
            "i != 0 and j != 0\n",
            "i != 0 and j == 0\n",
            "i != 0 and j != 0\n",
            "i != 0 and j != 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrCxYwqHFgTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b0bd4a0-050c-430f-c563-2716f6c525bd"
      },
      "source": [
        "print(network)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (conv1): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (layers): Sequential(\n",
            "    (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): BottleNeck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (2): BottleNeck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (3): BottleNeck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (4): BottleNeck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): BottleNeck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): BottleNeck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (7): BottleNeck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (8): BottleNeck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (9): BottleNeck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (10): BottleNeck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (11): BottleNeck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (12): BottleNeck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (13): BottleNeck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (14): BottleNeck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (15): BottleNeck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (16): BottleNeck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (activation): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (layers): Sequential(\n",
            "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): ReLU(inplace=True)\n",
            "        (6): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (7): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (linear_label): Linear(in_features=2048, out_features=2300, bias=False)\n",
            "  (linear_closs): Linear(in_features=2048, out_features=100, bias=False)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (relu_closs): ReLU(inplace=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXAija5bn148",
        "colab_type": "text"
      },
      "source": [
        "## Start Training .... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evhgC8xG5A5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2889656c-590a-4e89-acda-2f5fa93dbdfb"
      },
      "source": [
        "# optimizer_label = torch.optim.Adam(network.parameters(), amsgrad=True, lr=0.001)\n",
        "# optimizer_closs = torch.optim.Adam(criterion_closs.parameters(), amsgrad=True, lr = 0.001)\n",
        "network.train()\n",
        "network.to(device)\n",
        "train_closs(optimizer_closs, optimizer_label, network, train_loader, validation_loader)\n",
        "model_save_name = 'classifier_trail2_final.pt'\n",
        "path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "torch.save(network.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current running lr is:  0.01\n",
            "Epoch: 1\tBatch: 50\tAvg-Loss: 106.8876\n",
            "Time for 50 batch  11.152533292770386\n",
            "Epoch: 1\tBatch: 100\tAvg-Loss: 101.6963\n",
            "Time for 50 batch  20.62079429626465\n",
            "Epoch: 1\tBatch: 150\tAvg-Loss: 97.1377\n",
            "Time for 50 batch  30.113090753555298\n",
            "Epoch: 1\tBatch: 200\tAvg-Loss: 93.3351\n",
            "Time for 50 batch  39.53418254852295\n",
            "Epoch: 1\tBatch: 250\tAvg-Loss: 88.7976\n",
            "Time for 50 batch  48.95110034942627\n",
            "Epoch: 1\tBatch: 300\tAvg-Loss: 85.4999\n",
            "Time for 50 batch  58.34293293952942\n",
            "Epoch: 1\tBatch: 350\tAvg-Loss: 82.8654\n",
            "Time for 50 batch  67.7856125831604\n",
            "Epoch: 1\tBatch: 400\tAvg-Loss: 78.1947\n",
            "Time for 50 batch  77.20900654792786\n",
            "Epoch: 1\tBatch: 450\tAvg-Loss: 75.0018\n",
            "Time for 50 batch  86.62083911895752\n",
            "Epoch: 1\tBatch: 500\tAvg-Loss: 73.0690\n",
            "Time for 50 batch  96.0624508857727\n",
            "Epoch: 1\tBatch: 550\tAvg-Loss: 69.6154\n",
            "Time for 50 batch  105.48491597175598\n",
            "Epoch: 1\tBatch: 600\tAvg-Loss: 67.3595\n",
            "Time for 50 batch  114.88822841644287\n",
            "Epoch: 1\tBatch: 650\tAvg-Loss: 64.6115\n",
            "Time for 50 batch  124.29357361793518\n",
            "Epoch: 1\tBatch: 700\tAvg-Loss: 62.3668\n",
            "Time for 50 batch  133.69505095481873\n",
            "Epoch: 1\tBatch: 750\tAvg-Loss: 60.0291\n",
            "Time for 50 batch  143.11300349235535\n",
            "Epoch: 1\tBatch: 800\tAvg-Loss: 57.8151\n",
            "Time for 50 batch  152.56285881996155\n",
            "Epoch: 1\tBatch: 850\tAvg-Loss: 55.1810\n",
            "Time for 50 batch  161.97032356262207\n",
            "Epoch: 1\tBatch: 900\tAvg-Loss: 53.5689\n",
            "Time for 50 batch  171.3697760105133\n",
            "Epoch: 1\tBatch: 950\tAvg-Loss: 52.2666\n",
            "Time for 50 batch  180.80740523338318\n",
            "Epoch: 1\tBatch: 1000\tAvg-Loss: 49.6110\n",
            "Time for 50 batch  190.21735906600952\n",
            "Epoch: 1\tBatch: 1050\tAvg-Loss: 47.9244\n",
            "Time for 50 batch  199.69589495658875\n",
            "Epoch: 1\tBatch: 1100\tAvg-Loss: 45.8015\n",
            "Time for 50 batch  209.12542581558228\n",
            "Epoch: 1\tBatch: 1150\tAvg-Loss: 44.7244\n",
            "Time for 50 batch  218.56790566444397\n",
            "Epoch: 1\tBatch: 1200\tAvg-Loss: 43.3739\n",
            "Time for 50 batch  227.9769515991211\n",
            "Epoch: 1\tBatch: 1250\tAvg-Loss: 41.1942\n",
            "Time for 50 batch  237.43875765800476\n",
            "Epoch: 1\tBatch: 1300\tAvg-Loss: 39.7572\n",
            "Time for 50 batch  246.89339303970337\n",
            "Epoch: 1\tBatch: 1350\tAvg-Loss: 38.4085\n",
            "Time for 50 batch  256.35796070098877\n",
            "Epoch: 1\tBatch: 1400\tAvg-Loss: 37.7002\n",
            "Time for 50 batch  265.76742672920227\n",
            "Epoch: 1\tBatch: 1450\tAvg-Loss: 35.8692\n",
            "Time for 50 batch  275.21754002571106\n",
            "Epoch: 1\tBatch: 1500\tAvg-Loss: 34.7502\n",
            "Time for 50 batch  284.67381978034973\n",
            "Epoch: 1\tBatch: 1550\tAvg-Loss: 33.8245\n",
            "Time for 50 batch  294.13451504707336\n",
            "Epoch: 1\tBatch: 1600\tAvg-Loss: 32.5842\n",
            "Time for 50 batch  303.54352498054504\n",
            "Epoch: 1\tBatch: 1650\tAvg-Loss: 31.3416\n",
            "Time for 50 batch  313.0158531665802\n",
            "Epoch: 1\tBatch: 1700\tAvg-Loss: 30.9686\n",
            "Time for 50 batch  322.46469497680664\n",
            "Epoch: 1\tBatch: 1750\tAvg-Loss: 30.2577\n",
            "Time for 50 batch  331.9153368473053\n",
            "Epoch: 1\tBatch: 1800\tAvg-Loss: 28.7792\n",
            "Time for 50 batch  341.3715944290161\n",
            "Epoch: 1\tBatch: 1850\tAvg-Loss: 28.0910\n",
            "Time for 50 batch  350.7571654319763\n",
            "Epoch: 1\tBatch: 1900\tAvg-Loss: 27.0088\n",
            "Time for 50 batch  360.1845655441284\n",
            "Epoch: 1\tBatch: 1950\tAvg-Loss: 26.1970\n",
            "Time for 50 batch  369.61589217185974\n",
            "Epoch: 1\tBatch: 2000\tAvg-Loss: 25.3455\n",
            "Time for 50 batch  379.0974850654602\n",
            "Epoch: 1\tBatch: 2050\tAvg-Loss: 24.9055\n",
            "Time for 50 batch  388.5202705860138\n",
            "Epoch: 1\tBatch: 2100\tAvg-Loss: 24.4009\n",
            "Time for 50 batch  398.033704996109\n",
            "Epoch: 1\tBatch: 2150\tAvg-Loss: 24.1345\n",
            "Time for 50 batch  407.51261711120605\n",
            "Epoch: 1\tBatch: 2200\tAvg-Loss: 23.1552\n",
            "Time for 50 batch  416.9433488845825\n",
            "Epoch: 1\tBatch: 2250\tAvg-Loss: 22.3582\n",
            "Time for 50 batch  426.3389654159546\n",
            "Epoch: 1\tBatch: 2300\tAvg-Loss: 22.0735\n",
            "Time for 50 batch  435.78685665130615\n",
            "Epoch: 1\tBatch: 2350\tAvg-Loss: 20.8927\n",
            "Time for 50 batch  445.23363494873047\n",
            "Epoch: 1\tBatch: 2400\tAvg-Loss: 20.7286\n",
            "Time for 50 batch  454.72897815704346\n",
            "Epoch: 1\tBatch: 2450\tAvg-Loss: 20.4813\n",
            "Time for 50 batch  464.19611978530884\n",
            "Epoch: 1\tBatch: 2500\tAvg-Loss: 19.5287\n",
            "Time for 50 batch  473.7032742500305\n",
            "Epoch: 1\tBatch: 2550\tAvg-Loss: 19.3909\n",
            "Time for 50 batch  483.20190930366516\n",
            "Epoch: 1\tBatch: 2600\tAvg-Loss: 18.8561\n",
            "Time for 50 batch  492.6964740753174\n",
            "Epoch: 1\tBatch: 2650\tAvg-Loss: 18.6432\n",
            "Time for 50 batch  502.18896532058716\n",
            "Epoch: 1\tBatch: 2700\tAvg-Loss: 17.7651\n",
            "Time for 50 batch  511.65081906318665\n",
            "Epoch: 1\tBatch: 2750\tAvg-Loss: 17.4474\n",
            "Time for 50 batch  521.1253507137299\n",
            "Epoch: 1\tBatch: 2800\tAvg-Loss: 17.2395\n",
            "Time for 50 batch  530.6269202232361\n",
            "Epoch: 1\tBatch: 2850\tAvg-Loss: 16.5616\n",
            "Time for 50 batch  540.0820398330688\n",
            "Epoch: 1\tBatch: 2900\tAvg-Loss: 16.5803\n",
            "Time for 50 batch  549.6089692115784\n",
            "Epoch: 1\tBatch: 2950\tAvg-Loss: 16.2385\n",
            "Time for 50 batch  559.1320908069611\n",
            "Epoch: 1\tBatch: 3000\tAvg-Loss: 15.6170\n",
            "Time for 50 batch  568.6066873073578\n",
            "Epoch: 1\tBatch: 3050\tAvg-Loss: 15.5253\n",
            "Time for 50 batch  578.1294751167297\n",
            "Epoch: 1\tBatch: 3100\tAvg-Loss: 15.1027\n",
            "Time for 50 batch  587.6387686729431\n",
            "Epoch: 1\tBatch: 3150\tAvg-Loss: 14.6865\n",
            "Time for 50 batch  597.1511678695679\n",
            "Epoch: 1\tBatch: 3200\tAvg-Loss: 14.7041\n",
            "Time for 50 batch  606.6495022773743\n",
            "Epoch: 1\tBatch: 3250\tAvg-Loss: 14.5161\n",
            "Time for 50 batch  616.1650695800781\n",
            "Epoch: 1\tBatch: 3300\tAvg-Loss: 14.1605\n",
            "Time for 50 batch  625.6855895519257\n",
            "Epoch: 1\tBatch: 3350\tAvg-Loss: 14.1296\n",
            "Time for 50 batch  635.1842050552368\n",
            "Epoch: 1\tBatch: 3400\tAvg-Loss: 13.8464\n",
            "Time for 50 batch  644.7247173786163\n",
            "Epoch: 1\tBatch: 3450\tAvg-Loss: 13.7971\n",
            "Time for 50 batch  654.2918457984924\n",
            "Epoch: 1\tBatch: 3500\tAvg-Loss: 13.0149\n",
            "Time for 50 batch  663.8650860786438\n",
            "Epoch: 1\tBatch: 3550\tAvg-Loss: 12.9264\n",
            "Time for 50 batch  673.3907980918884\n",
            "Epoch: 1\tBatch: 3600\tAvg-Loss: 12.8959\n",
            "Time for 50 batch  682.9528195858002\n",
            "Epoch: 1\tBatch: 3650\tAvg-Loss: 12.5505\n",
            "Time for 50 batch  692.4825451374054\n",
            "Epoch: 1\tBatch: 3700\tAvg-Loss: 12.7089\n",
            "Time for 50 batch  702.0061731338501\n",
            "Epoch: 1\tBatch: 3750\tAvg-Loss: 12.3788\n",
            "Time for 50 batch  711.5153706073761\n",
            "Epoch: 1\tBatch: 3800\tAvg-Loss: 12.1758\n",
            "Time for 50 batch  721.0538055896759\n",
            "Epoch: 1\tBatch: 3850\tAvg-Loss: 12.0764\n",
            "Time for 50 batch  730.5871224403381\n",
            "Epoch: 1\tBatch: 3900\tAvg-Loss: 11.8782\n",
            "Time for 50 batch  740.1379497051239\n",
            "Epoch: 1\tBatch: 3950\tAvg-Loss: 11.7162\n",
            "Time for 50 batch  749.7083964347839\n",
            "Epoch: 1\tBatch: 4000\tAvg-Loss: 11.4760\n",
            "Time for 50 batch  759.2353951931\n",
            "Epoch: 1\tBatch: 4050\tAvg-Loss: 11.5964\n",
            "Time for 50 batch  768.7438540458679\n",
            "Epoch: 1\tBatch: 4100\tAvg-Loss: 11.2878\n",
            "Time for 50 batch  778.2639327049255\n",
            "Epoch: 1\tBatch: 4150\tAvg-Loss: 11.3533\n",
            "Time for 50 batch  787.7454771995544\n",
            "Epoch: 1\tBatch: 4200\tAvg-Loss: 11.0360\n",
            "Time for 50 batch  797.2671625614166\n",
            "Epoch: 1\tBatch: 4250\tAvg-Loss: 11.1789\n",
            "Time for 50 batch  806.7764658927917\n",
            "Epoch: 1\tBatch: 4300\tAvg-Loss: 10.7412\n",
            "Time for 50 batch  816.2502522468567\n",
            "Epoch: 1\tBatch: 4350\tAvg-Loss: 10.6366\n",
            "Time for 50 batch  825.7312211990356\n",
            "Epoch: 1\tBatch: 4400\tAvg-Loss: 10.6443\n",
            "Time for 50 batch  835.2453637123108\n",
            "Epoch: 1\tBatch: 4450\tAvg-Loss: 10.4691\n",
            "Time for 50 batch  844.7144496440887\n",
            "Epoch: 1\tBatch: 4500\tAvg-Loss: 10.3302\n",
            "Time for 50 batch  854.2367341518402\n",
            "Epoch: 1\tBatch: 4550\tAvg-Loss: 10.3356\n",
            "Time for 50 batch  863.7212116718292\n",
            "Epoch: 1\tBatch: 4600\tAvg-Loss: 10.3327\n",
            "Time for 50 batch  873.2181310653687\n",
            "Epoch: 1\tBatch: 4650\tAvg-Loss: 10.1239\n",
            "Time for 50 batch  882.7221775054932\n",
            "Epoch: 1\tBatch: 4700\tAvg-Loss: 10.0624\n",
            "Time for 50 batch  892.172581911087\n",
            "Epoch: 1\tBatch: 4750\tAvg-Loss: 10.0323\n",
            "Time for 50 batch  901.6243755817413\n",
            "Epoch: 1\tBatch: 4800\tAvg-Loss: 9.7866\n",
            "Time for 50 batch  911.0906989574432\n",
            "Epoch: 1\tBatch: 4850\tAvg-Loss: 9.9681\n",
            "Time for 50 batch  920.5950031280518\n",
            "Epoch: 1\tBatch: 4900\tAvg-Loss: 9.8693\n",
            "Time for 50 batch  930.0648109912872\n",
            "Epoch: 1\tBatch: 4950\tAvg-Loss: 9.6199\n",
            "Time for 50 batch  939.5584111213684\n",
            "Epoch: 1\tBatch: 5000\tAvg-Loss: 9.6744\n",
            "Time for 50 batch  949.0526597499847\n",
            "Epoch: 1\tBatch: 5050\tAvg-Loss: 9.6053\n",
            "Time for 50 batch  958.6593120098114\n",
            "Epoch: 1\tBatch: 5100\tAvg-Loss: 9.5721\n",
            "Time for 50 batch  968.1577699184418\n",
            "Epoch: 1\tBatch: 5150\tAvg-Loss: 9.4582\n",
            "Time for 50 batch  977.6284079551697\n",
            "Epoch: 1\tBatch: 5200\tAvg-Loss: 9.2542\n",
            "Time for 50 batch  987.1698412895203\n",
            "Epoch: 1\tBatch: 5250\tAvg-Loss: 9.3750\n",
            "Time for 50 batch  996.7005581855774\n",
            "Epoch: 1\tBatch: 5300\tAvg-Loss: 9.2127\n",
            "Time for 50 batch  1006.1808481216431\n",
            "Epoch: 1\tBatch: 5350\tAvg-Loss: 9.2081\n",
            "Time for 50 batch  1015.6773285865784\n",
            "Epoch: 1\tBatch: 5400\tAvg-Loss: 9.1754\n",
            "Time for 50 batch  1025.1901106834412\n",
            "Epoch: 1\tBatch: 5450\tAvg-Loss: 9.0054\n",
            "Time for 50 batch  1034.7111761569977\n",
            "Epoch: 1\tBatch: 5500\tAvg-Loss: 9.0626\n",
            "Time for 50 batch  1044.2379434108734\n",
            "Epoch: 1\tBatch: 5550\tAvg-Loss: 9.0173\n",
            "Time for 50 batch  1053.7157328128815\n",
            "Epoch: 1\tBatch: 5600\tAvg-Loss: 8.8366\n",
            "Time for 50 batch  1063.2038984298706\n",
            "Epoch: 1\tBatch: 5650\tAvg-Loss: 8.9600\n",
            "Time for 50 batch  1072.7060616016388\n",
            "Epoch: 1\tBatch: 5700\tAvg-Loss: 8.8556\n",
            "Time for 50 batch  1082.2015793323517\n",
            "Epoch: 1\tBatch: 5750\tAvg-Loss: 8.8173\n",
            "Time for 50 batch  1091.7261712551117\n",
            "Epoch: 1\tBatch: 5800\tAvg-Loss: 8.8351\n",
            "Time for 50 batch  1101.291392326355\n",
            "Epoch: 1\tBatch: 5850\tAvg-Loss: 8.6690\n",
            "Time for 50 batch  1110.8024234771729\n",
            "Epoch: 1\tBatch: 5900\tAvg-Loss: 8.7555\n",
            "Time for 50 batch  1120.3024952411652\n",
            "Epoch: 1\tBatch: 5950\tAvg-Loss: 8.5816\n",
            "Time for 50 batch  1129.8089611530304\n",
            "Epoch: 1\tBatch: 6000\tAvg-Loss: 8.6256\n",
            "Time for 50 batch  1139.3137176036835\n",
            "Epoch: 1\tBatch: 6050\tAvg-Loss: 8.6116\n",
            "Time for 50 batch  1148.84406042099\n",
            "Epoch: 1\tBatch: 6100\tAvg-Loss: 8.5026\n",
            "Time for 50 batch  1158.3716831207275\n",
            "Epoch: 1\tBatch: 6150\tAvg-Loss: 8.5094\n",
            "Time for 50 batch  1167.8893101215363\n",
            "Epoch: 1\tBatch: 6200\tAvg-Loss: 8.5556\n",
            "Time for 50 batch  1177.3859295845032\n",
            "Epoch: 1\tBatch: 6250\tAvg-Loss: 8.4665\n",
            "Time for 50 batch  1186.8705341815948\n",
            "Epoch: 1\tBatch: 6300\tAvg-Loss: 8.4060\n",
            "Time for 50 batch  1196.3885595798492\n",
            "Epoch: 1\tBatch: 6350\tAvg-Loss: 8.3439\n",
            "Time for 50 batch  1205.9137127399445\n",
            "Epoch: 1\tBatch: 6400\tAvg-Loss: 8.4076\n",
            "Time for 50 batch  1215.458352804184\n",
            "Epoch: 1\tBatch: 6450\tAvg-Loss: 8.4522\n",
            "Time for 50 batch  1225.0172410011292\n",
            "Epoch: 1\tBatch: 6500\tAvg-Loss: 8.3556\n",
            "Time for 50 batch  1234.5434131622314\n",
            "Epoch: 1\tBatch: 6550\tAvg-Loss: 8.2638\n",
            "Time for 50 batch  1244.0442471504211\n",
            "Epoch: 1\tBatch: 6600\tAvg-Loss: 8.3622\n",
            "Time for 50 batch  1253.5652375221252\n",
            "Epoch: 1\tBatch: 6650\tAvg-Loss: 8.2550\n",
            "Time for 50 batch  1263.12370967865\n",
            "Epoch: 1\tBatch: 6700\tAvg-Loss: 8.3125\n",
            "Time for 50 batch  1272.654013633728\n",
            "Epoch: 1\tBatch: 6750\tAvg-Loss: 8.2601\n",
            "Time for 50 batch  1282.2041413784027\n",
            "Epoch: 1\tBatch: 6800\tAvg-Loss: 8.2006\n",
            "Time for 50 batch  1291.7869880199432\n",
            "Epoch: 1\tBatch: 6850\tAvg-Loss: 8.1865\n",
            "Time for 50 batch  1301.3129432201385\n",
            "Epoch: 1\tBatch: 6900\tAvg-Loss: 8.1207\n",
            "Time for 50 batch  1310.7943904399872\n",
            "Epoch: 1\tBatch: 6950\tAvg-Loss: 8.1101\n",
            "Time for 50 batch  1320.3071014881134\n",
            "Epoch: 1\tBatch: 7000\tAvg-Loss: 8.1146\n",
            "Time for 50 batch  1329.7613728046417\n",
            "Epoch: 1\tBatch: 7050\tAvg-Loss: 8.0568\n",
            "Time for 50 batch  1339.2629516124725\n",
            "Epoch: 1\tBatch: 7100\tAvg-Loss: 8.1187\n",
            "Time for 50 batch  1348.806712627411\n",
            "Epoch: 1\tBatch: 7150\tAvg-Loss: 8.0527\n",
            "Time for 50 batch  1358.3340106010437\n",
            "Epoch: 1\tBatch: 7200\tAvg-Loss: 8.0392\n",
            "Time for 50 batch  1367.8618512153625\n",
            "Epoch: 1\tBatch: 7250\tAvg-Loss: 7.9800\n",
            "Time for 50 batch  1377.3864977359772\n",
            "Epoch: 1\tBatch: 7300\tAvg-Loss: 8.0539\n",
            "Time for 50 batch  1386.9133694171906\n",
            "Epoch: 1\tBatch: 7350\tAvg-Loss: 8.0729\n",
            "Time for 50 batch  1396.4228026866913\n",
            "Epoch: 1\tBatch: 7400\tAvg-Loss: 8.0715\n",
            "Time for 50 batch  1405.9340872764587\n",
            "Epoch: 1\tBatch: 7450\tAvg-Loss: 8.0165\n",
            "Time for 50 batch  1415.4613552093506\n",
            "Epoch: 1\tBatch: 7500\tAvg-Loss: 7.9984\n",
            "Time for 50 batch  1424.9754786491394\n",
            "Epoch: 1\tBatch: 7550\tAvg-Loss: 7.9381\n",
            "Time for 50 batch  1434.4605205059052\n",
            "Epoch: 1\tBatch: 7600\tAvg-Loss: 7.9341\n",
            "Time for 50 batch  1443.9396164417267\n",
            "Epoch: 1\tBatch: 7650\tAvg-Loss: 7.9000\n",
            "Time for 50 batch  1453.4241254329681\n",
            "Epoch: 1\tBatch: 7700\tAvg-Loss: 7.9838\n",
            "Time for 50 batch  1462.9341115951538\n",
            "Epoch: 1\tBatch: 7750\tAvg-Loss: 7.8506\n",
            "Time for 50 batch  1472.428321838379\n",
            "Epoch: 1\tBatch: 7800\tAvg-Loss: 7.9135\n",
            "Time for 50 batch  1481.9232559204102\n",
            "Epoch: 1\tBatch: 7850\tAvg-Loss: 7.8496\n",
            "Time for 50 batch  1491.4107773303986\n",
            "Epoch: 1\tBatch: 7900\tAvg-Loss: 7.8169\n",
            "Time for 50 batch  1500.9671585559845\n",
            "Epoch: 1\tBatch: 7950\tAvg-Loss: 7.8811\n",
            "Time for 50 batch  1510.522100687027\n",
            "Epoch: 1\tBatch: 8000\tAvg-Loss: 7.8665\n",
            "Time for 50 batch  1520.0790767669678\n",
            "Epoch: 1\tBatch: 8050\tAvg-Loss: 7.7941\n",
            "Time for 50 batch  1529.5721142292023\n",
            "Epoch: 1\tBatch: 8100\tAvg-Loss: 7.8071\n",
            "Time for 50 batch  1539.076642036438\n",
            "Epoch: 1\tBatch: 8150\tAvg-Loss: 7.7602\n",
            "Time for 50 batch  1548.584007024765\n",
            "Epoch: 1\tBatch: 8200\tAvg-Loss: 7.8441\n",
            "Time for 50 batch  1558.0838422775269\n",
            "Epoch: 1\tBatch: 8250\tAvg-Loss: 7.8354\n",
            "Time for 50 batch  1567.568558216095\n",
            "Epoch: 1\tBatch: 8300\tAvg-Loss: 7.7823\n",
            "Time for 50 batch  1577.1196775436401\n",
            "Epoch: 1\tBatch: 8350\tAvg-Loss: 7.7611\n",
            "Time for 50 batch  1586.6907346248627\n",
            "Epoch: 1\tBatch: 8400\tAvg-Loss: 7.7695\n",
            "Time for 50 batch  1596.2443039417267\n",
            "Epoch: 1\tBatch: 8450\tAvg-Loss: 7.7871\n",
            "Time for 50 batch  1605.7594892978668\n",
            "Epoch: 1\tBatch: 8500\tAvg-Loss: 7.7022\n",
            "Time for 50 batch  1615.276088476181\n",
            "Epoch: 1\tBatch: 8550\tAvg-Loss: 7.7324\n",
            "Time for 50 batch  1624.7973980903625\n",
            "Epoch: 1\tBatch: 8600\tAvg-Loss: 7.7281\n",
            "Time for 50 batch  1634.3173706531525\n",
            "Epoch: 1\tBatch: 8650\tAvg-Loss: 7.7402\n",
            "Time for 50 batch  1643.832210302353\n",
            "Epoch: 1\tBatch: 8700\tAvg-Loss: 7.7415\n",
            "Time for 50 batch  1653.3771889209747\n",
            "Epoch: 1\tBatch: 8750\tAvg-Loss: 7.7239\n",
            "Time for 50 batch  1662.8712570667267\n",
            "Epoch: 1\tBatch: 8800\tAvg-Loss: 7.6794\n",
            "Time for 50 batch  1672.4007382392883\n",
            "Epoch: 1\tBatch: 8850\tAvg-Loss: 7.6979\n",
            "Time for 50 batch  1681.9365720748901\n",
            "Epoch: 1\tBatch: 8900\tAvg-Loss: 7.7102\n",
            "Time for 50 batch  1691.4711685180664\n",
            "Epoch: 1\tBatch: 8950\tAvg-Loss: 7.6352\n",
            "Time for 50 batch  1701.0268664360046\n",
            "Epoch: 1\tBatch: 9000\tAvg-Loss: 7.6928\n",
            "Time for 50 batch  1710.5336594581604\n",
            "Epoch: 1\tBatch: 9050\tAvg-Loss: 7.6847\n",
            "Time for 50 batch  1720.0663940906525\n",
            "Epoch: 1\tBatch: 9100\tAvg-Loss: 7.6613\n",
            "Time for 50 batch  1729.6222512722015\n",
            "Epoch: 1\tBatch: 9150\tAvg-Loss: 7.6369\n",
            "Time for 50 batch  1739.1676137447357\n",
            "Epoch: 1\tBatch: 9200\tAvg-Loss: 7.6641\n",
            "Time for 50 batch  1748.688273191452\n",
            "Epoch: 1\tBatch: 9250\tAvg-Loss: 7.6499\n",
            "Time for 50 batch  1758.241168498993\n",
            "Epoch: 1\tBatch: 9300\tAvg-Loss: 7.5995\n",
            "Time for 50 batch  1767.7732255458832\n",
            "Epoch: 1\tBatch: 9350\tAvg-Loss: 7.6115\n",
            "Time for 50 batch  1777.2850065231323\n",
            "Epoch: 1\tBatch: 9400\tAvg-Loss: 7.6736\n",
            "Time for 50 batch  1786.8021113872528\n",
            "Epoch: 1\tBatch: 9450\tAvg-Loss: 7.5772\n",
            "Time for 50 batch  1796.3198735713959\n",
            "Epoch: 1\tBatch: 9500\tAvg-Loss: 7.6238\n",
            "Time for 50 batch  1805.8672652244568\n",
            "Epoch: 1\tBatch: 9550\tAvg-Loss: 7.6349\n",
            "Time for 50 batch  1815.4119231700897\n",
            "Epoch: 1\tBatch: 9600\tAvg-Loss: 7.5984\n",
            "Time for 50 batch  1824.9496994018555\n",
            "Epoch: 1\tBatch: 9650\tAvg-Loss: 7.5977\n",
            "Time for 50 batch  1834.4604070186615\n",
            "Epoch: 1\tBatch: 9700\tAvg-Loss: 7.6023\n",
            "Time for 50 batch  1843.9899265766144\n",
            "Epoch: 1\tBatch: 9750\tAvg-Loss: 7.5636\n",
            "Time for 50 batch  1853.5155081748962\n",
            "Epoch: 1\tBatch: 9800\tAvg-Loss: 7.6207\n",
            "Time for 50 batch  1863.052588224411\n",
            "Epoch: 1\tBatch: 9850\tAvg-Loss: 7.6019\n",
            "Time for 50 batch  1872.5278058052063\n",
            "Epoch: 1\tBatch: 9900\tAvg-Loss: 7.6079\n",
            "Time for 50 batch  1882.0371587276459\n",
            "Epoch: 1\tBatch: 9950\tAvg-Loss: 7.6019\n",
            "Time for 50 batch  1891.644999742508\n",
            "Epoch: 1\tBatch: 10000\tAvg-Loss: 7.5890\n",
            "Time for 50 batch  1901.1547574996948\n",
            "Epoch: 1\tBatch: 10050\tAvg-Loss: 7.6126\n",
            "Time for 50 batch  1910.6525859832764\n",
            "Epoch: 1\tBatch: 10100\tAvg-Loss: 7.5757\n",
            "Time for 50 batch  1920.1711988449097\n",
            "Epoch: 1\tBatch: 10150\tAvg-Loss: 7.5650\n",
            "Time for 50 batch  1929.714629650116\n",
            "Epoch: 1\tBatch: 10200\tAvg-Loss: 7.5602\n",
            "Time for 50 batch  1939.1864416599274\n",
            "Epoch: 1\tBatch: 10250\tAvg-Loss: 7.5772\n",
            "Time for 50 batch  1948.728372335434\n",
            "Epoch: 1\tBatch: 10300\tAvg-Loss: 7.6168\n",
            "Time for 50 batch  1958.289110660553\n",
            "Epoch: 1\tBatch: 10350\tAvg-Loss: 7.5774\n",
            "Time for 50 batch  1967.8228199481964\n",
            "Epoch: 1\tBatch: 10400\tAvg-Loss: 7.5385\n",
            "Time for 50 batch  1977.3657324314117\n",
            "Epoch: 1\tBatch: 10450\tAvg-Loss: 7.5080\n",
            "Time for 50 batch  1986.9221541881561\n",
            "Epoch: 1\tBatch: 10500\tAvg-Loss: 7.5367\n",
            "Time for 50 batch  1996.440310716629\n",
            "Epoch: 1\tBatch: 10550\tAvg-Loss: 7.5561\n",
            "Time for 50 batch  2005.9616358280182\n",
            "Epoch: 1\tBatch: 10600\tAvg-Loss: 7.5652\n",
            "Time for 50 batch  2015.4773836135864\n",
            "Epoch: 1\tBatch: 10650\tAvg-Loss: 7.5539\n",
            "Time for 50 batch  2025.0148515701294\n",
            "Epoch: 1\tBatch: 10700\tAvg-Loss: 7.5809\n",
            "Time for 50 batch  2034.5043547153473\n",
            "Epoch: 1\tBatch: 10750\tAvg-Loss: 7.5563\n",
            "Time for 50 batch  2044.0314645767212\n",
            "Epoch: 1\tBatch: 10800\tAvg-Loss: 7.4978\n",
            "Time for 50 batch  2053.5601892471313\n",
            "Epoch: 1\tBatch: 10850\tAvg-Loss: 7.5181\n",
            "Time for 50 batch  2063.083776473999\n",
            "Epoch: 1\tBatch: 10900\tAvg-Loss: 7.5211\n",
            "Time for 50 batch  2072.6411731243134\n",
            "Epoch: 1\tBatch: 10950\tAvg-Loss: 7.5222\n",
            "Time for 50 batch  2082.166257619858\n",
            "Epoch: 1\tBatch: 11000\tAvg-Loss: 7.5360\n",
            "Time for 50 batch  2091.690990924835\n",
            "Epoch: 1\tBatch: 11050\tAvg-Loss: 7.4931\n",
            "Time for 50 batch  2101.211572408676\n",
            "Epoch: 1\tBatch: 11100\tAvg-Loss: 7.5077\n",
            "Time for 50 batch  2110.71129322052\n",
            "Epoch: 1\tBatch: 11150\tAvg-Loss: 7.5073\n",
            "Time for 50 batch  2120.2106635570526\n",
            "Epoch: 1\tBatch: 11200\tAvg-Loss: 7.4342\n",
            "Time for 50 batch  2129.736321210861\n",
            "Epoch: 1\tBatch: 11250\tAvg-Loss: 7.5249\n",
            "Time for 50 batch  2139.2406845092773\n",
            "Epoch: 1\tBatch: 11300\tAvg-Loss: 7.5149\n",
            "Time for 50 batch  2148.7270925045013\n",
            "Epoch: 1\tBatch: 11350\tAvg-Loss: 7.5562\n",
            "Time for 50 batch  2158.1879835128784\n",
            "Epoch: 1\tBatch: 11400\tAvg-Loss: 7.4885\n",
            "Time for 50 batch  2167.6725277900696\n",
            "Epoch: 1\tBatch: 11450\tAvg-Loss: 7.4531\n",
            "Time for 50 batch  2177.215491771698\n",
            "Epoch: 1\tBatch: 11500\tAvg-Loss: 7.5029\n",
            "Time for 50 batch  2186.754052877426\n",
            "Epoch: 1\tBatch: 11550\tAvg-Loss: 7.4629\n",
            "Time for 50 batch  2196.3522148132324\n",
            "Epoch: 1\tBatch: 11600\tAvg-Loss: 7.4684\n",
            "Time for 50 batch  2205.993040561676\n",
            "Epoch: 1\tBatch: 11650\tAvg-Loss: 7.4818\n",
            "Time for 50 batch  2215.592684984207\n",
            "Epoch: 1\tBatch: 11700\tAvg-Loss: 7.5014\n",
            "Time for 50 batch  2225.1666328907013\n",
            "Epoch: 1\tBatch: 11750\tAvg-Loss: 7.4966\n",
            "Time for 50 batch  2234.7351565361023\n",
            "Epoch: 1\tBatch: 11800\tAvg-Loss: 7.4766\n",
            "Time for 50 batch  2244.316177368164\n",
            "Epoch: 1\tBatch: 11850\tAvg-Loss: 7.4638\n",
            "Time for 50 batch  2253.8870725631714\n",
            "Epoch: 1\tBatch: 11900\tAvg-Loss: 7.4674\n",
            "Time for 50 batch  2263.41175365448\n",
            "Epoch: 1\tBatch: 11950\tAvg-Loss: 7.4744\n",
            "Time for 50 batch  2272.9901781082153\n",
            "Epoch: 1\tBatch: 12000\tAvg-Loss: 7.4827\n",
            "Time for 50 batch  2282.5406601428986\n",
            "Epoch: 1\tBatch: 12050\tAvg-Loss: 7.4940\n",
            "Time for 50 batch  2292.1090803146362\n",
            "Epoch: 1\tBatch: 12100\tAvg-Loss: 7.4818\n",
            "Time for 50 batch  2301.6913809776306\n",
            "Epoch: 1\tBatch: 12150\tAvg-Loss: 7.4329\n",
            "Time for 50 batch  2311.265697479248\n",
            "Epoch: 1\tBatch: 12200\tAvg-Loss: 7.4573\n",
            "Time for 50 batch  2320.8297271728516\n",
            "Epoch: 1\tBatch: 12250\tAvg-Loss: 7.4709\n",
            "Time for 50 batch  2330.3794510364532\n",
            "Epoch: 1\tBatch: 12300\tAvg-Loss: 7.4355\n",
            "Time for 50 batch  2339.956079006195\n",
            "Epoch: 1\tBatch: 12350\tAvg-Loss: 7.4646\n",
            "Time for 50 batch  2349.5035350322723\n",
            "Epoch: 1\tBatch: 12400\tAvg-Loss: 7.4542\n",
            "Time for 50 batch  2359.0861642360687\n",
            "Epoch: 1\tBatch: 12450\tAvg-Loss: 7.4539\n",
            "Time for 50 batch  2368.659153699875\n",
            "Epoch: 1\tBatch: 12500\tAvg-Loss: 7.4799\n",
            "Time for 50 batch  2378.256779193878\n",
            "Epoch: 1\tBatch: 12550\tAvg-Loss: 7.4431\n",
            "Time for 50 batch  2387.803144931793\n",
            "Epoch: 1\tBatch: 12600\tAvg-Loss: 7.4726\n",
            "Time for 50 batch  2397.3560705184937\n",
            "Epoch: 1\tBatch: 12650\tAvg-Loss: 7.4676\n",
            "Time for 50 batch  2406.9448862075806\n",
            "Epoch: 1\tBatch: 12700\tAvg-Loss: 7.4692\n",
            "Time for 50 batch  2416.5410804748535\n",
            "Epoch: 1\tBatch: 12750\tAvg-Loss: 7.4371\n",
            "Time for 50 batch  2426.130054473877\n",
            "Epoch: 1\tBatch: 12800\tAvg-Loss: 7.4388\n",
            "Time for 50 batch  2435.745915412903\n",
            "Epoch: 1\tBatch: 12850\tAvg-Loss: 7.4305\n",
            "Time for 50 batch  2445.3069775104523\n",
            "Epoch: 1\tBatch: 12900\tAvg-Loss: 7.4296\n",
            "Time for 50 batch  2454.917854785919\n",
            "Epoch: 1\tBatch: 12950\tAvg-Loss: 7.4264\n",
            "Time for 50 batch  2464.495356798172\n",
            "Epoch: 1\tBatch: 13000\tAvg-Loss: 7.4175\n",
            "Time for 50 batch  2474.072303533554\n",
            "Epoch: 1\tBatch: 13050\tAvg-Loss: 7.4317\n",
            "Time for 50 batch  2483.6562027931213\n",
            "Epoch: 1\tBatch: 13100\tAvg-Loss: 7.4242\n",
            "Time for 50 batch  2493.224864244461\n",
            "Epoch: 1\tBatch: 13150\tAvg-Loss: 7.4332\n",
            "Time for 50 batch  2502.7862741947174\n",
            "Epoch: 1\tBatch: 13200\tAvg-Loss: 7.4160\n",
            "Time for 50 batch  2512.370749473572\n",
            "Epoch: 1\tBatch: 13250\tAvg-Loss: 7.4286\n",
            "Time for 50 batch  2521.9027054309845\n",
            "Epoch: 1\tBatch: 13300\tAvg-Loss: 7.4254\n",
            "Time for 50 batch  2531.4453094005585\n",
            "Epoch: 1\tBatch: 13350\tAvg-Loss: 7.4274\n",
            "Time for 50 batch  2540.9681282043457\n",
            "Epoch: 1\tBatch: 13400\tAvg-Loss: 7.4239\n",
            "Time for 50 batch  2550.5267493724823\n",
            "Epoch: 1\tBatch: 13450\tAvg-Loss: 7.4077\n",
            "Time for 50 batch  2560.052440881729\n",
            "Epoch: 1\tBatch: 13500\tAvg-Loss: 7.3864\n",
            "Time for 50 batch  2569.6040785312653\n",
            "Epoch: 1\tBatch: 13550\tAvg-Loss: 7.3894\n",
            "Time for 50 batch  2579.166157722473\n",
            "Epoch: 1\tBatch: 13600\tAvg-Loss: 7.3910\n",
            "Time for 50 batch  2588.6437397003174\n",
            "Epoch: 1\tBatch: 13650\tAvg-Loss: 7.3985\n",
            "Time for 50 batch  2598.2014865875244\n",
            "Epoch: 1\tBatch: 13700\tAvg-Loss: 7.3681\n",
            "Time for 50 batch  2607.7644271850586\n",
            "Epoch: 1\tBatch: 13750\tAvg-Loss: 7.3983\n",
            "Time for 50 batch  2617.302623987198\n",
            "Epoch: 1\tBatch: 13800\tAvg-Loss: 7.4043\n",
            "Time for 50 batch  2626.8081953525543\n",
            "Epoch: 1\tBatch: 13850\tAvg-Loss: 7.4058\n",
            "Time for 50 batch  2636.3293998241425\n",
            "Epoch: 1\tBatch: 13900\tAvg-Loss: 7.4100\n",
            "Time for 50 batch  2645.7922706604004\n",
            "Epoch: 1\tBatch: 13950\tAvg-Loss: 7.3665\n",
            "Time for 50 batch  2655.31551861763\n",
            "Epoch: 1\tBatch: 14000\tAvg-Loss: 7.3662\n",
            "Time for 50 batch  2664.81587767601\n",
            "Epoch: 1\tBatch: 14050\tAvg-Loss: 7.3693\n",
            "Time for 50 batch  2674.3392190933228\n",
            "Epoch: 1\tBatch: 14100\tAvg-Loss: 7.3943\n",
            "Time for 50 batch  2683.845734357834\n",
            "Epoch: 1\tBatch: 14150\tAvg-Loss: 7.4075\n",
            "Time for 50 batch  2693.3430058956146\n",
            "Epoch: 1\tBatch: 14200\tAvg-Loss: 7.3568\n",
            "Time for 50 batch  2702.8095972537994\n",
            "Epoch: 1\tBatch: 14250\tAvg-Loss: 7.4026\n",
            "Time for 50 batch  2712.298567533493\n",
            "Epoch: 1\tBatch: 14300\tAvg-Loss: 7.4125\n",
            "Time for 50 batch  2721.8152589797974\n",
            "Epoch: 1\tBatch: 14350\tAvg-Loss: 7.3691\n",
            "Time for 50 batch  2731.4117081165314\n",
            "Epoch: 1\tBatch: 14400\tAvg-Loss: 7.3975\n",
            "Time for 50 batch  2740.946135044098\n",
            "Epoch: 1\tBatch: 14450\tAvg-Loss: 7.3938\n",
            "Time for 50 batch  2750.4507813453674\n",
            "Epoch: 1\tBatch: 14500\tAvg-Loss: 7.3624\n",
            "Time for 50 batch  2759.9646849632263\n",
            "Epoch: 1\tBatch: 14550\tAvg-Loss: 7.3550\n",
            "Time for 50 batch  2769.482250213623\n",
            "Epoch: 1\tBatch: 14600\tAvg-Loss: 7.3708\n",
            "Time for 50 batch  2778.978224515915\n",
            "Epoch: 1\tBatch: 14650\tAvg-Loss: 7.3904\n",
            "Time for 50 batch  2788.4658682346344\n",
            "Epoch: 1\tBatch: 14700\tAvg-Loss: 7.4141\n",
            "Time for 50 batch  2797.9901254177094\n",
            "Epoch: 1\tBatch: 14750\tAvg-Loss: 7.3265\n",
            "Time for 50 batch  2807.523008108139\n",
            "Epoch: 1\tBatch: 14800\tAvg-Loss: 7.3812\n",
            "Time for 50 batch  2817.0492734909058\n",
            "Epoch: 1\tBatch: 14850\tAvg-Loss: 7.3524\n",
            "Time for 50 batch  2826.571780204773\n",
            "Epoch: 1\tBatch: 14900\tAvg-Loss: 7.3276\n",
            "Time for 50 batch  2836.132079601288\n",
            "Epoch: 1\tBatch: 14950\tAvg-Loss: 7.3359\n",
            "Time for 50 batch  2845.6428356170654\n",
            "Epoch: 1\tBatch: 15000\tAvg-Loss: 7.3322\n",
            "Time for 50 batch  2855.186676979065\n",
            "Epoch: 1\tBatch: 15050\tAvg-Loss: 7.3593\n",
            "Time for 50 batch  2864.72594666481\n",
            "Epoch: 1\tBatch: 15100\tAvg-Loss: 7.3150\n",
            "Time for 50 batch  2874.2627086639404\n",
            "Epoch: 1\tBatch: 15150\tAvg-Loss: 7.3361\n",
            "Time for 50 batch  2883.7356276512146\n",
            "Epoch: 1\tBatch: 15200\tAvg-Loss: 7.3479\n",
            "Time for 50 batch  2893.298638820648\n",
            "Epoch: 1\tBatch: 15250\tAvg-Loss: 7.3346\n",
            "Time for 50 batch  2902.8373296260834\n",
            "Epoch: 1\tBatch: 15300\tAvg-Loss: 7.3530\n",
            "Time for 50 batch  2912.3577089309692\n",
            "Epoch: 1\tBatch: 15350\tAvg-Loss: 7.3407\n",
            "Time for 50 batch  2921.917900800705\n",
            "Epoch: 1\tBatch: 15400\tAvg-Loss: 7.3189\n",
            "Time for 50 batch  2931.4216406345367\n",
            "Epoch: 1\tBatch: 15450\tAvg-Loss: 7.3323\n",
            "Time for 50 batch  2940.9062147140503\n",
            "Epoch: 1\tBatch: 15500\tAvg-Loss: 7.3365\n",
            "Time for 50 batch  2950.428337574005\n",
            "Epoch: 1\tBatch: 15550\tAvg-Loss: 7.3258\n",
            "Time for 50 batch  2959.918858528137\n",
            "Epoch: 1\tBatch: 15600\tAvg-Loss: 7.3688\n",
            "Time for 50 batch  2969.456160545349\n",
            "Epoch: 1\tBatch: 15650\tAvg-Loss: 7.3531\n",
            "Time for 50 batch  2979.005626678467\n",
            "Epoch: 1\tBatch: 15700\tAvg-Loss: 7.3022\n",
            "Time for 50 batch  2988.5447046756744\n",
            "Epoch: 1\tBatch: 15750\tAvg-Loss: 7.3572\n",
            "Time for 50 batch  2998.090392589569\n",
            "Epoch: 1\tBatch: 15800\tAvg-Loss: 7.3316\n",
            "Time for 50 batch  3007.645032644272\n",
            "Epoch: 1\tBatch: 15850\tAvg-Loss: 7.3056\n",
            "Time for 50 batch  3017.1536478996277\n",
            "Epoch: 1\tBatch: 15900\tAvg-Loss: 7.3177\n",
            "Time for 50 batch  3026.6593079566956\n",
            "Epoch: 1\tBatch: 15950\tAvg-Loss: 7.3450\n",
            "Time for 50 batch  3036.226713657379\n",
            "Epoch: 1\tBatch: 16000\tAvg-Loss: 7.3683\n",
            "Time for 50 batch  3045.7421522140503\n",
            "Epoch: 1\tBatch: 16050\tAvg-Loss: 7.3001\n",
            "Time for 50 batch  3055.282105922699\n",
            "Epoch: 1\tBatch: 16100\tAvg-Loss: 7.3350\n",
            "Time for 50 batch  3064.787881374359\n",
            "Epoch: 1\tBatch: 16150\tAvg-Loss: 7.3110\n",
            "Time for 50 batch  3074.3102350234985\n",
            "Epoch: 1\tBatch: 16200\tAvg-Loss: 7.3210\n",
            "Time for 50 batch  3083.8347330093384\n",
            "Epoch: 1\tBatch: 16250\tAvg-Loss: 7.2938\n",
            "Time for 50 batch  3093.3621900081635\n",
            "Epoch: 1\tBatch: 16300\tAvg-Loss: 7.3278\n",
            "Time for 50 batch  3102.905612707138\n",
            "Epoch: 1\tBatch: 16350\tAvg-Loss: 7.2970\n",
            "Time for 50 batch  3112.419337749481\n",
            "Epoch: 1\tBatch: 16400\tAvg-Loss: 7.3489\n",
            "Time for 50 batch  3121.9562425613403\n",
            "Epoch: 1\tBatch: 16450\tAvg-Loss: 7.3291\n",
            "Time for 50 batch  3131.545948266983\n",
            "Epoch: 1\tBatch: 16500\tAvg-Loss: 7.3244\n",
            "Time for 50 batch  3141.0810515880585\n",
            "Epoch: 1\tBatch: 16550\tAvg-Loss: 7.2968\n",
            "Time for 50 batch  3150.5985820293427\n",
            "Epoch: 1\tBatch: 16600\tAvg-Loss: 7.3205\n",
            "Time for 50 batch  3160.0825781822205\n",
            "Epoch: 1\tBatch: 16650\tAvg-Loss: 7.2807\n",
            "Time for 50 batch  3169.6113617420197\n",
            "Epoch: 1\tBatch: 16700\tAvg-Loss: 7.2712\n",
            "Time for 50 batch  3179.132785320282\n",
            "Epoch: 1\tBatch: 16750\tAvg-Loss: 7.2964\n",
            "Time for 50 batch  3188.6525378227234\n",
            "Epoch: 1\tBatch: 16800\tAvg-Loss: 7.3213\n",
            "Time for 50 batch  3198.1800212860107\n",
            "Epoch: 1\tBatch: 16850\tAvg-Loss: 7.2895\n",
            "Time for 50 batch  3207.7015290260315\n",
            "Epoch: 1\tBatch: 16900\tAvg-Loss: 7.3340\n",
            "Time for 50 batch  3217.199723482132\n",
            "Epoch: 1\tBatch: 16950\tAvg-Loss: 7.2690\n",
            "Time for 50 batch  3226.720544576645\n",
            "Epoch: 1\tBatch: 17000\tAvg-Loss: 7.2463\n",
            "Time for 50 batch  3236.2438740730286\n",
            "Epoch: 1\tBatch: 17050\tAvg-Loss: 7.2965\n",
            "Time for 50 batch  3245.781985759735\n",
            "Epoch: 1\tBatch: 17100\tAvg-Loss: 7.3097\n",
            "Time for 50 batch  3255.3284022808075\n",
            "Epoch: 1\tBatch: 17150\tAvg-Loss: 7.2740\n",
            "Time for 50 batch  3264.887828350067\n",
            "Epoch: 1\tBatch: 17200\tAvg-Loss: 7.3077\n",
            "Time for 50 batch  3274.4168000221252\n",
            "Epoch: 1\tBatch: 17250\tAvg-Loss: 7.2355\n",
            "Time for 50 batch  3283.933001279831\n",
            "Epoch: 1\tBatch: 17300\tAvg-Loss: 7.2618\n",
            "Time for 50 batch  3293.454509973526\n",
            "Epoch: 1\tBatch: 17350\tAvg-Loss: 7.2903\n",
            "Time for 50 batch  3302.984545469284\n",
            "Epoch: 1\tBatch: 17400\tAvg-Loss: 7.2787\n",
            "Time for 50 batch  3312.5093433856964\n",
            "Epoch: 1\tBatch: 17450\tAvg-Loss: 7.3001\n",
            "Time for 50 batch  3322.063551425934\n",
            "Epoch: 1\tBatch: 17500\tAvg-Loss: 7.2845\n",
            "Time for 50 batch  3331.581391096115\n",
            "Epoch: 1\tBatch: 17550\tAvg-Loss: 7.2688\n",
            "Time for 50 batch  3341.1283996105194\n",
            "Epoch: 1\tBatch: 17600\tAvg-Loss: 7.2989\n",
            "Time for 50 batch  3350.6345086097717\n",
            "Epoch: 1\tBatch: 17650\tAvg-Loss: 7.2709\n",
            "Time for 50 batch  3360.161137342453\n",
            "Epoch: 1\tBatch: 17700\tAvg-Loss: 7.2774\n",
            "Time for 50 batch  3369.670753955841\n",
            "Epoch: 1\tBatch: 17750\tAvg-Loss: 7.2437\n",
            "Time for 50 batch  3379.190173149109\n",
            "Epoch: 1\tBatch: 17800\tAvg-Loss: 7.2522\n",
            "Time for 50 batch  3388.720046043396\n",
            "Epoch: 1\tBatch: 17850\tAvg-Loss: 7.2757\n",
            "Time for 50 batch  3398.2375934123993\n",
            "Epoch: 1\tBatch: 17900\tAvg-Loss: 7.2346\n",
            "Time for 50 batch  3407.7679195404053\n",
            "Epoch: 1\tBatch: 17950\tAvg-Loss: 7.2440\n",
            "Time for 50 batch  3417.2726180553436\n",
            "Epoch: 1\tBatch: 18000\tAvg-Loss: 7.2568\n",
            "Time for 50 batch  3426.796459197998\n",
            "Epoch: 1\tBatch: 18050\tAvg-Loss: 7.2563\n",
            "Time for 50 batch  3436.323690891266\n",
            "Epoch: 1\tBatch: 18100\tAvg-Loss: 7.2453\n",
            "Time for 50 batch  3445.9048643112183\n",
            "Epoch: 1\tBatch: 18150\tAvg-Loss: 7.2658\n",
            "Time for 50 batch  3455.3971898555756\n",
            "Epoch: 1\tBatch: 18200\tAvg-Loss: 7.2493\n",
            "Time for 50 batch  3464.9342455863953\n",
            "Epoch: 1\tBatch: 18250\tAvg-Loss: 7.2334\n",
            "Time for 50 batch  3474.473955631256\n",
            "Epoch: 1\tBatch: 18300\tAvg-Loss: 7.2211\n",
            "Time for 50 batch  3483.967410326004\n",
            "Epoch: 1\tBatch: 18350\tAvg-Loss: 7.2011\n",
            "Time for 50 batch  3493.4746997356415\n",
            "Epoch: 1\tBatch: 18400\tAvg-Loss: 7.2671\n",
            "Time for 50 batch  3502.939130783081\n",
            "Epoch: 1\tBatch: 18450\tAvg-Loss: 7.2571\n",
            "Time for 50 batch  3512.4687175750732\n",
            "Epoch: 1\tBatch: 18500\tAvg-Loss: 7.2738\n",
            "Time for 50 batch  3522.011864900589\n",
            "Epoch: 1\tBatch: 18550\tAvg-Loss: 7.2241\n",
            "Time for 50 batch  3531.5166351795197\n",
            "Epoch: 1\tBatch: 18600\tAvg-Loss: 7.2296\n",
            "Time for 50 batch  3541.037078857422\n",
            "Epoch: 1\tBatch: 18650\tAvg-Loss: 7.2192\n",
            "Time for 50 batch  3550.5845408439636\n",
            "Epoch: 1\tBatch: 18700\tAvg-Loss: 7.2253\n",
            "Time for 50 batch  3560.0936937332153\n",
            "Epoch: 1\tBatch: 18750\tAvg-Loss: 7.1996\n",
            "Time for 50 batch  3569.6124942302704\n",
            "Epoch: 1\tBatch: 18800\tAvg-Loss: 7.2605\n",
            "Time for 50 batch  3579.1199791431427\n",
            "Epoch: 1\tBatch: 18850\tAvg-Loss: 7.2332\n",
            "Time for 50 batch  3588.6407539844513\n",
            "Epoch: 1\tBatch: 18900\tAvg-Loss: 7.2239\n",
            "Time for 50 batch  3598.116760492325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxNJSvgun8BV",
        "colab_type": "text"
      },
      "source": [
        "## Resume Training .... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAkIVn7bsbGN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resume learning\n",
        "closs_weight = 1\n",
        "lr_cent = 0.5\n",
        "feat_dim = 100\n",
        "\n",
        "num_feats = 3\n",
        "hidden_sizes1 = [128, 256, 512]\n",
        "hidden_sizes2 = [512, 1024, 2048]\n",
        "reps = [4,6,3]\n",
        "\n",
        "my_model = Network(num_feats, hidden_sizes1, hidden_sizes2, reps, num_classes, feat_dim)\n",
        "\n",
        "# load model\n",
        "checkpoint = torch.load('./models/classifier_trial2_15.pt')\n",
        "my_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# load optimizer\n",
        "criterion_label = nn.CrossEntropyLoss()\n",
        "criterion_closs = CenterLoss(num_classes, feat_dim, device)\n",
        "optimizer_label = torch.optim.Adam(my_model.parameters(), amsgrad=True, lr=0.008)\n",
        "optimizer_closs = torch.optim.Adam(criterion_closs.parameters(), amsgrad=True, lr=0.4)\n",
        "# optimizer_label.load_state_dict(checkpoint['optimizer_state_dict'][1])\n",
        "# optimizer_closs.load_state_dict(checkpoint['optimizer_state_dict'][0])\n",
        "\n",
        "# retrain\n",
        "my_model.to(device)\n",
        "my_model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zCTrrnyMxJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_closs(optimizer_closs, optimizer_label, my_model, train_loader, validation_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjUiDhmtKRU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_classify_closs(my_model, validation_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PlfncVk5A5W",
        "colab_type": "text"
      },
      "source": [
        "## Triplet Loss\n",
        "___\n",
        "You can make a dataloader that returns a tuple of three images. Two being from the same class and one from a different class. You can then use triplet loss to seperate out the different class pair distance and decrease same class pair distance.\n",
        "\n",
        "More on this link: https://github.com/adambielski/siamese-triplet/blob/master/losses.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJjvsZ2z5A5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "face_img1, label_img1 = trainset.__getitem__(0)\n",
        "face_img2, label_img2 = trainset.__getitem__(1)\n",
        "face_img3, label_img3 = trainset.__getitem__(-1)\n",
        "\n",
        "print(label_img1, label_img2, label_img3)\n",
        "## face_img1 and face_img2 are from the same class and face_img3 is from a different class.\n",
        "loss = triplet_loss(face_img1, face_img2, face_img3)\n",
        "print (\"Loss={:0.2f}\".format(loss))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}