{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW4P2_revised-2-Copy1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Environment (conda_pytorch_p36)",
      "language": "python",
      "name": "conda_pytorch_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2pldSzIKG7bo"
      },
      "source": [
        "# Encoder-decoder for sequence-to-sequence tasks\n",
        "\n",
        "In this tutorial, we will walk through a simple example of encoder-decoder with attention mechanism.\n",
        "\n",
        "This tutorial focuses on implementation. For basic concepts, refer to slides for lectures and recitations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ur0DiwaHG7bq"
      },
      "source": [
        "## Toy task: English spelling to pronunciation\n",
        "\n",
        "We consider the same toy task as the last recitation, which is predicting the pronunciation (as sequence of phonemes) of an English word given its spelling.\n",
        "\n",
        "The model architecture and hyperparameters are for demonstration purpose only. Do not copy them to your actual homework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPYymKKsEhzn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "82892097-8e93-47c7-ae90-cc865e451efb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4_geuS6tIPlm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01dfce21-fdcb-4084-a95a-366c65fee196"
      },
      "source": [
        "# !nvidia-smi\n",
        "\n",
        "print(\"changed again 2\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "changed again 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZHjnr4U8JKIm",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import *\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE\n",
        "\n",
        "HIDDEN_SIZE = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "58qza_udJYhv",
        "colab": {}
      },
      "source": [
        "# loading test data\n",
        "# with ZipFile(\"/content/gdrive/My Drive/Kaggle/test_new.npy.zip\") as f:\n",
        "#     f.extractall()\n",
        "test_data = np.load(\"/content/drive/My Drive/Kaggle/test_new.npy\",allow_pickle=True,encoding='bytes')\n",
        "\n",
        "# loading dev data\n",
        "# with ZipFile(\"/content/gdrive/My Drive/Kaggle/dev_new.npy.zip\") as f:\n",
        "#     f.extractall()\n",
        "dev_data = np.load(\"/content/drive/My Drive/Kaggle/dev_new.npy\",allow_pickle=True,encoding='bytes')\n",
        "dev_labels = np.load(\"/content/drive/My Drive/Kaggle/dev_transcripts.npy\",allow_pickle=True,encoding='bytes')\n",
        "\n",
        "# loading training data\n",
        "# with ZipFile(\"/content/gdrive/My Drive/Kaggle/train_new.npy.zip\") as f:\n",
        "#     f.extractall()\n",
        "train_data = np.load(\"/content/drive/My Drive/Kaggle/train_new.npy\",allow_pickle=True,encoding='bytes')\n",
        "# with ZipFile(\"/content/gdrive/My Drive/Kaggle/train_transcripts.npy.zip\") as f:\n",
        "#     f.extractall()\n",
        "train_labels = np.load(\"/content/drive/My Drive/Kaggle/train_transcripts.npy\",allow_pickle=True,encoding='bytes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jZ-PzdoKKItB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b687420d-8950-4195-ff98-facd560e9764"
      },
      "source": [
        "# check the info of datasets\n",
        "print(\"The sample size of train & train_labels is: \",train_data.shape)\n",
        "print(\"The sample size of dev & dev_labels is: \",dev_data.shape)\n",
        "print('')\n",
        "\n",
        "# check samples for dataset\n",
        "print(\"The sample input for training set is: \\n\",train_data[0].shape)\n",
        "print(\"The sample output for training set is: \\n\",train_labels[0].shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sample size of train & train_labels is:  (24724,)\n",
            "The sample size of dev & dev_labels is:  (1106,)\n",
            "\n",
            "The sample input for training set is: \n",
            " (477, 40)\n",
            "The sample output for training set is: \n",
            " (14,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O1mSTzs9OPhv"
      },
      "source": [
        "## steps for data preprocessing (word level)\n",
        "\n",
        "1. Insert \\<s> and \\</s> before & after a sentence  \n",
        "2. Pack & padded the sequence before data loader\n",
        "3. How to construct a words list? (what if words never seen before), the same words list will also applied in dev\n",
        "4. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lSNLdwEyG7b0"
      },
      "source": [
        "Every target sequence is prepended with `<s>` and appended with `</s>`. This is necessary for the decoder to predict the first token and the end of the sequence.\n",
        "\n",
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c6mukJdJUW7d",
        "colab": {}
      },
      "source": [
        "LETTER_LIST = ['<pad>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \\\n",
        "               'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']\n",
        "               \n",
        "def create_dictionaries(letter_list):\n",
        "    letter2index = dict()\n",
        "    index2letter = dict()\n",
        "    for i in range(len(letter_list)):\n",
        "        letter2index[letter_list[i]] = i\n",
        "        index2letter[i] = letter_list[i]\n",
        "    return letter2index, index2letter\n",
        "\n",
        "def transform_letter_to_index(transcript, letter_list):\n",
        "    '''\n",
        "    :param transcript :(N, ) Transcripts are the text input\n",
        "    :param letter_list: Letter list defined above\n",
        "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
        "    '''\n",
        "    letter2index, index2letter = create_dictionaries(letter_list)\n",
        "    letter_to_index_list = [torch.LongTensor([letter2index[p] for p in (['<sos>'] + list(\" \".join(item.astype(str))) + ['<eos>'])]) for item in transcript]\n",
        "\n",
        "    return letter_to_index_list\n",
        "\n",
        "def decode_sentence(predictions, letter_list):\n",
        "    '''\n",
        "    :param predictions: (total_batch_size, max_lens, vocab_size)\n",
        "    :return decoded_list: list of strings\n",
        "    '''\n",
        "\n",
        "    _, index2letter = create_dictionaries(letter_list)\n",
        "    decoded_list = []\n",
        "    # loop the dataset\n",
        "    num = predictions.shape[0]\n",
        "    for i in range(num):\n",
        "        temp = predictions[i,:,:] # (max_len, vocab_size)\n",
        "\n",
        "        _,indices = torch.max(temp, dim=1) # (max_len)\n",
        "\n",
        "        temp_list = []\n",
        "\n",
        "        for i in indices:\n",
        "            # if reach <eos>, early break\n",
        "            if index2letter[i.item()] == '<eos>':\n",
        "                break\n",
        "            else:\n",
        "                # stored in temp_list\n",
        "                temp_list.append(index2letter[i.item()])\n",
        "        # add temp_list\n",
        "#         print((\"\".join(temp_list)))\n",
        "        decoded_list.append(\"\".join(temp_list))\n",
        "\n",
        "    return decoded_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Y93qnQlDIXL",
        "colab": {}
      },
      "source": [
        "# decoded_list = decode_sentence(predictions, LETTER_LIST)\n",
        "# print(len(decoded_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-kbHjJSEo__N"
      },
      "source": [
        "## Speech2TextDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZgGA5-JdTn72",
        "colab": {}
      },
      "source": [
        "def normalize(x, m, s): return (x-m)/s\n",
        "def normalize_to(train):\n",
        "    m,s = train.mean(axis = 0),train.std(axis = 0)\n",
        "    return normalize(train, m, s)\n",
        "\n",
        "\n",
        "class Speech2TextDataset(Dataset):\n",
        "    def __init__(self,data, labels, isTrain = True):\n",
        "        # apply normalization on features\n",
        "        self.X = [torch.tensor([c for c in word]) for word in data]\n",
        "        self.X_lens = torch.LongTensor([len(seq) for seq in self.X])\n",
        "        self.isTrain = isTrain\n",
        "        if isTrain:\n",
        "            self.Y = transform_letter_to_index(labels, LETTER_LIST)     \n",
        "            self.Y_lens = torch.LongTensor([len(seq) for seq in self.Y])\n",
        "        del data\n",
        "        del labels\n",
        "        \n",
        "    def __getitem__(self,index):\n",
        "        if self.isTrain:\n",
        "            return self.X[index], self.X_lens[index], self.Y[index], self.Y_lens[index]\n",
        "        else:\n",
        "            return self.X[index], self.X_lens[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "#### when use pad_sequence or pack_padded_sequence, remember the batch_first=True ####\n",
        "#### this is mandantary when using CTCLoss\n",
        "\n",
        "def my_collate_train(batch):\n",
        "    data = [normalize_to(item[0]) for item in batch]\n",
        "    target = [item[2] for item in batch]\n",
        "    data_lens = torch.LongTensor([seq[1] for seq in batch])\n",
        "    target_lens = torch.LongTensor([seq[3] for seq in batch])\n",
        "    data = pad_sequence(data)\n",
        "    target = pad_sequence(target,batch_first=True)\n",
        "    # del batch\n",
        "    return data, data_lens, target, target_lens\n",
        "\n",
        "def my_collate_test(batch):\n",
        "    data = [normalize_to(item[0]) for item in batch]\n",
        "    data_lens = torch.LongTensor([seq[1] for seq in batch])\n",
        "    data = pad_sequence(data)\n",
        "    # del batch\n",
        "    return data, data_lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QQx4vpULUKDx",
        "colab": {}
      },
      "source": [
        "# loading training data\n",
        "train_dataset = Speech2TextDataset(train_data,train_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=32,shuffle=True,collate_fn=my_collate_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DiV46jEmUySn",
        "colab": {}
      },
      "source": [
        "# loading dev data\n",
        "dev_dataset = Speech2TextDataset(dev_data,dev_labels)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset,batch_size=32, shuffle=True, collate_fn=my_collate_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HEu0foIScl9U",
        "colab": {}
      },
      "source": [
        "# loading test data\n",
        "test_dataset = Speech2TextDataset(test_data,None,isTrain=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle=False, collate_fn = my_collate_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEKcG71lEUcv",
        "colab_type": "text"
      },
      "source": [
        "## Locked Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5xcnrPqDGH6P",
        "colab": {}
      },
      "source": [
        "# https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5\n",
        "class LockedDropout(nn.Module):\n",
        "    \"\"\" LockedDropout applies the same dropout mask to every time step.\n",
        "\n",
        "    **Thank you** to Sales Force for their initial implementation of :class:`WeightDrop`. Here is\n",
        "    their `License\n",
        "    <https://github.com/salesforce/awd-lstm-lm/blob/master/LICENSE>`__.\n",
        "\n",
        "    Args:\n",
        "        p (float): Probability of an element in the dropout mask to be zeroed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p=0.9):\n",
        "        self.p = p\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (:class:`torch.FloatTensor` [sequence length, batch size, rnn hidden size]): Input to\n",
        "                apply dropout too.\n",
        "        \"\"\"\n",
        "        if not self.training or not self.p:\n",
        "            return x\n",
        "        x = x.clone()\n",
        "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.p)\n",
        "        mask = mask.div_(1 - self.p)\n",
        "        mask = mask.expand_as(x)\n",
        "        return x * mask\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(' \\\n",
        "            + 'p=' + str(self.p) + ')'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gO3RTgShNhwQ"
      },
      "source": [
        "## pBLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UkDwekoONhB8",
        "colab": {}
      },
      "source": [
        "class pBLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size // 2, bidirectional = True)\n",
        "    \n",
        "    def forward(self, X, lengths):\n",
        "        \"\"\"\n",
        "        :param X: (max_len, batch_size, input_size), input sequences\n",
        "        :returns: encoded_input sequences (max_len // 2, batch_size, hidden_size * 2)\n",
        "        \"\"\"\n",
        "\n",
        "        X = pack_padded_sequence(X, lengths, enforce_sorted=False)\n",
        "        out, state = self.lstm(X) # (max_len, batch, hidden_size)\n",
        "        out, out_lens = pad_packed_sequence(out)\n",
        "        max_len, batch_size, hidden_size = out.shape\n",
        "\n",
        "        # if max len is odd\n",
        "        if max_len % 2 == 1:\n",
        "            # padding\n",
        "            pad = torch.zeros((1, batch_size, hidden_size),dtype = torch.float,device = DEVICE)\n",
        "\n",
        "            # concatenate\n",
        "            out = torch.cat((out,pad),0)\n",
        "            out = out.transpose(0,1)\n",
        "\n",
        "            # reshape\n",
        "            out = out.reshape(batch_size, (max_len + 1)// 2, hidden_size * 2)\n",
        "            out = out.transpose(0,1)\n",
        "            \n",
        "\n",
        "        # if max len is even\n",
        "        else:\n",
        "            \n",
        "            out = out.transpose(0,1)\n",
        "            out = out.contiguous().view(batch_size, max_len // 2, hidden_size * 2)\n",
        "            out = out.transpose(0,1)\n",
        "\n",
        "        # calculate the return length\n",
        "        rLens = torch.zeros(out_lens.shape,device=DEVICE)\n",
        "\n",
        "        # 2. keep track of the lengths size\n",
        "        for i in range(len(out_lens)):\n",
        "            if out_lens[i] % 2 == 1:\n",
        "                rLens[i] = (out_lens[i] + 1) // 2\n",
        "            else:\n",
        "                rLens[i] = out_lens[i] // 2\n",
        "\n",
        "        return out, rLens\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iXnKIt3uG7b6"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "The encoder is just a one-layer bi-directional LSTM.\n",
        "The encoder returns not only the encoded sequence, but also the final hidden state of the LSTM, which will be the initial hidden state of the decoder.\n",
        "\n",
        "notation: `batch_size` is the batch size, `max_len` is the maximum input sequence length, `hidden_size` is the hidden size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Ne15XBFG7b7",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, key_dim=HIDDEN_SIZE, value_dim=HIDDEN_SIZE):\n",
        "        super(Encoder, self).__init__()\n",
        "                \n",
        "        # the default \"batch_first\" for LSTM is False \n",
        "        # because the bidirectional=True, the final output dimension will be hidden_size\n",
        "        self.lstm1 = pBLSTM(input_size, hidden_size)\n",
        "        self.lstm2 = pBLSTM(hidden_size*2, hidden_size *2)\n",
        "        self.lstm3 = pBLSTM(hidden_size*4, hidden_size *2)\n",
        "        \n",
        "        # locked dropout\n",
        "        self.drop = LockedDropout()\n",
        "        \n",
        "        # linear output for key\n",
        "        self.KeyLinear = nn.Linear(hidden_size * 4, key_dim, bias = False)\n",
        "\n",
        "        # linear output for value\n",
        "        self.ValueLinear = nn.Linear(hidden_size * 4, value_dim, bias = False)\n",
        "        \n",
        "\n",
        "    def forward(self, X, lengths):\n",
        "        \"\"\"\n",
        "        :param X: (max_len, batch_size, hidden_size), input sequences\n",
        "        :param lengths: (batch_size, ), lengths of input sequences\n",
        "        :returns: key: (batch_size, max_len // 2, key_dim)\n",
        "                  value: (batch_size, max_len // 2, value_dim)\n",
        "        \"\"\" \n",
        "\n",
        "        # Initialize inputs\n",
        "        out = X\n",
        "        lens = lengths\n",
        "\n",
        "\n",
        "        # Three pBLSTM layers\n",
        "        # for i in range(3):    \n",
        "        out,lens = self.lstm1(out,lens) # out: (max_len // 2, batch_size, hidden_size * 2)\n",
        "        out = self.drop(out)\n",
        "        out,lens = self.lstm2(out,lens) # out: (max_len // 4, batch_size, hidden_size * 4)\n",
        "        out = self.drop(out)\n",
        "        out,lens = self.lstm3(out,lens) # out: (max_len // 8, batch_size, hidden_size * 8)\n",
        "\n",
        "        # Linear layers\n",
        "        key = self.KeyLinear(out)\n",
        "        value = self.ValueLinear(out)\n",
        "\n",
        "        ######## do I need the state? #########              \n",
        "        # final_state is a tuple, containing hidden_state and cell_state \n",
        "\n",
        "        # transpose before return\n",
        "        key = key.transpose(0,1)\n",
        "        value = value.transpose(0,1)\n",
        "        # return (key, value) for attention model\n",
        "        # instead of giving some output, will just give out the key & value pair\n",
        "        return key, value, lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M32ZDcSuG7cA"
      },
      "source": [
        "## Attention\n",
        "\n",
        "This the simplest \"dot product\" attention, meaning that every attention logit is the dot product of a target (`query`) vector and a source (`context`) vector. Such an attention model has no parameter, but you may use more advanced attention mechanism with learnable parameters in real world.\n",
        "\n",
        "Since there are a lot of uncommon operators, every line of code is annotated with input and output tensor sizes. \n",
        "\n",
        "The attention vectors are not used by other computations, but are returned for visualization. You will often want to visualize the attention matrix when debugging sequence-to-sequence models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nl0IykzCG7cB",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key, value and query from Encoder and decoder.\n",
        "    Below are the set of operations you need to perform for computing attention:\n",
        "        energy = bmm(key, query)\n",
        "        attention = softmax(energy)\n",
        "        context = bmm(attention, value)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "\n",
        "    def forward(self, query, key, value, lens):\n",
        "        '''\n",
        "        :param query :(N, context_size) Query is the output of LSTMCell from Decoder\n",
        "        :param key: (N, key_size) Key Projection from Encoder per time step\n",
        "        :param value: (N, value_size) Value Projection from Encoder per time step\n",
        "        :param lens: (N,) lens for each data sample\n",
        "        :return output: Attended Context\n",
        "        :return attention_mask: Attention mask that can be plotted  \n",
        "        '''\n",
        "        # key, value is (N, key_size), (N, value_size) because partial of it is input\n",
        "\n",
        "        \"\"\"\n",
        "        energy = bmm(key, query)\n",
        "        attention = softmax(energy)\n",
        "        context = bmm(attention, value)\n",
        "        \"\"\"\n",
        "        # key: (batch_size, lens, key_size)\n",
        "        # value: (batch_size, lens, value_size)\n",
        "        # query: (batch_size, key_size)\n",
        "        # energy = bmm(key, query)\n",
        "        attention = torch.bmm(key,query.unsqueeze(2)).squeeze(2) # (batch_size, lens)\n",
        "\n",
        "        # attention = softmax(energy)\n",
        "        mask = torch.arange(key.shape[1],device=DEVICE).unsqueeze(0) >= lens.unsqueeze(1)\n",
        "        attention.masked_fill_(mask, 0) # (batch_size, lens)\n",
        "\n",
        "        attention = nn.functional.softmax(attention, dim=1)\n",
        "\n",
        "        # context = bmm(attention, value)\n",
        "        context = torch.bmm(attention.unsqueeze(1), value).squeeze(1) # (batch_size, value_size)\n",
        "\n",
        "        return context, attention # context: (batch_size, value_size)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZoktS2A5f2Il",
        "colab": {}
      },
      "source": [
        "# test attention model\n",
        "\n",
        "# batch_size = 5\n",
        "# timesteps = 4\n",
        "# key_size = 3\n",
        "# value_size = 3\n",
        "\n",
        "# query = torch.ones(batch_size, key_size)\n",
        "# key = torch.ones(batch_size, timesteps, key_size)\n",
        "# value = torch.ones( batch_size, timesteps,value_size)\n",
        "# lens = torch.ones(batch_size)*4\n",
        "# query, key, value, lens = query.to(DEVICE), key.to(DEVICE), value.to(DEVICE), lens.to(DEVICE)\n",
        "# result = Attention()(query, key, value, lens)\n",
        "# print(\"**Example Expected:**\\n{result}\",result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SGIR9vk_G7cE"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "Each `forward` call of decoder deals with only one timestep.\n",
        "\n",
        "Here, we use the LSTM output as the query of attention and concatenate the attended context with the LSTM output. There are many other (better) ways to use attention context in the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WAJd9c_uiWih",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# import torch.distributions.gumbel.Gumbel\n",
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, \n",
        "    thus we use LSTMCell instead of LSLTM here.\n",
        "    The output from the second LSTMCell can be used as query here for attention module.\n",
        "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, hidden_dim, value_size=HIDDEN_SIZE, key_size=HIDDEN_SIZE, isAttended=True):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
        "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim) # hidden_dim = 128\n",
        "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
        "\n",
        "        self.isAttended = isAttended\n",
        "        if (isAttended == True):\n",
        "            self.attention = Attention()\n",
        "\n",
        "        self.character_prob = nn.Linear(key_size + value_size, vocab_size)\n",
        "\n",
        "        self.hidden_size = hidden_dim\n",
        "\n",
        "    def forward(self, key, values, rLens,rate, text=None,  isTrain=True, isVal = False):\n",
        "        '''\n",
        "        :param key :(T, N, key_size) Output of the Encoder Key projection layer\n",
        "        :param values: (T, N, value_size) Output of the Encoder Value projection layer\n",
        "        :param text: (N, text_len) Batch input of text with text_length\n",
        "        :param isTrain: Train or eval mode\n",
        "        :return predictions: Returns the character perdiction probability \n",
        "        '''\n",
        "        batch_size = key.shape[0]\n",
        "        if (isTrain == True):\n",
        "            max_len =  text.shape[1] # max_len = text_len\n",
        "            embeddings = self.embedding(text) # text is Y, with size (batch_size, text_len, hidden_size)\n",
        "        elif isVal:\n",
        "#             print(\"get is Val\")\n",
        "            max_len = text.shape[1]        \n",
        "        else:\n",
        "            # if prediction, then set a max_len\n",
        "            max_len = 250\n",
        "\n",
        "        predictions = []\n",
        "        hidden_states = [None, None]\n",
        "        prediction = torch.ones(batch_size,1).to(DEVICE) * 33\n",
        "        all_attention = []\n",
        "\n",
        "        # if attention, init context\n",
        "        if self.isAttended:\n",
        "            context = torch.zeros((batch_size,self.hidden_size),device = DEVICE)\n",
        "\n",
        "        # use max_len - 1 because we want to ignore eos\n",
        "        for i in range(max_len-1):\n",
        "            # * Implement Gumble noise and teacher forcing techniques \n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
        "            # * If you haven't implemented attention yet, then you may want to check the index and break \n",
        "            #   out of the loop so you do you do not get index out of range errors. \n",
        "\n",
        "            ############# SET INPUT #################\n",
        "\n",
        "            if (isTrain):\n",
        "\n",
        "                tf_rate = torch.tensor(np.random.binomial(1,rate,size=(batch_size,1)),device=DEVICE)\n",
        "                pred = self.embedding(prediction.argmax(dim=-1))\n",
        "\n",
        "                m = torch.distributions.gumbel.Gumbel(prediction, 0.1)\n",
        "                prediction = m.sample()\n",
        "                prediction = self.embedding(prediction.argmax(dim=-1))\n",
        "\n",
        "                char_embed =  tf_rate * embeddings[:, i, :] + (1-tf_rate) * prediction           \n",
        "         \n",
        "\n",
        "                \n",
        "            else:\n",
        "#                 print(\" ****** got into isTrain = False mode ********\")\n",
        "#                 m = torch.distributions.gumbel.Gumbel(prediction, 0.1)\n",
        "#                 prediction = m.sample()\n",
        "\n",
        "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "            \n",
        "            ############## LSTM layers ##############\n",
        "          \n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
        "            if self.isAttended:\n",
        "\n",
        "                # if isTrain:\n",
        "                inp = torch.cat([char_embed, context], dim=1) # (batch_size, hidden_dim + value_size)\n",
        "                hidden_states[0] = self.lstm1(inp, hidden_states[0]) # (batch_size, 128)\n",
        "\n",
        "                context,attention = self.attention(hidden_states[0][0], key, values, rLens) # (batch_size, value_size)\n",
        "\n",
        "            else:\n",
        "                # print(\" Got into isAttended = False mode\")\n",
        "\n",
        "                inp = torch.cat([char_embed, values[i,:,:]], dim=1)\n",
        "                hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
        "            \n",
        "            # hidden_states[0] = (output, state)\n",
        "            # hidden_states[0][0] = output from lstm1\n",
        "\n",
        "            # inp_2 = hidden_states[0][0]\n",
        "            inp_2 = context\n",
        "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
        "\n",
        "            ### Compute attention from the output of the second LSTM Cell ###\n",
        "            output = hidden_states[1][0] # (batch_size,hidden_dim)\n",
        "\n",
        "            ############## Linear Layers ##############\n",
        "\n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention. \n",
        "            if self.isAttended:\n",
        "                # print(\" is attended True in linear layers .........\")\n",
        "                prediction = self.character_prob(torch.cat([output, context], dim=1)) \n",
        "            else:\n",
        "                prediction = self.character_prob(torch.cat([output, values[i,:,:]], dim=1)) # (batch_size, vocab_size)\n",
        "\n",
        "            # predictions store the value from prediction\n",
        "            predictions.append(prediction.unsqueeze(1))\n",
        "\n",
        "        return torch.cat(predictions, dim=1) # (batch_size, max_len, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "alSZQEY7lbim"
      },
      "source": [
        "## Seq2Seq Model\n",
        "Integrate all parts of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oso7cMiClZxF",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size, hidden_dim, value_size=HIDDEN_SIZE, key_size=HIDDEN_SIZE, isAttended=False):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim)\n",
        "\n",
        "    def forward(self, speech_input, speech_len, text_input=None,rate = 0.9,isTrain=True,isVal = False):\n",
        "        key, value, rLens = self.encoder(speech_input, speech_len)\n",
        "        # if in Train mode, then provide text_input\n",
        "        if (isTrain == True):\n",
        "            predictions = self.decoder(key, value,  rLens, rate,text_input)\n",
        "            \n",
        "        elif (isVal == True):\n",
        "#             print(\"**** get is val *********\")\n",
        "            predictions = self.decoder(key, value, rLens, rate, text_input,  isTrain=False, isVal = True)\n",
        "\n",
        "        # if not in Train mode, then not provide text, generate of value is bounded by max_len\n",
        "        else:\n",
        "            predictions = self.decoder(key, value,  rLens,rate , text=None,  isTrain=False)\n",
        "        return predictions # (batch_size, max_len, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ba4BhVUz5ug9"
      },
      "source": [
        "## Training and Testing\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQq7UOSaEUdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3a973af6-57a4-446d-a409-6ad660a7c40d"
      },
      "source": [
        "!pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.6/dist-packages (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko4_9XJqEUdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkWZODRDEUdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(Y, Y_lens, letter_list):\n",
        "    rList = []\n",
        "    letter2index, index2letter = create_dictionaries(letter_list)\n",
        "    for i in Y:\n",
        "        rList.append(index2letter[i.item()])\n",
        "    return \"\".join(rList[1:Y_lens-1])\n",
        "\n",
        "def LevenshteinDistance(prediction, Y, Y_lens):\n",
        "    '''\n",
        "    :param prediction: prediction matrix (batch_size, max_lens)\n",
        "    :param Y: real input (batch_size, max_lens)\n",
        "    :param Y_lens: real input lens (batch_size,)\n",
        "    :return distance: average Levenshtein Distance (scalar)\n",
        "    '''\n",
        "    distance = 0\n",
        "    decoded_list = decode_sentence(prediction.transpose(1,2),LETTER_LIST)\n",
        "    length = len(decoded_list) \n",
        "\n",
        "    for i in range(length):\n",
        "        reference = translate(Y[i], Y_lens[i], LETTER_LIST)\n",
        "        distance += Levenshtein.distance(reference, decoded_list[i])\n",
        "    \n",
        "    del prediction\n",
        "    del Y\n",
        "    del Y_lens\n",
        "    \n",
        "    return distance / length\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RrIB2Vnl5dbK",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(model, train_loader, criterion, optimizer, epoch):\n",
        "    # model is seq2seq model\n",
        "    model.train()\n",
        "    model.to(DEVICE)\n",
        "    start_epoch = time.time()\n",
        "    start = time.time()\n",
        "    batch_id = 0\n",
        "    # 1) Iterate through your loader\n",
        "    print(\"Learning rate for epoch \", epoch, \" is\", optimizer.param_groups[0]['lr'])\n",
        "    \n",
        "    # #################### remove later #############################\n",
        "    # count = -1\n",
        "    # #################### remove later #############################\n",
        "\n",
        "\n",
        "    for X, X_lens, Y, Y_lens in train_loader:\n",
        "\n",
        "        # #################### remove later #############################\n",
        "        # count += 1\n",
        "        # if batch_id == 100:\n",
        "        #     return model\n",
        "        ################### remove later #############################\n",
        "\n",
        "        # 2) Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion\n",
        "#         with torch.autograd.set_detect_anomaly(True):\n",
        "\n",
        "        # 3) Set the inputs to the device.\n",
        "        X, X_lens, Y, Y_lens = X.to(DEVICE), X_lens.to(DEVICE), Y.to(DEVICE), Y_lens.to(DEVICE)\n",
        "\n",
        "        # 4) Pass your inputs, and length of speech into the model.\n",
        "        \n",
        "        # if epoch <= 15:\n",
        "        #     predictions = model(X, X_lens, Y, rate=0.9)\n",
        "        # elif epoch >=15 and epoch <25:\n",
        "        #     predictions = model(X, X_lens, Y, rate=0.8) # (batch_size, max_len_Y, vocab_size)\n",
        "        # elif epoch >= 25 and epoch < 35:\n",
        "        #     predictions = model(X, X_lens, Y, rate=0.7)\n",
        "        # elif epoch >= 35:\n",
        "        predictions = model(X, X_lens, Y, rate=0.6)\n",
        "            \n",
        "        predictions = predictions.transpose(1,2) # (batch_size, vocab_size, max_len_Y)\n",
        "\n",
        "        batch_size = predictions.shape[0]\n",
        "\n",
        "        # 5) Generate a mask based on the lengths of the text to create a masked loss. \n",
        "        # 5.1) Ensure the mask is on the device and is the correct shape.\n",
        "        lens  =  Y_lens - 1\n",
        "        label_mask = torch.arange(predictions.size(2),device=DEVICE).unsqueeze(0) >= lens.unsqueeze(1) # (batch_size, max_len_Y)\n",
        "\n",
        "        # predictions.masked_fill_(label_mask.unsqueeze(1),0) # (batch_size, vocab_size, max_len_Y)\n",
        "\n",
        "        # change the predictions a little bit\n",
        "        loss = criterion(predictions,Y[:,1:])\n",
        "\n",
        "        _, indices = torch.max(predictions, 1)\n",
        "\n",
        "\n",
        "        # 8) Use the mask to calculate a masked loss.\n",
        "\n",
        "        # sum the loss\n",
        "        n_tokens = lens.sum()\n",
        "        loss = loss.sum() / n_tokens\n",
        "\n",
        "        # 9) Run the backward pass on the masked loss. \n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()\n",
        "\n",
        "        # 10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "#             torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "        # 11) Take a step with your optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # calculate the perplexity\n",
        "        avg_loss = loss\n",
        "        perplexity  = torch.exp(avg_loss)\n",
        "\n",
        "        # test\n",
        "        # test(model, dev_loader, 0)\n",
        "\n",
        "        # 13) Optionally print the training loss after every N batches\n",
        "        if batch_id % 10 == 0:\n",
        "            print(\"Avg 10 batchs takes \",round(( time.time() - start)/60,2),\" min\")\n",
        "            print('EPOCH ',epoch, 'batch ',batch_id, ': Loss:', round(avg_loss.item(),4), 'Perplexity:', round(perplexity.item(),4))\n",
        "\n",
        "        # inspect the Levenshtein Distance of output\n",
        "        if batch_id % 300 == 0:\n",
        "            avg_distance = LevenshteinDistance(predictions, Y, Y_lens)\n",
        "            print(\" ********* Epoch \", epoch, \" Average Levenshtein Distance is ****: \", avg_distance)\n",
        "\n",
        "\n",
        "        batch_id += 1\n",
        "        del X\n",
        "        del X_lens\n",
        "        del Y\n",
        "        del Y_lens\n",
        "        del predictions\n",
        "        del label_mask\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Training loss after one epoch is:\", loss.item())\n",
        "    print(\"Time take for an epoch is:\", round((end - start_epoch)/60,2), \" min\")\n",
        "\n",
        "def test(model, test_loader,criterion, epoch):\n",
        "    # if model.is_cuda == False:\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    avg_loss = 0\n",
        "    avg_distance = 0\n",
        "    count = 0\n",
        "    # criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "    for X, X_lens, Y, Y_lens in test_loader:\n",
        "        X, X_lens, Y, Y_lens = X.to(DEVICE), X_lens.to(DEVICE), Y.to(DEVICE), Y_lens.to(DEVICE)\n",
        "        \n",
        "        # use inference mode for prediction\n",
        "        predictions = model(X, X_lens, Y, isTrain = False, isVal = True) # (batch_size, max_len_Y, vocab_size)    \n",
        "        predictions = predictions.transpose(1,2) # (batch_size, vocab_size, max_len_Y)\n",
        "        \n",
        "        # slice with the max_len\n",
        "#         max_len = torch.max(Y_lens).item()\n",
        "#         predictions = predictions[:,:,:max_len-1]\n",
        "\n",
        "        lens  =  Y_lens - 1\n",
        "        label_mask = torch.arange(predictions.size(2),device=DEVICE).unsqueeze(0) >= lens.unsqueeze(1)\n",
        "        loss = criterion(predictions,Y[:,1:])\n",
        "\n",
        "        # mask loss\n",
        "        loss.masked_fill_(label_mask, 0)\n",
        "\n",
        "        # sum the loss\n",
        "        n_tokens = lens.sum()\n",
        "        loss = loss.sum() / n_tokens\n",
        "\n",
        "        # Levenshtein Distance\n",
        "        avg_distance += LevenshteinDistance(predictions, Y, Y_lens)\n",
        "\n",
        "        # avg_loss\n",
        "        avg_loss += float(loss)\n",
        "        del X\n",
        "        del X_lens\n",
        "        del Y_lens\n",
        "        del Y\n",
        "        del predictions\n",
        "        del label_mask \n",
        "        count += 1\n",
        "\n",
        "    avg_loss /= count\n",
        "    avg_distance /= count \n",
        "\n",
        "    ppl = np.exp(avg_loss)\n",
        "\n",
        "    # inspection\n",
        "    print(\" ********************* Epoch \",epoch, \" ******************\")\n",
        "    print(\"Test loss  \",epoch, \" is \",avg_loss)\n",
        "    print(\"Perplexity is: \",ppl)\n",
        "    print(\"Avg distance is: \", avg_distance)\n",
        "    model.train()\n",
        "    return ppl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKms7ytg1gt9"
      },
      "source": [
        "## Main Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "udjoaLkG9P2_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "966e6a77-1240-40c1-d711-0067dc316e8b"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "import time\n",
        "\n",
        "# How to convert the result.\n",
        "def main():\n",
        "        ############# resume training ? ######################\n",
        "    model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=HIDDEN_SIZE)\n",
        "    model.to(DEVICE)\n",
        "    #     ######################################################\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr= 3e-4)\n",
        "\n",
        "    # use scheduler to change learning rate\n",
        "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience =0,factor = 0.5)\n",
        "    criterion = nn.CrossEntropyLoss(reduce=False)\n",
        "    nepochs = 50\n",
        "    batch_size = 64 if DEVICE == 'cuda' else 1\n",
        "\n",
        "    lower = 100000\n",
        "    model_id = 0\n",
        "    for epoch in range(nepochs):\n",
        "        # if epoch == 10:\n",
        "        #     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 1,factor = 0.5)\n",
        "        # if epoch == 20:\n",
        "        #     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 3,factor = 0.5)\n",
        "        # if epoch == 30:\n",
        "        #     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        #     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 1,factor = 0.1)       \n",
        "        train(model, train_loader, criterion, optimizer, epoch)\n",
        "        # val()\n",
        "        distance = test(model, dev_loader, criterion, epoch)\n",
        "        if distance < lower:\n",
        "            lower = distance\n",
        "            model_id = epoch\n",
        "        # if epoch >= 30:\n",
        "        #     scheduler.step(distance)\n",
        "\n",
        "        # save model\n",
        "        model_save_name = 'classifier' + str(epoch+1)+ '.pt'\n",
        "        path = F\"/content/drive/My Drive/Kaggle/{model_save_name}\" \n",
        "        torch.save(model,path)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learning rate for epoch  0  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  0 batch  0 : Loss: 6.5948 Perplexity: 731.2764\n",
            " ********* Epoch  0  Average Levenshtein Distance is ****:  185.21875\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  0 batch  10 : Loss: 5.3709 Perplexity: 215.0579\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  0 batch  20 : Loss: 4.2494 Perplexity: 70.065\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  0 batch  30 : Loss: 4.4383 Perplexity: 84.6338\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  0 batch  40 : Loss: 3.9522 Perplexity: 52.0519\n",
            "Avg 10 batchs takes  0.55  min\n",
            "EPOCH  0 batch  50 : Loss: 3.5515 Perplexity: 34.8661\n",
            "Avg 10 batchs takes  0.66  min\n",
            "EPOCH  0 batch  60 : Loss: 3.2776 Perplexity: 26.5115\n",
            "Avg 10 batchs takes  0.77  min\n",
            "EPOCH  0 batch  70 : Loss: 3.3857 Perplexity: 29.5372\n",
            "Avg 10 batchs takes  0.88  min\n",
            "EPOCH  0 batch  80 : Loss: 3.238 Perplexity: 25.4835\n",
            "Avg 10 batchs takes  0.99  min\n",
            "EPOCH  0 batch  90 : Loss: 3.1208 Perplexity: 22.6643\n",
            "Avg 10 batchs takes  1.09  min\n",
            "EPOCH  0 batch  100 : Loss: 3.0994 Perplexity: 22.1857\n",
            "Avg 10 batchs takes  1.19  min\n",
            "EPOCH  0 batch  110 : Loss: 3.034 Perplexity: 20.7796\n",
            "Avg 10 batchs takes  1.3  min\n",
            "EPOCH  0 batch  120 : Loss: 3.0289 Perplexity: 20.6752\n",
            "Avg 10 batchs takes  1.4  min\n",
            "EPOCH  0 batch  130 : Loss: 2.9986 Perplexity: 20.0578\n",
            "Avg 10 batchs takes  1.51  min\n",
            "EPOCH  0 batch  140 : Loss: 2.9812 Perplexity: 19.7108\n",
            "Avg 10 batchs takes  1.62  min\n",
            "EPOCH  0 batch  150 : Loss: 2.9882 Perplexity: 19.8494\n",
            "Avg 10 batchs takes  1.73  min\n",
            "EPOCH  0 batch  160 : Loss: 2.9829 Perplexity: 19.7453\n",
            "Avg 10 batchs takes  1.84  min\n",
            "EPOCH  0 batch  170 : Loss: 3.0261 Perplexity: 20.6157\n",
            "Avg 10 batchs takes  1.95  min\n",
            "EPOCH  0 batch  180 : Loss: 3.035 Perplexity: 20.8001\n",
            "Avg 10 batchs takes  2.05  min\n",
            "EPOCH  0 batch  190 : Loss: 3.0152 Perplexity: 20.3923\n",
            "Avg 10 batchs takes  2.16  min\n",
            "EPOCH  0 batch  200 : Loss: 2.9918 Perplexity: 19.9211\n",
            "Avg 10 batchs takes  2.26  min\n",
            "EPOCH  0 batch  210 : Loss: 3.0188 Perplexity: 20.4667\n",
            "Avg 10 batchs takes  2.37  min\n",
            "EPOCH  0 batch  220 : Loss: 2.9815 Perplexity: 19.7165\n",
            "Avg 10 batchs takes  2.48  min\n",
            "EPOCH  0 batch  230 : Loss: 2.9804 Perplexity: 19.6953\n",
            "Avg 10 batchs takes  2.58  min\n",
            "EPOCH  0 batch  240 : Loss: 3.0212 Perplexity: 20.5157\n",
            "Avg 10 batchs takes  2.69  min\n",
            "EPOCH  0 batch  250 : Loss: 2.9904 Perplexity: 19.8943\n",
            "Avg 10 batchs takes  2.8  min\n",
            "EPOCH  0 batch  260 : Loss: 2.9964 Perplexity: 20.0126\n",
            "Avg 10 batchs takes  2.9  min\n",
            "EPOCH  0 batch  270 : Loss: 2.988 Perplexity: 19.8462\n",
            "Avg 10 batchs takes  3.01  min\n",
            "EPOCH  0 batch  280 : Loss: 2.9872 Perplexity: 19.8296\n",
            "Avg 10 batchs takes  3.12  min\n",
            "EPOCH  0 batch  290 : Loss: 2.9642 Perplexity: 19.3788\n",
            "Avg 10 batchs takes  3.23  min\n",
            "EPOCH  0 batch  300 : Loss: 2.9842 Perplexity: 19.7702\n",
            " ********* Epoch  0  Average Levenshtein Distance is ****:  368.90625\n",
            "Avg 10 batchs takes  3.34  min\n",
            "EPOCH  0 batch  310 : Loss: 2.9679 Perplexity: 19.4518\n",
            "Avg 10 batchs takes  3.44  min\n",
            "EPOCH  0 batch  320 : Loss: 2.9411 Perplexity: 18.9375\n",
            "Avg 10 batchs takes  3.54  min\n",
            "EPOCH  0 batch  330 : Loss: 2.909 Perplexity: 18.338\n",
            "Avg 10 batchs takes  3.65  min\n",
            "EPOCH  0 batch  340 : Loss: 2.9007 Perplexity: 18.1878\n",
            "Avg 10 batchs takes  3.76  min\n",
            "EPOCH  0 batch  350 : Loss: 2.9509 Perplexity: 19.1227\n",
            "Avg 10 batchs takes  3.86  min\n",
            "EPOCH  0 batch  360 : Loss: 2.9371 Perplexity: 18.8615\n",
            "Avg 10 batchs takes  3.97  min\n",
            "EPOCH  0 batch  370 : Loss: 2.9157 Perplexity: 18.4623\n",
            "Avg 10 batchs takes  4.08  min\n",
            "EPOCH  0 batch  380 : Loss: 2.8835 Perplexity: 17.8773\n",
            "Avg 10 batchs takes  4.18  min\n",
            "EPOCH  0 batch  390 : Loss: 2.8846 Perplexity: 17.8971\n",
            "Avg 10 batchs takes  4.29  min\n",
            "EPOCH  0 batch  400 : Loss: 2.9244 Perplexity: 18.6235\n",
            "Avg 10 batchs takes  4.4  min\n",
            "EPOCH  0 batch  410 : Loss: 2.8993 Perplexity: 18.1612\n",
            "Avg 10 batchs takes  4.5  min\n",
            "EPOCH  0 batch  420 : Loss: 2.9114 Perplexity: 18.3818\n",
            "Avg 10 batchs takes  4.61  min\n",
            "EPOCH  0 batch  430 : Loss: 2.8641 Perplexity: 17.5324\n",
            "Avg 10 batchs takes  4.71  min\n",
            "EPOCH  0 batch  440 : Loss: 2.8718 Perplexity: 17.669\n",
            "Avg 10 batchs takes  4.82  min\n",
            "EPOCH  0 batch  450 : Loss: 2.8687 Perplexity: 17.6136\n",
            "Avg 10 batchs takes  4.92  min\n",
            "EPOCH  0 batch  460 : Loss: 2.8709 Perplexity: 17.6525\n",
            "Avg 10 batchs takes  5.02  min\n",
            "EPOCH  0 batch  470 : Loss: 2.8284 Perplexity: 16.9178\n",
            "Avg 10 batchs takes  5.13  min\n",
            "EPOCH  0 batch  480 : Loss: 2.849 Perplexity: 17.2698\n",
            "Avg 10 batchs takes  5.23  min\n",
            "EPOCH  0 batch  490 : Loss: 2.8526 Perplexity: 17.3325\n",
            "Avg 10 batchs takes  5.34  min\n",
            "EPOCH  0 batch  500 : Loss: 2.8792 Perplexity: 17.8007\n",
            "Avg 10 batchs takes  5.43  min\n",
            "EPOCH  0 batch  510 : Loss: 2.8176 Perplexity: 16.7371\n",
            "Avg 10 batchs takes  5.54  min\n",
            "EPOCH  0 batch  520 : Loss: 2.8036 Perplexity: 16.5044\n",
            "Avg 10 batchs takes  5.65  min\n",
            "EPOCH  0 batch  530 : Loss: 2.8347 Perplexity: 17.0254\n",
            "Avg 10 batchs takes  5.75  min\n",
            "EPOCH  0 batch  540 : Loss: 2.83 Perplexity: 16.9447\n",
            "Avg 10 batchs takes  5.85  min\n",
            "EPOCH  0 batch  550 : Loss: 2.8195 Perplexity: 16.769\n",
            "Avg 10 batchs takes  5.96  min\n",
            "EPOCH  0 batch  560 : Loss: 2.7949 Perplexity: 16.3602\n",
            "Avg 10 batchs takes  6.06  min\n",
            "EPOCH  0 batch  570 : Loss: 2.8196 Perplexity: 16.77\n",
            "Avg 10 batchs takes  6.16  min\n",
            "EPOCH  0 batch  580 : Loss: 2.7715 Perplexity: 15.9827\n",
            "Avg 10 batchs takes  6.27  min\n",
            "EPOCH  0 batch  590 : Loss: 2.8048 Perplexity: 16.5245\n",
            "Avg 10 batchs takes  6.37  min\n",
            "EPOCH  0 batch  600 : Loss: 2.758 Perplexity: 15.7682\n",
            " ********* Epoch  0  Average Levenshtein Distance is ****:  642.5625\n",
            "Avg 10 batchs takes  6.48  min\n",
            "EPOCH  0 batch  610 : Loss: 2.7454 Perplexity: 15.5711\n",
            "Avg 10 batchs takes  6.58  min\n",
            "EPOCH  0 batch  620 : Loss: 2.7605 Perplexity: 15.808\n",
            "Avg 10 batchs takes  6.69  min\n",
            "EPOCH  0 batch  630 : Loss: 2.7338 Perplexity: 15.3909\n",
            "Avg 10 batchs takes  6.79  min\n",
            "EPOCH  0 batch  640 : Loss: 2.7618 Perplexity: 15.8279\n",
            "Avg 10 batchs takes  6.89  min\n",
            "EPOCH  0 batch  650 : Loss: 2.7219 Perplexity: 15.2094\n",
            "Avg 10 batchs takes  7.0  min\n",
            "EPOCH  0 batch  660 : Loss: 2.7445 Perplexity: 15.557\n",
            "Avg 10 batchs takes  7.1  min\n",
            "EPOCH  0 batch  670 : Loss: 2.7361 Perplexity: 15.426\n",
            "Avg 10 batchs takes  7.21  min\n",
            "EPOCH  0 batch  680 : Loss: 2.7196 Perplexity: 15.1743\n",
            "Avg 10 batchs takes  7.31  min\n",
            "EPOCH  0 batch  690 : Loss: 2.7182 Perplexity: 15.1523\n",
            "Avg 10 batchs takes  7.43  min\n",
            "EPOCH  0 batch  700 : Loss: 2.6774 Perplexity: 14.5476\n",
            "Avg 10 batchs takes  7.53  min\n",
            "EPOCH  0 batch  710 : Loss: 2.6886 Perplexity: 14.7108\n",
            "Avg 10 batchs takes  7.63  min\n",
            "EPOCH  0 batch  720 : Loss: 2.7101 Perplexity: 15.0313\n",
            "Avg 10 batchs takes  7.74  min\n",
            "EPOCH  0 batch  730 : Loss: 2.6951 Perplexity: 14.8077\n",
            "Avg 10 batchs takes  7.85  min\n",
            "EPOCH  0 batch  740 : Loss: 2.7166 Perplexity: 15.1287\n",
            "Avg 10 batchs takes  7.95  min\n",
            "EPOCH  0 batch  750 : Loss: 2.655 Perplexity: 14.2245\n",
            "Avg 10 batchs takes  8.05  min\n",
            "EPOCH  0 batch  760 : Loss: 2.7194 Perplexity: 15.1709\n",
            "Avg 10 batchs takes  8.16  min\n",
            "EPOCH  0 batch  770 : Loss: 2.6907 Perplexity: 14.7417\n",
            "Training loss after one epoch is: 2.6766676902770996\n",
            "Time take for an epoch is: 8.18  min\n",
            " ********************* Epoch  0  ******************\n",
            "Test loss   0  is  3.2760588100978305\n",
            "Perplexity is:  26.47123866242365\n",
            "Avg distance is:  158.0875992063492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type pBLSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LockedDropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "EPOCH  19 batch  710 : Loss: 2.4254 Perplexity: 11.3069\n",
            "Avg 10 batchs takes  7.55  min\n",
            "EPOCH  19 batch  720 : Loss: 2.383 Perplexity: 10.8377\n",
            "Avg 10 batchs takes  7.66  min\n",
            "EPOCH  19 batch  730 : Loss: 2.4089 Perplexity: 11.1222\n",
            "Avg 10 batchs takes  7.76  min\n",
            "EPOCH  19 batch  740 : Loss: 2.4229 Perplexity: 11.2781\n",
            "Avg 10 batchs takes  7.86  min\n",
            "EPOCH  19 batch  750 : Loss: 2.4053 Perplexity: 11.0812\n",
            "Avg 10 batchs takes  7.97  min\n",
            "EPOCH  19 batch  760 : Loss: 2.3283 Perplexity: 10.2607\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  19 batch  770 : Loss: 2.4427 Perplexity: 11.5038\n",
            "Training loss after one epoch is: 2.4283413887023926\n",
            "Time take for an epoch is: 8.09  min\n",
            " ********************* Epoch  19  ******************\n",
            "Test loss   19  is  3.7454959869384767\n",
            "Perplexity is:  42.329997138731024\n",
            "Avg distance is:  201.3947420634921\n",
            "Learning rate for epoch  20  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  20 batch  0 : Loss: 2.3848 Perplexity: 10.8571\n",
            " ********* Epoch  20  Average Levenshtein Distance is ****:  286.4375\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  20 batch  10 : Loss: 2.4431 Perplexity: 11.5089\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  20 batch  20 : Loss: 2.3873 Perplexity: 10.8845\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  20 batch  30 : Loss: 2.4228 Perplexity: 11.2779\n",
            "Avg 10 batchs takes  0.43  min\n",
            "EPOCH  20 batch  40 : Loss: 2.4474 Perplexity: 11.5579\n",
            "Avg 10 batchs takes  0.54  min\n",
            "EPOCH  20 batch  50 : Loss: 2.4642 Perplexity: 11.7546\n",
            "Avg 10 batchs takes  0.64  min\n",
            "EPOCH  20 batch  60 : Loss: 2.4156 Perplexity: 11.1964\n",
            "Avg 10 batchs takes  0.74  min\n",
            "EPOCH  20 batch  70 : Loss: 2.6344 Perplexity: 13.9349\n",
            "Avg 10 batchs takes  0.84  min\n",
            "EPOCH  20 batch  80 : Loss: 2.419 Perplexity: 11.2342\n",
            "Avg 10 batchs takes  0.94  min\n",
            "EPOCH  20 batch  90 : Loss: 2.4159 Perplexity: 11.1994\n",
            "Avg 10 batchs takes  1.04  min\n",
            "EPOCH  20 batch  100 : Loss: 2.4492 Perplexity: 11.5786\n",
            "Avg 10 batchs takes  1.14  min\n",
            "EPOCH  20 batch  110 : Loss: 2.5039 Perplexity: 12.2303\n",
            "Avg 10 batchs takes  1.25  min\n",
            "EPOCH  20 batch  120 : Loss: 2.4402 Perplexity: 11.4758\n",
            "Avg 10 batchs takes  1.35  min\n",
            "EPOCH  20 batch  130 : Loss: 2.4406 Perplexity: 11.4798\n",
            "Avg 10 batchs takes  1.46  min\n",
            "EPOCH  20 batch  140 : Loss: 2.4946 Perplexity: 12.1169\n",
            "Avg 10 batchs takes  1.56  min\n",
            "EPOCH  20 batch  150 : Loss: 2.4042 Perplexity: 11.0693\n",
            "Avg 10 batchs takes  1.66  min\n",
            "EPOCH  20 batch  160 : Loss: 2.457 Perplexity: 11.67\n",
            "Avg 10 batchs takes  1.77  min\n",
            "EPOCH  20 batch  170 : Loss: 2.4858 Perplexity: 12.0103\n",
            "Avg 10 batchs takes  1.87  min\n",
            "EPOCH  20 batch  180 : Loss: 2.4175 Perplexity: 11.2181\n",
            "Avg 10 batchs takes  1.98  min\n",
            "EPOCH  20 batch  190 : Loss: 2.4678 Perplexity: 11.7965\n",
            "Avg 10 batchs takes  2.08  min\n",
            "EPOCH  20 batch  200 : Loss: 2.396 Perplexity: 10.9788\n",
            "Avg 10 batchs takes  2.18  min\n",
            "EPOCH  20 batch  210 : Loss: 2.4205 Perplexity: 11.251\n",
            "Avg 10 batchs takes  2.28  min\n",
            "EPOCH  20 batch  220 : Loss: 2.4386 Perplexity: 11.4566\n",
            "Avg 10 batchs takes  2.39  min\n",
            "EPOCH  20 batch  230 : Loss: 2.4261 Perplexity: 11.3142\n",
            "Avg 10 batchs takes  2.49  min\n",
            "EPOCH  20 batch  240 : Loss: 2.3942 Perplexity: 10.9597\n",
            "Avg 10 batchs takes  2.6  min\n",
            "EPOCH  20 batch  250 : Loss: 2.4706 Perplexity: 11.8292\n",
            "Avg 10 batchs takes  2.71  min\n",
            "EPOCH  20 batch  260 : Loss: 2.4102 Perplexity: 11.1358\n",
            "Avg 10 batchs takes  2.82  min\n",
            "EPOCH  20 batch  270 : Loss: 2.3997 Perplexity: 11.0197\n",
            "Avg 10 batchs takes  2.92  min\n",
            "EPOCH  20 batch  280 : Loss: 2.3973 Perplexity: 10.9936\n",
            "Avg 10 batchs takes  3.03  min\n",
            "EPOCH  20 batch  290 : Loss: 2.4122 Perplexity: 11.1579\n",
            "Avg 10 batchs takes  3.14  min\n",
            "EPOCH  20 batch  300 : Loss: 2.3992 Perplexity: 11.0148\n",
            " ********* Epoch  20  Average Levenshtein Distance is ****:  259.21875\n",
            "Avg 10 batchs takes  3.25  min\n",
            "EPOCH  20 batch  310 : Loss: 2.4053 Perplexity: 11.0817\n",
            "Avg 10 batchs takes  3.35  min\n",
            "EPOCH  20 batch  320 : Loss: 2.4555 Perplexity: 11.6521\n",
            "Avg 10 batchs takes  3.46  min\n",
            "EPOCH  20 batch  330 : Loss: 2.4129 Perplexity: 11.166\n",
            "Avg 10 batchs takes  3.56  min\n",
            "EPOCH  20 batch  340 : Loss: 2.4054 Perplexity: 11.0823\n",
            "Avg 10 batchs takes  3.67  min\n",
            "EPOCH  20 batch  350 : Loss: 2.3685 Perplexity: 10.6815\n",
            "Avg 10 batchs takes  3.77  min\n",
            "EPOCH  20 batch  360 : Loss: 2.4313 Perplexity: 11.374\n",
            "Avg 10 batchs takes  3.88  min\n",
            "EPOCH  20 batch  370 : Loss: 2.3964 Perplexity: 10.984\n",
            "Avg 10 batchs takes  3.98  min\n",
            "EPOCH  20 batch  380 : Loss: 2.4579 Perplexity: 11.6805\n",
            "Avg 10 batchs takes  4.08  min\n",
            "EPOCH  20 batch  390 : Loss: 2.4466 Perplexity: 11.5493\n",
            "Avg 10 batchs takes  4.19  min\n",
            "EPOCH  20 batch  400 : Loss: 2.4184 Perplexity: 11.228\n",
            "Avg 10 batchs takes  4.3  min\n",
            "EPOCH  20 batch  410 : Loss: 2.4114 Perplexity: 11.1497\n",
            "Avg 10 batchs takes  4.41  min\n",
            "EPOCH  20 batch  420 : Loss: 2.3546 Perplexity: 10.5344\n",
            "Avg 10 batchs takes  4.5  min\n",
            "EPOCH  20 batch  430 : Loss: 2.4478 Perplexity: 11.5634\n",
            "Avg 10 batchs takes  4.61  min\n",
            "EPOCH  20 batch  440 : Loss: 2.4175 Perplexity: 11.2183\n",
            "Avg 10 batchs takes  4.72  min\n",
            "EPOCH  20 batch  450 : Loss: 2.4595 Perplexity: 11.6988\n",
            "Avg 10 batchs takes  4.82  min\n",
            "EPOCH  20 batch  460 : Loss: 2.4188 Perplexity: 11.2323\n",
            "Avg 10 batchs takes  4.92  min\n",
            "EPOCH  20 batch  470 : Loss: 2.3705 Perplexity: 10.7026\n",
            "Avg 10 batchs takes  5.03  min\n",
            "EPOCH  20 batch  480 : Loss: 2.4525 Perplexity: 11.617\n",
            "Avg 10 batchs takes  5.13  min\n",
            "EPOCH  20 batch  490 : Loss: 2.3406 Perplexity: 10.3876\n",
            "Avg 10 batchs takes  5.24  min\n",
            "EPOCH  20 batch  500 : Loss: 2.3542 Perplexity: 10.5293\n",
            "Avg 10 batchs takes  5.35  min\n",
            "EPOCH  20 batch  510 : Loss: 2.4416 Perplexity: 11.4918\n",
            "Avg 10 batchs takes  5.46  min\n",
            "EPOCH  20 batch  520 : Loss: 2.4556 Perplexity: 11.6534\n",
            "Avg 10 batchs takes  5.56  min\n",
            "EPOCH  20 batch  530 : Loss: 2.4147 Perplexity: 11.1861\n",
            "Avg 10 batchs takes  5.67  min\n",
            "EPOCH  20 batch  540 : Loss: 2.4443 Perplexity: 11.5222\n",
            "Avg 10 batchs takes  5.77  min\n",
            "EPOCH  20 batch  550 : Loss: 2.4262 Perplexity: 11.3162\n",
            "Avg 10 batchs takes  5.88  min\n",
            "EPOCH  20 batch  560 : Loss: 2.4046 Perplexity: 11.074\n",
            "Avg 10 batchs takes  5.98  min\n",
            "EPOCH  20 batch  570 : Loss: 2.4672 Perplexity: 11.7889\n",
            "Avg 10 batchs takes  6.09  min\n",
            "EPOCH  20 batch  580 : Loss: 2.3385 Perplexity: 10.3654\n",
            "Avg 10 batchs takes  6.2  min\n",
            "EPOCH  20 batch  590 : Loss: 2.3868 Perplexity: 10.8785\n",
            "Avg 10 batchs takes  6.3  min\n",
            "EPOCH  20 batch  600 : Loss: 2.4779 Perplexity: 11.9161\n",
            " ********* Epoch  20  Average Levenshtein Distance is ****:  286.96875\n",
            "Avg 10 batchs takes  6.4  min\n",
            "EPOCH  20 batch  610 : Loss: 2.3846 Perplexity: 10.8551\n",
            "Avg 10 batchs takes  6.5  min\n",
            "EPOCH  20 batch  620 : Loss: 2.3931 Perplexity: 10.9471\n",
            "Avg 10 batchs takes  6.61  min\n",
            "EPOCH  20 batch  630 : Loss: 2.4073 Perplexity: 11.1043\n",
            "Avg 10 batchs takes  6.71  min\n",
            "EPOCH  20 batch  640 : Loss: 2.4216 Perplexity: 11.2642\n",
            "Avg 10 batchs takes  6.81  min\n",
            "EPOCH  20 batch  650 : Loss: 2.4287 Perplexity: 11.3438\n",
            "Avg 10 batchs takes  6.92  min\n",
            "EPOCH  20 batch  660 : Loss: 2.4054 Perplexity: 11.083\n",
            "Avg 10 batchs takes  7.03  min\n",
            "EPOCH  20 batch  670 : Loss: 2.4033 Perplexity: 11.0598\n",
            "Avg 10 batchs takes  7.14  min\n",
            "EPOCH  20 batch  680 : Loss: 2.4023 Perplexity: 11.0486\n",
            "Avg 10 batchs takes  7.24  min\n",
            "EPOCH  20 batch  690 : Loss: 2.4448 Perplexity: 11.5281\n",
            "Avg 10 batchs takes  7.35  min\n",
            "EPOCH  20 batch  700 : Loss: 2.4224 Perplexity: 11.2729\n",
            "Avg 10 batchs takes  7.45  min\n",
            "EPOCH  20 batch  710 : Loss: 2.3128 Perplexity: 10.1023\n",
            "Avg 10 batchs takes  7.55  min\n",
            "EPOCH  20 batch  720 : Loss: 2.4731 Perplexity: 11.8594\n",
            "Avg 10 batchs takes  7.66  min\n",
            "EPOCH  20 batch  730 : Loss: 2.4507 Perplexity: 11.5966\n",
            "Avg 10 batchs takes  7.76  min\n",
            "EPOCH  20 batch  740 : Loss: 2.463 Perplexity: 11.7399\n",
            "Avg 10 batchs takes  7.87  min\n",
            "EPOCH  20 batch  750 : Loss: 2.3999 Perplexity: 11.0215\n",
            "Avg 10 batchs takes  7.98  min\n",
            "EPOCH  20 batch  760 : Loss: 2.4826 Perplexity: 11.9724\n",
            "Avg 10 batchs takes  8.09  min\n",
            "EPOCH  20 batch  770 : Loss: 2.3986 Perplexity: 11.0079\n",
            "Training loss after one epoch is: 2.411391496658325\n",
            "Time take for an epoch is: 8.11  min\n",
            " ********************* Epoch  20  ******************\n",
            "Test loss   20  is  3.716432809829712\n",
            "Perplexity is:  41.11745839718755\n",
            "Avg distance is:  180.66855158730158\n",
            "Learning rate for epoch  21  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  21 batch  0 : Loss: 2.4812 Perplexity: 11.9556\n",
            " ********* Epoch  21  Average Levenshtein Distance is ****:  344.375\n",
            "Avg 10 batchs takes  0.11  min\n",
            "EPOCH  21 batch  10 : Loss: 2.3878 Perplexity: 10.8898\n",
            "Avg 10 batchs takes  0.22  min\n",
            "EPOCH  21 batch  20 : Loss: 2.427 Perplexity: 11.3253\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  21 batch  30 : Loss: 2.4155 Perplexity: 11.1954\n",
            "Avg 10 batchs takes  0.43  min\n",
            "EPOCH  21 batch  40 : Loss: 2.4149 Perplexity: 11.1883\n",
            "Avg 10 batchs takes  0.53  min\n",
            "EPOCH  21 batch  50 : Loss: 2.4644 Perplexity: 11.756\n",
            "Avg 10 batchs takes  0.63  min\n",
            "EPOCH  21 batch  60 : Loss: 2.4248 Perplexity: 11.2996\n",
            "Avg 10 batchs takes  0.74  min\n",
            "EPOCH  21 batch  70 : Loss: 2.3497 Perplexity: 10.4823\n",
            "Avg 10 batchs takes  0.84  min\n",
            "EPOCH  21 batch  80 : Loss: 2.3992 Perplexity: 11.0144\n",
            "Avg 10 batchs takes  0.95  min\n",
            "EPOCH  21 batch  90 : Loss: 2.4679 Perplexity: 11.7972\n",
            "Avg 10 batchs takes  1.05  min\n",
            "EPOCH  21 batch  100 : Loss: 2.4438 Perplexity: 11.5164\n",
            "Avg 10 batchs takes  1.15  min\n",
            "EPOCH  21 batch  110 : Loss: 2.4203 Perplexity: 11.2487\n",
            "Avg 10 batchs takes  1.25  min\n",
            "EPOCH  21 batch  120 : Loss: 2.4029 Perplexity: 11.0553\n",
            "Avg 10 batchs takes  1.36  min\n",
            "EPOCH  21 batch  130 : Loss: 2.4828 Perplexity: 11.9744\n",
            "Avg 10 batchs takes  1.47  min\n",
            "EPOCH  21 batch  140 : Loss: 2.4522 Perplexity: 11.6136\n",
            "Avg 10 batchs takes  1.57  min\n",
            "EPOCH  21 batch  150 : Loss: 2.515 Perplexity: 12.3669\n",
            "Avg 10 batchs takes  1.67  min\n",
            "EPOCH  21 batch  160 : Loss: 2.4366 Perplexity: 11.4337\n",
            "Avg 10 batchs takes  1.78  min\n",
            "EPOCH  21 batch  170 : Loss: 2.4103 Perplexity: 11.1379\n",
            "Avg 10 batchs takes  1.88  min\n",
            "EPOCH  21 batch  180 : Loss: 2.3561 Perplexity: 10.5498\n",
            "Avg 10 batchs takes  1.99  min\n",
            "EPOCH  21 batch  190 : Loss: 2.3665 Perplexity: 10.6602\n",
            "Avg 10 batchs takes  2.09  min\n",
            "EPOCH  21 batch  200 : Loss: 2.4625 Perplexity: 11.7345\n",
            "Avg 10 batchs takes  2.2  min\n",
            "EPOCH  21 batch  210 : Loss: 2.3977 Perplexity: 10.9983\n",
            "Avg 10 batchs takes  2.3  min\n",
            "EPOCH  21 batch  220 : Loss: 2.4117 Perplexity: 11.1529\n",
            "Avg 10 batchs takes  2.41  min\n",
            "EPOCH  21 batch  230 : Loss: 2.4233 Perplexity: 11.2832\n",
            "Avg 10 batchs takes  2.52  min\n",
            "EPOCH  21 batch  240 : Loss: 2.3478 Perplexity: 10.4627\n",
            "Avg 10 batchs takes  2.62  min\n",
            "EPOCH  21 batch  250 : Loss: 2.3917 Perplexity: 10.9322\n",
            "Avg 10 batchs takes  2.73  min\n",
            "EPOCH  21 batch  260 : Loss: 2.402 Perplexity: 11.0457\n",
            "Avg 10 batchs takes  2.83  min\n",
            "EPOCH  21 batch  270 : Loss: 2.3934 Perplexity: 10.9502\n",
            "Avg 10 batchs takes  2.94  min\n",
            "EPOCH  21 batch  280 : Loss: 2.4569 Perplexity: 11.6686\n",
            "Avg 10 batchs takes  3.05  min\n",
            "EPOCH  21 batch  290 : Loss: 2.3744 Perplexity: 10.745\n",
            "Avg 10 batchs takes  3.15  min\n",
            "EPOCH  21 batch  300 : Loss: 2.3909 Perplexity: 10.9235\n",
            " ********* Epoch  21  Average Levenshtein Distance is ****:  360.40625\n",
            "Avg 10 batchs takes  3.25  min\n",
            "EPOCH  21 batch  310 : Loss: 2.4446 Perplexity: 11.5258\n",
            "Avg 10 batchs takes  3.36  min\n",
            "EPOCH  21 batch  320 : Loss: 2.3829 Perplexity: 10.8367\n",
            "Avg 10 batchs takes  3.47  min\n",
            "EPOCH  21 batch  330 : Loss: 2.3765 Perplexity: 10.7672\n",
            "Avg 10 batchs takes  3.57  min\n",
            "EPOCH  21 batch  340 : Loss: 2.3544 Perplexity: 10.5315\n",
            "Avg 10 batchs takes  3.68  min\n",
            "EPOCH  21 batch  350 : Loss: 2.277 Perplexity: 9.747\n",
            "Avg 10 batchs takes  3.79  min\n",
            "EPOCH  21 batch  360 : Loss: 2.3873 Perplexity: 10.8846\n",
            "Avg 10 batchs takes  3.89  min\n",
            "EPOCH  21 batch  370 : Loss: 2.3435 Perplexity: 10.4171\n",
            "Avg 10 batchs takes  3.99  min\n",
            "EPOCH  21 batch  380 : Loss: 2.4113 Perplexity: 11.1489\n",
            "Avg 10 batchs takes  4.09  min\n",
            "EPOCH  21 batch  390 : Loss: 2.4391 Perplexity: 11.4632\n",
            "Avg 10 batchs takes  4.2  min\n",
            "EPOCH  21 batch  400 : Loss: 2.4002 Perplexity: 11.0256\n",
            "Avg 10 batchs takes  4.3  min\n",
            "EPOCH  21 batch  410 : Loss: 2.3935 Perplexity: 10.9514\n",
            "Avg 10 batchs takes  4.41  min\n",
            "EPOCH  21 batch  420 : Loss: 2.344 Perplexity: 10.4231\n",
            "Avg 10 batchs takes  4.51  min\n",
            "EPOCH  21 batch  430 : Loss: 2.4344 Perplexity: 11.4088\n",
            "Avg 10 batchs takes  4.61  min\n",
            "EPOCH  21 batch  440 : Loss: 2.3683 Perplexity: 10.679\n",
            "Avg 10 batchs takes  4.71  min\n",
            "EPOCH  21 batch  450 : Loss: 2.4353 Perplexity: 11.4194\n",
            "Avg 10 batchs takes  4.82  min\n",
            "EPOCH  21 batch  460 : Loss: 2.352 Perplexity: 10.5067\n",
            "Avg 10 batchs takes  4.93  min\n",
            "EPOCH  21 batch  470 : Loss: 2.4101 Perplexity: 11.1353\n",
            "Avg 10 batchs takes  5.03  min\n",
            "EPOCH  21 batch  480 : Loss: 2.3167 Perplexity: 10.1424\n",
            "Avg 10 batchs takes  5.13  min\n",
            "EPOCH  21 batch  490 : Loss: 2.4029 Perplexity: 11.0548\n",
            "Avg 10 batchs takes  5.24  min\n",
            "EPOCH  21 batch  500 : Loss: 2.4491 Perplexity: 11.5779\n",
            "Avg 10 batchs takes  5.34  min\n",
            "EPOCH  21 batch  510 : Loss: 2.4782 Perplexity: 11.9194\n",
            "Avg 10 batchs takes  5.45  min\n",
            "EPOCH  21 batch  520 : Loss: 2.4023 Perplexity: 11.0491\n",
            "Avg 10 batchs takes  5.55  min\n",
            "EPOCH  21 batch  530 : Loss: 2.4219 Perplexity: 11.2668\n",
            "Avg 10 batchs takes  5.65  min\n",
            "EPOCH  21 batch  540 : Loss: 2.3675 Perplexity: 10.6703\n",
            "Avg 10 batchs takes  5.76  min\n",
            "EPOCH  21 batch  550 : Loss: 2.3804 Perplexity: 10.8094\n",
            "Avg 10 batchs takes  5.86  min\n",
            "EPOCH  21 batch  560 : Loss: 2.2778 Perplexity: 9.7556\n",
            "Avg 10 batchs takes  5.96  min\n",
            "EPOCH  21 batch  570 : Loss: 2.4501 Perplexity: 11.5897\n",
            "Avg 10 batchs takes  6.07  min\n",
            "EPOCH  21 batch  580 : Loss: 2.4244 Perplexity: 11.2954\n",
            "Avg 10 batchs takes  6.17  min\n",
            "EPOCH  21 batch  590 : Loss: 2.4585 Perplexity: 11.6871\n",
            "Avg 10 batchs takes  6.28  min\n",
            "EPOCH  21 batch  600 : Loss: 2.439 Perplexity: 11.4618\n",
            " ********* Epoch  21  Average Levenshtein Distance is ****:  381.71875\n",
            "Avg 10 batchs takes  6.39  min\n",
            "EPOCH  21 batch  610 : Loss: 2.4227 Perplexity: 11.2758\n",
            "Avg 10 batchs takes  6.49  min\n",
            "EPOCH  21 batch  620 : Loss: 2.4059 Perplexity: 11.0883\n",
            "Avg 10 batchs takes  6.6  min\n",
            "EPOCH  21 batch  630 : Loss: 2.4011 Perplexity: 11.0349\n",
            "Avg 10 batchs takes  6.7  min\n",
            "EPOCH  21 batch  640 : Loss: 2.3765 Perplexity: 10.7669\n",
            "Avg 10 batchs takes  6.81  min\n",
            "EPOCH  21 batch  650 : Loss: 2.4227 Perplexity: 11.2759\n",
            "Avg 10 batchs takes  6.92  min\n",
            "EPOCH  21 batch  660 : Loss: 2.4156 Perplexity: 11.1965\n",
            "Avg 10 batchs takes  7.02  min\n",
            "EPOCH  21 batch  670 : Loss: 2.4901 Perplexity: 12.0623\n",
            "Avg 10 batchs takes  7.12  min\n",
            "EPOCH  21 batch  680 : Loss: 2.3751 Perplexity: 10.7526\n",
            "Avg 10 batchs takes  7.23  min\n",
            "EPOCH  21 batch  690 : Loss: 2.4104 Perplexity: 11.1384\n",
            "Avg 10 batchs takes  7.33  min\n",
            "EPOCH  21 batch  700 : Loss: 2.4978 Perplexity: 12.1559\n",
            "Avg 10 batchs takes  7.44  min\n",
            "EPOCH  21 batch  710 : Loss: 2.4147 Perplexity: 11.1866\n",
            "Avg 10 batchs takes  7.54  min\n",
            "EPOCH  21 batch  720 : Loss: 2.4401 Perplexity: 11.4742\n",
            "Avg 10 batchs takes  7.65  min\n",
            "EPOCH  21 batch  730 : Loss: 2.4018 Perplexity: 11.0429\n",
            "Avg 10 batchs takes  7.75  min\n",
            "EPOCH  21 batch  740 : Loss: 2.4292 Perplexity: 11.3493\n",
            "Avg 10 batchs takes  7.86  min\n",
            "EPOCH  21 batch  750 : Loss: 2.431 Perplexity: 11.3698\n",
            "Avg 10 batchs takes  7.96  min\n",
            "EPOCH  21 batch  760 : Loss: 2.3677 Perplexity: 10.6724\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  21 batch  770 : Loss: 2.4527 Perplexity: 11.6195\n",
            "Training loss after one epoch is: 2.4036285877227783\n",
            "Time take for an epoch is: 8.09  min\n",
            " ********************* Epoch  21  ******************\n",
            "Test loss   21  is  3.9396079540252686\n",
            "Perplexity is:  51.39844679586509\n",
            "Avg distance is:  174.9141865079365\n",
            "Learning rate for epoch  22  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  22 batch  0 : Loss: 2.3678 Perplexity: 10.6737\n",
            " ********* Epoch  22  Average Levenshtein Distance is ****:  235.28125\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  22 batch  10 : Loss: 2.3802 Perplexity: 10.8074\n",
            "Avg 10 batchs takes  0.22  min\n",
            "EPOCH  22 batch  20 : Loss: 2.4226 Perplexity: 11.2754\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  22 batch  30 : Loss: 2.3932 Perplexity: 10.9482\n",
            "Avg 10 batchs takes  0.43  min\n",
            "EPOCH  22 batch  40 : Loss: 2.3972 Perplexity: 10.9922\n",
            "Avg 10 batchs takes  0.54  min\n",
            "EPOCH  22 batch  50 : Loss: 2.4641 Perplexity: 11.7525\n",
            "Avg 10 batchs takes  0.64  min\n",
            "EPOCH  22 batch  60 : Loss: 2.4336 Perplexity: 11.4003\n",
            "Avg 10 batchs takes  0.75  min\n",
            "EPOCH  22 batch  70 : Loss: 2.4304 Perplexity: 11.363\n",
            "Avg 10 batchs takes  0.85  min\n",
            "EPOCH  22 batch  80 : Loss: 2.4137 Perplexity: 11.1757\n",
            "Avg 10 batchs takes  0.96  min\n",
            "EPOCH  22 batch  90 : Loss: 2.3994 Perplexity: 11.0165\n",
            "Avg 10 batchs takes  1.06  min\n",
            "EPOCH  22 batch  100 : Loss: 2.5264 Perplexity: 12.5088\n",
            "Avg 10 batchs takes  1.17  min\n",
            "EPOCH  22 batch  110 : Loss: 2.3839 Perplexity: 10.8471\n",
            "Avg 10 batchs takes  1.27  min\n",
            "EPOCH  22 batch  120 : Loss: 2.4148 Perplexity: 11.1875\n",
            "Avg 10 batchs takes  1.38  min\n",
            "EPOCH  22 batch  130 : Loss: 2.4434 Perplexity: 11.5125\n",
            "Avg 10 batchs takes  1.48  min\n",
            "EPOCH  22 batch  140 : Loss: 2.4444 Perplexity: 11.524\n",
            "Avg 10 batchs takes  1.59  min\n",
            "EPOCH  22 batch  150 : Loss: 2.4232 Perplexity: 11.2816\n",
            "Avg 10 batchs takes  1.7  min\n",
            "EPOCH  22 batch  160 : Loss: 2.3511 Perplexity: 10.4972\n",
            "Avg 10 batchs takes  1.79  min\n",
            "EPOCH  22 batch  170 : Loss: 2.4429 Perplexity: 11.5066\n",
            "Avg 10 batchs takes  1.9  min\n",
            "EPOCH  22 batch  180 : Loss: 2.3743 Perplexity: 10.7436\n",
            "Avg 10 batchs takes  2.0  min\n",
            "EPOCH  22 batch  190 : Loss: 2.405 Perplexity: 11.0779\n",
            "Avg 10 batchs takes  2.11  min\n",
            "EPOCH  22 batch  200 : Loss: 2.4278 Perplexity: 11.3343\n",
            "Avg 10 batchs takes  2.21  min\n",
            "EPOCH  22 batch  210 : Loss: 2.4438 Perplexity: 11.5169\n",
            "Avg 10 batchs takes  2.32  min\n",
            "EPOCH  22 batch  220 : Loss: 2.4808 Perplexity: 11.9513\n",
            "Avg 10 batchs takes  2.42  min\n",
            "EPOCH  22 batch  230 : Loss: 2.3409 Perplexity: 10.3901\n",
            "Avg 10 batchs takes  2.53  min\n",
            "EPOCH  22 batch  240 : Loss: 2.4009 Perplexity: 11.0329\n",
            "Avg 10 batchs takes  2.64  min\n",
            "EPOCH  22 batch  250 : Loss: 2.4273 Perplexity: 11.3285\n",
            "Avg 10 batchs takes  2.75  min\n",
            "EPOCH  22 batch  260 : Loss: 2.4294 Perplexity: 11.3521\n",
            "Avg 10 batchs takes  2.85  min\n",
            "EPOCH  22 batch  270 : Loss: 2.4523 Perplexity: 11.6151\n",
            "Avg 10 batchs takes  2.96  min\n",
            "EPOCH  22 batch  280 : Loss: 2.4418 Perplexity: 11.4934\n",
            "Avg 10 batchs takes  3.06  min\n",
            "EPOCH  22 batch  290 : Loss: 2.353 Perplexity: 10.5167\n",
            "Avg 10 batchs takes  3.17  min\n",
            "EPOCH  22 batch  300 : Loss: 2.4114 Perplexity: 11.1499\n",
            " ********* Epoch  22  Average Levenshtein Distance is ****:  280.28125\n",
            "Avg 10 batchs takes  3.27  min\n",
            "EPOCH  22 batch  310 : Loss: 2.3304 Perplexity: 10.2819\n",
            "Avg 10 batchs takes  3.38  min\n",
            "EPOCH  22 batch  320 : Loss: 2.424 Perplexity: 11.2913\n",
            "Avg 10 batchs takes  3.49  min\n",
            "EPOCH  22 batch  330 : Loss: 2.3392 Perplexity: 10.3731\n",
            "Avg 10 batchs takes  3.59  min\n",
            "EPOCH  22 batch  340 : Loss: 2.437 Perplexity: 11.4381\n",
            "Avg 10 batchs takes  3.69  min\n",
            "EPOCH  22 batch  350 : Loss: 2.3664 Perplexity: 10.659\n",
            "Avg 10 batchs takes  3.8  min\n",
            "EPOCH  22 batch  360 : Loss: 2.4368 Perplexity: 11.436\n",
            "Avg 10 batchs takes  3.9  min\n",
            "EPOCH  22 batch  370 : Loss: 2.4379 Perplexity: 11.4484\n",
            "Avg 10 batchs takes  4.0  min\n",
            "EPOCH  22 batch  380 : Loss: 2.4375 Perplexity: 11.444\n",
            "Avg 10 batchs takes  4.1  min\n",
            "EPOCH  22 batch  390 : Loss: 2.4161 Perplexity: 11.2017\n",
            "Avg 10 batchs takes  4.2  min\n",
            "EPOCH  22 batch  400 : Loss: 2.4606 Perplexity: 11.7122\n",
            "Avg 10 batchs takes  4.31  min\n",
            "EPOCH  22 batch  410 : Loss: 2.444 Perplexity: 11.519\n",
            "Avg 10 batchs takes  4.42  min\n",
            "EPOCH  22 batch  420 : Loss: 2.4031 Perplexity: 11.0576\n",
            "Avg 10 batchs takes  4.53  min\n",
            "EPOCH  22 batch  430 : Loss: 2.5266 Perplexity: 12.511\n",
            "Avg 10 batchs takes  4.63  min\n",
            "EPOCH  22 batch  440 : Loss: 2.4048 Perplexity: 11.0762\n",
            "Avg 10 batchs takes  4.74  min\n",
            "EPOCH  22 batch  450 : Loss: 2.3709 Perplexity: 10.7071\n",
            "Avg 10 batchs takes  4.84  min\n",
            "EPOCH  22 batch  460 : Loss: 2.3837 Perplexity: 10.8449\n",
            "Avg 10 batchs takes  4.94  min\n",
            "EPOCH  22 batch  470 : Loss: 2.3678 Perplexity: 10.6742\n",
            "Avg 10 batchs takes  5.04  min\n",
            "EPOCH  22 batch  480 : Loss: 2.4207 Perplexity: 11.254\n",
            "Avg 10 batchs takes  5.15  min\n",
            "EPOCH  22 batch  490 : Loss: 2.4141 Perplexity: 11.1793\n",
            "Avg 10 batchs takes  5.26  min\n",
            "EPOCH  22 batch  500 : Loss: 2.3898 Perplexity: 10.911\n",
            "Avg 10 batchs takes  5.37  min\n",
            "EPOCH  22 batch  510 : Loss: 2.339 Perplexity: 10.3709\n",
            "Avg 10 batchs takes  5.47  min\n",
            "EPOCH  22 batch  520 : Loss: 2.3907 Perplexity: 10.9209\n",
            "Avg 10 batchs takes  5.58  min\n",
            "EPOCH  22 batch  530 : Loss: 2.4046 Perplexity: 11.0744\n",
            "Avg 10 batchs takes  5.68  min\n",
            "EPOCH  22 batch  540 : Loss: 2.4881 Perplexity: 12.0388\n",
            "Avg 10 batchs takes  5.79  min\n",
            "EPOCH  22 batch  550 : Loss: 2.4842 Perplexity: 11.9913\n",
            "Avg 10 batchs takes  5.89  min\n",
            "EPOCH  22 batch  560 : Loss: 2.3913 Perplexity: 10.9274\n",
            "Avg 10 batchs takes  6.0  min\n",
            "EPOCH  22 batch  570 : Loss: 2.4232 Perplexity: 11.2824\n",
            "Avg 10 batchs takes  6.1  min\n",
            "EPOCH  22 batch  580 : Loss: 2.3578 Perplexity: 10.5673\n",
            "Avg 10 batchs takes  6.2  min\n",
            "EPOCH  22 batch  590 : Loss: 2.411 Perplexity: 11.1452\n",
            "Avg 10 batchs takes  6.31  min\n",
            "EPOCH  22 batch  600 : Loss: 2.4496 Perplexity: 11.5836\n",
            " ********* Epoch  22  Average Levenshtein Distance is ****:  324.9375\n",
            "Avg 10 batchs takes  6.42  min\n",
            "EPOCH  22 batch  610 : Loss: 2.3884 Perplexity: 10.896\n",
            "Avg 10 batchs takes  6.52  min\n",
            "EPOCH  22 batch  620 : Loss: 2.4552 Perplexity: 11.6486\n",
            "Avg 10 batchs takes  6.62  min\n",
            "EPOCH  22 batch  630 : Loss: 2.4584 Perplexity: 11.6859\n",
            "Avg 10 batchs takes  6.73  min\n",
            "EPOCH  22 batch  640 : Loss: 2.3895 Perplexity: 10.9077\n",
            "Avg 10 batchs takes  6.83  min\n",
            "EPOCH  22 batch  650 : Loss: 2.393 Perplexity: 10.9466\n",
            "Avg 10 batchs takes  6.94  min\n",
            "EPOCH  22 batch  660 : Loss: 2.3559 Perplexity: 10.5476\n",
            "Avg 10 batchs takes  7.05  min\n",
            "EPOCH  22 batch  670 : Loss: 2.307 Perplexity: 10.0442\n",
            "Avg 10 batchs takes  7.15  min\n",
            "EPOCH  22 batch  680 : Loss: 2.3931 Perplexity: 10.9474\n",
            "Avg 10 batchs takes  7.25  min\n",
            "EPOCH  22 batch  690 : Loss: 2.4058 Perplexity: 11.0869\n",
            "Avg 10 batchs takes  7.36  min\n",
            "EPOCH  22 batch  700 : Loss: 2.3357 Perplexity: 10.3367\n",
            "Avg 10 batchs takes  7.46  min\n",
            "EPOCH  22 batch  710 : Loss: 2.4417 Perplexity: 11.4926\n",
            "Avg 10 batchs takes  7.56  min\n",
            "EPOCH  22 batch  720 : Loss: 2.3801 Perplexity: 10.8056\n",
            "Avg 10 batchs takes  7.67  min\n",
            "EPOCH  22 batch  730 : Loss: 2.4316 Perplexity: 11.3774\n",
            "Avg 10 batchs takes  7.77  min\n",
            "EPOCH  22 batch  740 : Loss: 2.3547 Perplexity: 10.535\n",
            "Avg 10 batchs takes  7.88  min\n",
            "EPOCH  22 batch  750 : Loss: 2.4127 Perplexity: 11.1644\n",
            "Avg 10 batchs takes  7.98  min\n",
            "EPOCH  22 batch  760 : Loss: 2.4549 Perplexity: 11.645\n",
            "Avg 10 batchs takes  8.08  min\n",
            "EPOCH  22 batch  770 : Loss: 2.391 Perplexity: 10.9249\n",
            "Training loss after one epoch is: 2.4090118408203125\n",
            "Time take for an epoch is: 8.1  min\n",
            " ********************* Epoch  22  ******************\n",
            "Test loss   22  is  4.232078524998256\n",
            "Perplexity is:  68.86021119999518\n",
            "Avg distance is:  197.18492063492064\n",
            "Learning rate for epoch  23  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  23 batch  0 : Loss: 2.4393 Perplexity: 11.4646\n",
            " ********* Epoch  23  Average Levenshtein Distance is ****:  256.125\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  23 batch  10 : Loss: 2.4246 Perplexity: 11.2976\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  23 batch  20 : Loss: 2.3998 Perplexity: 11.0207\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  23 batch  30 : Loss: 2.366 Perplexity: 10.6546\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  23 batch  40 : Loss: 2.4256 Perplexity: 11.3087\n",
            "Avg 10 batchs takes  0.55  min\n",
            "EPOCH  23 batch  50 : Loss: 2.3293 Perplexity: 10.2705\n",
            "Avg 10 batchs takes  0.65  min\n",
            "EPOCH  23 batch  60 : Loss: 2.4243 Perplexity: 11.2947\n",
            "Avg 10 batchs takes  0.75  min\n",
            "EPOCH  23 batch  70 : Loss: 2.4203 Perplexity: 11.2497\n",
            "Avg 10 batchs takes  0.86  min\n",
            "EPOCH  23 batch  80 : Loss: 2.4193 Perplexity: 11.2376\n",
            "Avg 10 batchs takes  0.96  min\n",
            "EPOCH  23 batch  90 : Loss: 2.362 Perplexity: 10.6123\n",
            "Avg 10 batchs takes  1.07  min\n",
            "EPOCH  23 batch  100 : Loss: 2.4107 Perplexity: 11.1423\n",
            "Avg 10 batchs takes  1.17  min\n",
            "EPOCH  23 batch  110 : Loss: 2.4107 Perplexity: 11.1415\n",
            "Avg 10 batchs takes  1.28  min\n",
            "EPOCH  23 batch  120 : Loss: 2.3954 Perplexity: 10.9724\n",
            "Avg 10 batchs takes  1.38  min\n",
            "EPOCH  23 batch  130 : Loss: 2.4095 Perplexity: 11.1284\n",
            "Avg 10 batchs takes  1.48  min\n",
            "EPOCH  23 batch  140 : Loss: 2.3785 Perplexity: 10.7883\n",
            "Avg 10 batchs takes  1.58  min\n",
            "EPOCH  23 batch  150 : Loss: 2.3831 Perplexity: 10.8381\n",
            "Avg 10 batchs takes  1.69  min\n",
            "EPOCH  23 batch  160 : Loss: 2.3875 Perplexity: 10.8863\n",
            "Avg 10 batchs takes  1.79  min\n",
            "EPOCH  23 batch  170 : Loss: 2.3713 Perplexity: 10.7117\n",
            "Avg 10 batchs takes  1.9  min\n",
            "EPOCH  23 batch  180 : Loss: 2.4199 Perplexity: 11.2444\n",
            "Avg 10 batchs takes  2.0  min\n",
            "EPOCH  23 batch  190 : Loss: 2.4222 Perplexity: 11.2708\n",
            "Avg 10 batchs takes  2.1  min\n",
            "EPOCH  23 batch  200 : Loss: 2.4556 Perplexity: 11.6536\n",
            "Avg 10 batchs takes  2.21  min\n",
            "EPOCH  23 batch  210 : Loss: 2.3845 Perplexity: 10.8541\n",
            "Avg 10 batchs takes  2.31  min\n",
            "EPOCH  23 batch  220 : Loss: 2.3637 Perplexity: 10.6306\n",
            "Avg 10 batchs takes  2.41  min\n",
            "EPOCH  23 batch  230 : Loss: 2.3429 Perplexity: 10.4114\n",
            "Avg 10 batchs takes  2.51  min\n",
            "EPOCH  23 batch  240 : Loss: 2.4598 Perplexity: 11.7026\n",
            "Avg 10 batchs takes  2.62  min\n",
            "EPOCH  23 batch  250 : Loss: 2.351 Perplexity: 10.496\n",
            "Avg 10 batchs takes  2.73  min\n",
            "EPOCH  23 batch  260 : Loss: 2.411 Perplexity: 11.145\n",
            "Avg 10 batchs takes  2.83  min\n",
            "EPOCH  23 batch  270 : Loss: 2.3712 Perplexity: 10.7098\n",
            "Avg 10 batchs takes  2.94  min\n",
            "EPOCH  23 batch  280 : Loss: 2.4419 Perplexity: 11.4953\n",
            "Avg 10 batchs takes  3.04  min\n",
            "EPOCH  23 batch  290 : Loss: 2.4063 Perplexity: 11.0928\n",
            "Avg 10 batchs takes  3.14  min\n",
            "EPOCH  23 batch  300 : Loss: 2.3889 Perplexity: 10.901\n",
            " ********* Epoch  23  Average Levenshtein Distance is ****:  385.875\n",
            "Avg 10 batchs takes  3.25  min\n",
            "EPOCH  23 batch  310 : Loss: 2.4074 Perplexity: 11.1046\n",
            "Avg 10 batchs takes  3.35  min\n",
            "EPOCH  23 batch  320 : Loss: 2.4089 Perplexity: 11.1216\n",
            "Avg 10 batchs takes  3.46  min\n",
            "EPOCH  23 batch  330 : Loss: 2.3961 Perplexity: 10.9804\n",
            "Avg 10 batchs takes  3.56  min\n",
            "EPOCH  23 batch  340 : Loss: 2.3739 Perplexity: 10.7396\n",
            "Avg 10 batchs takes  3.66  min\n",
            "EPOCH  23 batch  350 : Loss: 2.4293 Perplexity: 11.351\n",
            "Avg 10 batchs takes  3.77  min\n",
            "EPOCH  23 batch  360 : Loss: 2.3358 Perplexity: 10.3382\n",
            "Avg 10 batchs takes  3.87  min\n",
            "EPOCH  23 batch  370 : Loss: 2.3189 Perplexity: 10.1649\n",
            "Avg 10 batchs takes  3.98  min\n",
            "EPOCH  23 batch  380 : Loss: 2.3911 Perplexity: 10.9253\n",
            "Avg 10 batchs takes  4.08  min\n",
            "EPOCH  23 batch  390 : Loss: 2.3794 Perplexity: 10.7981\n",
            "Avg 10 batchs takes  4.18  min\n",
            "EPOCH  23 batch  400 : Loss: 2.4237 Perplexity: 11.2875\n",
            "Avg 10 batchs takes  4.28  min\n",
            "EPOCH  23 batch  410 : Loss: 2.3365 Perplexity: 10.3445\n",
            "Avg 10 batchs takes  4.39  min\n",
            "EPOCH  23 batch  420 : Loss: 2.3686 Perplexity: 10.6827\n",
            "Avg 10 batchs takes  4.49  min\n",
            "EPOCH  23 batch  430 : Loss: 2.3713 Perplexity: 10.7112\n",
            "Avg 10 batchs takes  4.6  min\n",
            "EPOCH  23 batch  440 : Loss: 2.3306 Perplexity: 10.2843\n",
            "Avg 10 batchs takes  4.7  min\n",
            "EPOCH  23 batch  450 : Loss: 2.3756 Perplexity: 10.7574\n",
            "Avg 10 batchs takes  4.8  min\n",
            "EPOCH  23 batch  460 : Loss: 2.3955 Perplexity: 10.9735\n",
            "Avg 10 batchs takes  4.91  min\n",
            "EPOCH  23 batch  470 : Loss: 2.3017 Perplexity: 9.9914\n",
            "Avg 10 batchs takes  5.01  min\n",
            "EPOCH  23 batch  480 : Loss: 2.4369 Perplexity: 11.4381\n",
            "Avg 10 batchs takes  5.11  min\n",
            "EPOCH  23 batch  490 : Loss: 2.3838 Perplexity: 10.8464\n",
            "Avg 10 batchs takes  5.21  min\n",
            "EPOCH  23 batch  500 : Loss: 2.306 Perplexity: 10.0345\n",
            "Avg 10 batchs takes  5.31  min\n",
            "EPOCH  23 batch  510 : Loss: 2.4073 Perplexity: 11.1037\n",
            "Avg 10 batchs takes  5.42  min\n",
            "EPOCH  23 batch  520 : Loss: 2.3955 Perplexity: 10.9736\n",
            "Avg 10 batchs takes  5.53  min\n",
            "EPOCH  23 batch  530 : Loss: 2.3918 Perplexity: 10.9335\n",
            "Avg 10 batchs takes  5.64  min\n",
            "EPOCH  23 batch  540 : Loss: 2.4111 Perplexity: 11.1462\n",
            "Avg 10 batchs takes  5.74  min\n",
            "EPOCH  23 batch  550 : Loss: 2.2954 Perplexity: 9.9283\n",
            "Avg 10 batchs takes  5.85  min\n",
            "EPOCH  23 batch  560 : Loss: 2.3211 Perplexity: 10.1865\n",
            "Avg 10 batchs takes  5.95  min\n",
            "EPOCH  23 batch  570 : Loss: 2.3696 Perplexity: 10.6933\n",
            "Avg 10 batchs takes  6.06  min\n",
            "EPOCH  23 batch  580 : Loss: 2.3756 Perplexity: 10.7576\n",
            "Avg 10 batchs takes  6.17  min\n",
            "EPOCH  23 batch  590 : Loss: 2.372 Perplexity: 10.719\n",
            "Avg 10 batchs takes  6.27  min\n",
            "EPOCH  23 batch  600 : Loss: 2.4287 Perplexity: 11.3441\n",
            " ********* Epoch  23  Average Levenshtein Distance is ****:  379.25\n",
            "Avg 10 batchs takes  6.38  min\n",
            "EPOCH  23 batch  610 : Loss: 2.4679 Perplexity: 11.798\n",
            "Avg 10 batchs takes  6.48  min\n",
            "EPOCH  23 batch  620 : Loss: 2.4815 Perplexity: 11.9589\n",
            "Avg 10 batchs takes  6.59  min\n",
            "EPOCH  23 batch  630 : Loss: 2.4651 Perplexity: 11.7642\n",
            "Avg 10 batchs takes  6.69  min\n",
            "EPOCH  23 batch  640 : Loss: 2.3129 Perplexity: 10.1035\n",
            "Avg 10 batchs takes  6.8  min\n",
            "EPOCH  23 batch  650 : Loss: 2.4138 Perplexity: 11.1769\n",
            "Avg 10 batchs takes  6.9  min\n",
            "EPOCH  23 batch  660 : Loss: 2.3967 Perplexity: 10.9864\n",
            "Avg 10 batchs takes  7.01  min\n",
            "EPOCH  23 batch  670 : Loss: 2.418 Perplexity: 11.2235\n",
            "Avg 10 batchs takes  7.12  min\n",
            "EPOCH  23 batch  680 : Loss: 2.3948 Perplexity: 10.9662\n",
            "Avg 10 batchs takes  7.22  min\n",
            "EPOCH  23 batch  690 : Loss: 2.3989 Perplexity: 11.0107\n",
            "Avg 10 batchs takes  7.33  min\n",
            "EPOCH  23 batch  700 : Loss: 2.3682 Perplexity: 10.6778\n",
            "Avg 10 batchs takes  7.44  min\n",
            "EPOCH  23 batch  710 : Loss: 2.3457 Perplexity: 10.4407\n",
            "Avg 10 batchs takes  7.54  min\n",
            "EPOCH  23 batch  720 : Loss: 2.3891 Perplexity: 10.9036\n",
            "Avg 10 batchs takes  7.65  min\n",
            "EPOCH  23 batch  730 : Loss: 2.3715 Perplexity: 10.7133\n",
            "Avg 10 batchs takes  7.75  min\n",
            "EPOCH  23 batch  740 : Loss: 2.4266 Perplexity: 11.3208\n",
            "Avg 10 batchs takes  7.85  min\n",
            "EPOCH  23 batch  750 : Loss: 2.318 Perplexity: 10.1553\n",
            "Avg 10 batchs takes  7.96  min\n",
            "EPOCH  23 batch  760 : Loss: 2.3331 Perplexity: 10.3094\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  23 batch  770 : Loss: 2.3976 Perplexity: 10.9972\n",
            "Training loss after one epoch is: 2.288353204727173\n",
            "Time take for an epoch is: 8.09  min\n",
            " ********************* Epoch  23  ******************\n",
            "Test loss   23  is  3.413174397604806\n",
            "Perplexity is:  30.361470828176067\n",
            "Avg distance is:  248.3859126984127\n",
            "Learning rate for epoch  24  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  24 batch  0 : Loss: 2.4411 Perplexity: 11.4853\n",
            " ********* Epoch  24  Average Levenshtein Distance is ****:  282.5625\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  24 batch  10 : Loss: 2.3612 Perplexity: 10.6039\n",
            "Avg 10 batchs takes  0.22  min\n",
            "EPOCH  24 batch  20 : Loss: 2.3534 Perplexity: 10.5208\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  24 batch  30 : Loss: 2.4116 Perplexity: 11.152\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  24 batch  40 : Loss: 2.4516 Perplexity: 11.6069\n",
            "Avg 10 batchs takes  0.54  min\n",
            "EPOCH  24 batch  50 : Loss: 2.3287 Perplexity: 10.2645\n",
            "Avg 10 batchs takes  0.65  min\n",
            "EPOCH  24 batch  60 : Loss: 2.3899 Perplexity: 10.9121\n",
            "Avg 10 batchs takes  0.75  min\n",
            "EPOCH  24 batch  70 : Loss: 2.419 Perplexity: 11.2347\n",
            "Avg 10 batchs takes  0.86  min\n",
            "EPOCH  24 batch  80 : Loss: 2.3551 Perplexity: 10.5389\n",
            "Avg 10 batchs takes  0.96  min\n",
            "EPOCH  24 batch  90 : Loss: 2.3346 Perplexity: 10.3255\n",
            "Avg 10 batchs takes  1.06  min\n",
            "EPOCH  24 batch  100 : Loss: 2.3776 Perplexity: 10.7786\n",
            "Avg 10 batchs takes  1.17  min\n",
            "EPOCH  24 batch  110 : Loss: 2.3685 Perplexity: 10.6817\n",
            "Avg 10 batchs takes  1.27  min\n",
            "EPOCH  24 batch  120 : Loss: 2.3631 Perplexity: 10.624\n",
            "Avg 10 batchs takes  1.37  min\n",
            "EPOCH  24 batch  130 : Loss: 2.401 Perplexity: 11.0342\n",
            "Avg 10 batchs takes  1.47  min\n",
            "EPOCH  24 batch  140 : Loss: 2.3903 Perplexity: 10.9167\n",
            "Avg 10 batchs takes  1.58  min\n",
            "EPOCH  24 batch  150 : Loss: 2.4007 Perplexity: 11.0304\n",
            "Avg 10 batchs takes  1.68  min\n",
            "EPOCH  24 batch  160 : Loss: 2.4406 Perplexity: 11.4797\n",
            "Avg 10 batchs takes  1.79  min\n",
            "EPOCH  24 batch  170 : Loss: 2.4074 Perplexity: 11.1052\n",
            "Avg 10 batchs takes  1.9  min\n",
            "EPOCH  24 batch  180 : Loss: 2.3778 Perplexity: 10.7807\n",
            "Avg 10 batchs takes  2.01  min\n",
            "EPOCH  24 batch  190 : Loss: 2.4058 Perplexity: 11.0869\n",
            "Avg 10 batchs takes  2.11  min\n",
            "EPOCH  24 batch  200 : Loss: 2.266 Perplexity: 9.6405\n",
            "Avg 10 batchs takes  2.22  min\n",
            "EPOCH  24 batch  210 : Loss: 2.4082 Perplexity: 11.114\n",
            "Avg 10 batchs takes  2.32  min\n",
            "EPOCH  24 batch  220 : Loss: 2.2454 Perplexity: 9.4446\n",
            "Avg 10 batchs takes  2.43  min\n",
            "EPOCH  24 batch  230 : Loss: 2.3426 Perplexity: 10.408\n",
            "Avg 10 batchs takes  2.53  min\n",
            "EPOCH  24 batch  240 : Loss: 2.396 Perplexity: 10.9794\n",
            "Avg 10 batchs takes  2.64  min\n",
            "EPOCH  24 batch  250 : Loss: 2.3117 Perplexity: 10.092\n",
            "Avg 10 batchs takes  2.75  min\n",
            "EPOCH  24 batch  260 : Loss: 2.3014 Perplexity: 9.9884\n",
            "Avg 10 batchs takes  2.85  min\n",
            "EPOCH  24 batch  270 : Loss: 2.256 Perplexity: 9.5451\n",
            "Avg 10 batchs takes  2.96  min\n",
            "EPOCH  24 batch  280 : Loss: 2.4089 Perplexity: 11.1217\n",
            "Avg 10 batchs takes  3.06  min\n",
            "EPOCH  24 batch  290 : Loss: 2.3959 Perplexity: 10.9779\n",
            "Avg 10 batchs takes  3.16  min\n",
            "EPOCH  24 batch  300 : Loss: 2.3468 Perplexity: 10.4521\n",
            " ********* Epoch  24  Average Levenshtein Distance is ****:  351.96875\n",
            "Avg 10 batchs takes  3.27  min\n",
            "EPOCH  24 batch  310 : Loss: 2.2955 Perplexity: 9.9299\n",
            "Avg 10 batchs takes  3.38  min\n",
            "EPOCH  24 batch  320 : Loss: 2.4165 Perplexity: 11.207\n",
            "Avg 10 batchs takes  3.49  min\n",
            "EPOCH  24 batch  330 : Loss: 2.5036 Perplexity: 12.2269\n",
            "Avg 10 batchs takes  3.59  min\n",
            "EPOCH  24 batch  340 : Loss: 2.3236 Perplexity: 10.2123\n",
            "Avg 10 batchs takes  3.7  min\n",
            "EPOCH  24 batch  350 : Loss: 2.3464 Perplexity: 10.4475\n",
            "Avg 10 batchs takes  3.8  min\n",
            "EPOCH  24 batch  360 : Loss: 2.3802 Perplexity: 10.8073\n",
            "Avg 10 batchs takes  3.91  min\n",
            "EPOCH  24 batch  370 : Loss: 2.456 Perplexity: 11.6584\n",
            "Avg 10 batchs takes  4.02  min\n",
            "EPOCH  24 batch  380 : Loss: 2.3918 Perplexity: 10.933\n",
            "Avg 10 batchs takes  4.12  min\n",
            "EPOCH  24 batch  390 : Loss: 2.4189 Perplexity: 11.2333\n",
            "Avg 10 batchs takes  4.22  min\n",
            "EPOCH  24 batch  400 : Loss: 2.4024 Perplexity: 11.0497\n",
            "Avg 10 batchs takes  4.33  min\n",
            "EPOCH  24 batch  410 : Loss: 2.355 Perplexity: 10.538\n",
            "Avg 10 batchs takes  4.43  min\n",
            "EPOCH  24 batch  420 : Loss: 2.3797 Perplexity: 10.802\n",
            "Avg 10 batchs takes  4.53  min\n",
            "EPOCH  24 batch  430 : Loss: 2.4698 Perplexity: 11.8206\n",
            "Avg 10 batchs takes  4.63  min\n",
            "EPOCH  24 batch  440 : Loss: 2.4227 Perplexity: 11.2759\n",
            "Avg 10 batchs takes  4.73  min\n",
            "EPOCH  24 batch  450 : Loss: 2.4324 Perplexity: 11.3865\n",
            "Avg 10 batchs takes  4.83  min\n",
            "EPOCH  24 batch  460 : Loss: 2.4054 Perplexity: 11.0831\n",
            "Avg 10 batchs takes  4.94  min\n",
            "EPOCH  24 batch  470 : Loss: 2.4442 Perplexity: 11.5211\n",
            "Avg 10 batchs takes  5.04  min\n",
            "EPOCH  24 batch  480 : Loss: 2.3562 Perplexity: 10.5504\n",
            "Avg 10 batchs takes  5.15  min\n",
            "EPOCH  24 batch  490 : Loss: 2.3609 Perplexity: 10.6007\n",
            "Avg 10 batchs takes  5.25  min\n",
            "EPOCH  24 batch  500 : Loss: 2.386 Perplexity: 10.8705\n",
            "Avg 10 batchs takes  5.36  min\n",
            "EPOCH  24 batch  510 : Loss: 2.3993 Perplexity: 11.0153\n",
            "Avg 10 batchs takes  5.46  min\n",
            "EPOCH  24 batch  520 : Loss: 2.3356 Perplexity: 10.3359\n",
            "Avg 10 batchs takes  5.56  min\n",
            "EPOCH  24 batch  530 : Loss: 2.4012 Perplexity: 11.036\n",
            "Avg 10 batchs takes  5.67  min\n",
            "EPOCH  24 batch  540 : Loss: 2.3752 Perplexity: 10.7535\n",
            "Avg 10 batchs takes  5.77  min\n",
            "EPOCH  24 batch  550 : Loss: 2.2408 Perplexity: 9.4009\n",
            "Avg 10 batchs takes  5.88  min\n",
            "EPOCH  24 batch  560 : Loss: 2.4044 Perplexity: 11.0722\n",
            "Avg 10 batchs takes  5.98  min\n",
            "EPOCH  24 batch  570 : Loss: 2.3286 Perplexity: 10.2634\n",
            "Avg 10 batchs takes  6.09  min\n",
            "EPOCH  24 batch  580 : Loss: 2.4277 Perplexity: 11.3331\n",
            "Avg 10 batchs takes  6.19  min\n",
            "EPOCH  24 batch  590 : Loss: 2.4278 Perplexity: 11.334\n",
            "Avg 10 batchs takes  6.29  min\n",
            "EPOCH  24 batch  600 : Loss: 2.3991 Perplexity: 11.0137\n",
            " ********* Epoch  24  Average Levenshtein Distance is ****:  271.375\n",
            "Avg 10 batchs takes  6.4  min\n",
            "EPOCH  24 batch  610 : Loss: 2.3666 Perplexity: 10.6607\n",
            "Avg 10 batchs takes  6.51  min\n",
            "EPOCH  24 batch  620 : Loss: 2.4455 Perplexity: 11.5365\n",
            "Avg 10 batchs takes  6.62  min\n",
            "EPOCH  24 batch  630 : Loss: 2.3229 Perplexity: 10.2056\n",
            "Avg 10 batchs takes  6.72  min\n",
            "EPOCH  24 batch  640 : Loss: 2.3858 Perplexity: 10.868\n",
            "Avg 10 batchs takes  6.83  min\n",
            "EPOCH  24 batch  650 : Loss: 2.3629 Perplexity: 10.6215\n",
            "Avg 10 batchs takes  6.93  min\n",
            "EPOCH  24 batch  660 : Loss: 2.3539 Perplexity: 10.5264\n",
            "Avg 10 batchs takes  7.04  min\n",
            "EPOCH  24 batch  670 : Loss: 2.3394 Perplexity: 10.3751\n",
            "Avg 10 batchs takes  7.14  min\n",
            "EPOCH  24 batch  680 : Loss: 2.4355 Perplexity: 11.421\n",
            "Avg 10 batchs takes  7.24  min\n",
            "EPOCH  24 batch  690 : Loss: 2.3032 Perplexity: 10.0059\n",
            "Avg 10 batchs takes  7.34  min\n",
            "EPOCH  24 batch  700 : Loss: 2.4029 Perplexity: 11.055\n",
            "Avg 10 batchs takes  7.45  min\n",
            "EPOCH  24 batch  710 : Loss: 2.3497 Perplexity: 10.4819\n",
            "Avg 10 batchs takes  7.55  min\n",
            "EPOCH  24 batch  720 : Loss: 2.3123 Perplexity: 10.0973\n",
            "Avg 10 batchs takes  7.65  min\n",
            "EPOCH  24 batch  730 : Loss: 2.4095 Perplexity: 11.1284\n",
            "Avg 10 batchs takes  7.76  min\n",
            "EPOCH  24 batch  740 : Loss: 2.4647 Perplexity: 11.7601\n",
            "Avg 10 batchs takes  7.86  min\n",
            "EPOCH  24 batch  750 : Loss: 2.4344 Perplexity: 11.4094\n",
            "Avg 10 batchs takes  7.96  min\n",
            "EPOCH  24 batch  760 : Loss: 2.4195 Perplexity: 11.2398\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  24 batch  770 : Loss: 2.3657 Perplexity: 10.6514\n",
            "Training loss after one epoch is: 2.4368090629577637\n",
            "Time take for an epoch is: 8.09  min\n",
            " ********************* Epoch  24  ******************\n",
            "Test loss   24  is  3.746992976324899\n",
            "Perplexity is:  42.3934121491311\n",
            "Avg distance is:  219.79761904761907\n",
            "Learning rate for epoch  25  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  25 batch  0 : Loss: 2.3507 Perplexity: 10.4931\n",
            " ********* Epoch  25  Average Levenshtein Distance is ****:  311.28125\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  25 batch  10 : Loss: 2.3532 Perplexity: 10.5193\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  25 batch  20 : Loss: 2.4254 Perplexity: 11.3071\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  25 batch  30 : Loss: 2.411 Perplexity: 11.145\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  25 batch  40 : Loss: 2.3734 Perplexity: 10.7338\n",
            "Avg 10 batchs takes  0.54  min\n",
            "EPOCH  25 batch  50 : Loss: 2.375 Perplexity: 10.7512\n",
            "Avg 10 batchs takes  0.64  min\n",
            "EPOCH  25 batch  60 : Loss: 2.4489 Perplexity: 11.5761\n",
            "Avg 10 batchs takes  0.75  min\n",
            "EPOCH  25 batch  70 : Loss: 2.3934 Perplexity: 10.9503\n",
            "Avg 10 batchs takes  0.85  min\n",
            "EPOCH  25 batch  80 : Loss: 2.3673 Perplexity: 10.6683\n",
            "Avg 10 batchs takes  0.96  min\n",
            "EPOCH  25 batch  90 : Loss: 2.407 Perplexity: 11.1006\n",
            "Avg 10 batchs takes  1.07  min\n",
            "EPOCH  25 batch  100 : Loss: 2.3455 Perplexity: 10.439\n",
            "Avg 10 batchs takes  1.17  min\n",
            "EPOCH  25 batch  110 : Loss: 2.3558 Perplexity: 10.5466\n",
            "Avg 10 batchs takes  1.28  min\n",
            "EPOCH  25 batch  120 : Loss: 2.3517 Perplexity: 10.5035\n",
            "Avg 10 batchs takes  1.38  min\n",
            "EPOCH  25 batch  130 : Loss: 2.3962 Perplexity: 10.981\n",
            "Avg 10 batchs takes  1.49  min\n",
            "EPOCH  25 batch  140 : Loss: 2.4227 Perplexity: 11.2767\n",
            "Avg 10 batchs takes  1.59  min\n",
            "EPOCH  25 batch  150 : Loss: 2.4146 Perplexity: 11.1847\n",
            "Avg 10 batchs takes  1.69  min\n",
            "EPOCH  25 batch  160 : Loss: 2.3724 Perplexity: 10.7234\n",
            "Avg 10 batchs takes  1.79  min\n",
            "EPOCH  25 batch  170 : Loss: 2.4322 Perplexity: 11.3844\n",
            "Avg 10 batchs takes  1.9  min\n",
            "EPOCH  25 batch  180 : Loss: 2.3811 Perplexity: 10.8173\n",
            "Avg 10 batchs takes  2.0  min\n",
            "EPOCH  25 batch  190 : Loss: 2.3916 Perplexity: 10.9309\n",
            "Avg 10 batchs takes  2.11  min\n",
            "EPOCH  25 batch  200 : Loss: 2.3819 Perplexity: 10.8257\n",
            "Avg 10 batchs takes  2.21  min\n",
            "EPOCH  25 batch  210 : Loss: 2.3769 Perplexity: 10.772\n",
            "Avg 10 batchs takes  2.31  min\n",
            "EPOCH  25 batch  220 : Loss: 2.3397 Perplexity: 10.3784\n",
            "Avg 10 batchs takes  2.42  min\n",
            "EPOCH  25 batch  230 : Loss: 2.3443 Perplexity: 10.4259\n",
            "Avg 10 batchs takes  2.53  min\n",
            "EPOCH  25 batch  240 : Loss: 2.3191 Perplexity: 10.1666\n",
            "Avg 10 batchs takes  2.63  min\n",
            "EPOCH  25 batch  250 : Loss: 2.4088 Perplexity: 11.1201\n",
            "Avg 10 batchs takes  2.73  min\n",
            "EPOCH  25 batch  260 : Loss: 2.3501 Perplexity: 10.4864\n",
            "Avg 10 batchs takes  2.84  min\n",
            "EPOCH  25 batch  270 : Loss: 2.4098 Perplexity: 11.1318\n",
            "Avg 10 batchs takes  2.94  min\n",
            "EPOCH  25 batch  280 : Loss: 2.4343 Perplexity: 11.4077\n",
            "Avg 10 batchs takes  3.05  min\n",
            "EPOCH  25 batch  290 : Loss: 2.4063 Perplexity: 11.0924\n",
            "Avg 10 batchs takes  3.15  min\n",
            "EPOCH  25 batch  300 : Loss: 2.3562 Perplexity: 10.551\n",
            " ********* Epoch  25  Average Levenshtein Distance is ****:  259.8125\n",
            "Avg 10 batchs takes  3.26  min\n",
            "EPOCH  25 batch  310 : Loss: 2.3918 Perplexity: 10.9327\n",
            "Avg 10 batchs takes  3.37  min\n",
            "EPOCH  25 batch  320 : Loss: 2.3811 Perplexity: 10.8168\n",
            "Avg 10 batchs takes  3.48  min\n",
            "EPOCH  25 batch  330 : Loss: 2.32 Perplexity: 10.176\n",
            "Avg 10 batchs takes  3.58  min\n",
            "EPOCH  25 batch  340 : Loss: 2.3518 Perplexity: 10.504\n",
            "Avg 10 batchs takes  3.69  min\n",
            "EPOCH  25 batch  350 : Loss: 2.4448 Perplexity: 11.5286\n",
            "Avg 10 batchs takes  3.8  min\n",
            "EPOCH  25 batch  360 : Loss: 2.4128 Perplexity: 11.1657\n",
            "Avg 10 batchs takes  3.9  min\n",
            "EPOCH  25 batch  370 : Loss: 2.3532 Perplexity: 10.5187\n",
            "Avg 10 batchs takes  4.01  min\n",
            "EPOCH  25 batch  380 : Loss: 2.3281 Perplexity: 10.2582\n",
            "Avg 10 batchs takes  4.11  min\n",
            "EPOCH  25 batch  390 : Loss: 2.3786 Perplexity: 10.7897\n",
            "Avg 10 batchs takes  4.21  min\n",
            "EPOCH  25 batch  400 : Loss: 2.3756 Perplexity: 10.7571\n",
            "Avg 10 batchs takes  4.32  min\n",
            "EPOCH  25 batch  410 : Loss: 2.3632 Perplexity: 10.6244\n",
            "Avg 10 batchs takes  4.42  min\n",
            "EPOCH  25 batch  420 : Loss: 2.4407 Perplexity: 11.4812\n",
            "Avg 10 batchs takes  4.53  min\n",
            "EPOCH  25 batch  430 : Loss: 2.3896 Perplexity: 10.9087\n",
            "Avg 10 batchs takes  4.63  min\n",
            "EPOCH  25 batch  440 : Loss: 2.4418 Perplexity: 11.4934\n",
            "Avg 10 batchs takes  4.74  min\n",
            "EPOCH  25 batch  450 : Loss: 2.3343 Perplexity: 10.3223\n",
            "Avg 10 batchs takes  4.85  min\n",
            "EPOCH  25 batch  460 : Loss: 2.3629 Perplexity: 10.6218\n",
            "Avg 10 batchs takes  4.96  min\n",
            "EPOCH  25 batch  470 : Loss: 2.378 Perplexity: 10.7833\n",
            "Avg 10 batchs takes  5.06  min\n",
            "EPOCH  25 batch  480 : Loss: 2.3672 Perplexity: 10.667\n",
            "Avg 10 batchs takes  5.17  min\n",
            "EPOCH  25 batch  490 : Loss: 2.4269 Perplexity: 11.3241\n",
            "Avg 10 batchs takes  5.27  min\n",
            "EPOCH  25 batch  500 : Loss: 2.3621 Perplexity: 10.6134\n",
            "Avg 10 batchs takes  5.37  min\n",
            "EPOCH  25 batch  510 : Loss: 2.3636 Perplexity: 10.6295\n",
            "Avg 10 batchs takes  5.48  min\n",
            "EPOCH  25 batch  520 : Loss: 2.4717 Perplexity: 11.8431\n",
            "Avg 10 batchs takes  5.58  min\n",
            "EPOCH  25 batch  530 : Loss: 2.3723 Perplexity: 10.7218\n",
            "Avg 10 batchs takes  5.69  min\n",
            "EPOCH  25 batch  540 : Loss: 2.3321 Perplexity: 10.2994\n",
            "Avg 10 batchs takes  5.79  min\n",
            "EPOCH  25 batch  550 : Loss: 2.3543 Perplexity: 10.5311\n",
            "Avg 10 batchs takes  5.9  min\n",
            "EPOCH  25 batch  560 : Loss: 2.3592 Perplexity: 10.5825\n",
            "Avg 10 batchs takes  6.0  min\n",
            "EPOCH  25 batch  570 : Loss: 2.3351 Perplexity: 10.3301\n",
            "Avg 10 batchs takes  6.1  min\n",
            "EPOCH  25 batch  580 : Loss: 2.3305 Perplexity: 10.2835\n",
            "Avg 10 batchs takes  6.21  min\n",
            "EPOCH  25 batch  590 : Loss: 2.3771 Perplexity: 10.7737\n",
            "Avg 10 batchs takes  6.31  min\n",
            "EPOCH  25 batch  600 : Loss: 2.3677 Perplexity: 10.6727\n",
            " ********* Epoch  25  Average Levenshtein Distance is ****:  354.0\n",
            "Avg 10 batchs takes  6.42  min\n",
            "EPOCH  25 batch  610 : Loss: 2.4044 Perplexity: 11.0718\n",
            "Avg 10 batchs takes  6.52  min\n",
            "EPOCH  25 batch  620 : Loss: 2.3609 Perplexity: 10.6008\n",
            "Avg 10 batchs takes  6.62  min\n",
            "EPOCH  25 batch  630 : Loss: 2.4324 Perplexity: 11.3865\n",
            "Avg 10 batchs takes  6.73  min\n",
            "EPOCH  25 batch  640 : Loss: 2.3722 Perplexity: 10.721\n",
            "Avg 10 batchs takes  6.83  min\n",
            "EPOCH  25 batch  650 : Loss: 2.3352 Perplexity: 10.3315\n",
            "Avg 10 batchs takes  6.93  min\n",
            "EPOCH  25 batch  660 : Loss: 2.3903 Perplexity: 10.917\n",
            "Avg 10 batchs takes  7.04  min\n",
            "EPOCH  25 batch  670 : Loss: 2.3461 Perplexity: 10.4446\n",
            "Avg 10 batchs takes  7.14  min\n",
            "EPOCH  25 batch  680 : Loss: 2.3953 Perplexity: 10.9719\n",
            "Avg 10 batchs takes  7.25  min\n",
            "EPOCH  25 batch  690 : Loss: 2.4157 Perplexity: 11.1972\n",
            "Avg 10 batchs takes  7.35  min\n",
            "EPOCH  25 batch  700 : Loss: 2.3899 Perplexity: 10.9127\n",
            "Avg 10 batchs takes  7.46  min\n",
            "EPOCH  25 batch  710 : Loss: 2.3195 Perplexity: 10.1708\n",
            "Avg 10 batchs takes  7.56  min\n",
            "EPOCH  25 batch  720 : Loss: 2.2976 Perplexity: 9.9498\n",
            "Avg 10 batchs takes  7.67  min\n",
            "EPOCH  25 batch  730 : Loss: 2.4306 Perplexity: 11.3655\n",
            "Avg 10 batchs takes  7.77  min\n",
            "EPOCH  25 batch  740 : Loss: 2.3955 Perplexity: 10.9741\n",
            "Avg 10 batchs takes  7.87  min\n",
            "EPOCH  25 batch  750 : Loss: 2.3923 Perplexity: 10.9384\n",
            "Avg 10 batchs takes  7.97  min\n",
            "EPOCH  25 batch  760 : Loss: 2.4223 Perplexity: 11.2713\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  25 batch  770 : Loss: 2.402 Perplexity: 11.0451\n",
            "Training loss after one epoch is: 2.3718667030334473\n",
            "Time take for an epoch is: 8.09  min\n",
            " ********************* Epoch  25  ******************\n",
            "Test loss   25  is  3.7641301495688304\n",
            "Perplexity is:  43.12617621666147\n",
            "Avg distance is:  223.81994047619048\n",
            "Learning rate for epoch  26  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  26 batch  0 : Loss: 2.382 Perplexity: 10.8267\n",
            " ********* Epoch  26  Average Levenshtein Distance is ****:  370.5625\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  26 batch  10 : Loss: 2.4361 Perplexity: 11.4285\n",
            "Avg 10 batchs takes  0.22  min\n",
            "EPOCH  26 batch  20 : Loss: 2.3869 Perplexity: 10.8798\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  26 batch  30 : Loss: 2.4389 Perplexity: 11.4607\n",
            "Avg 10 batchs takes  0.43  min\n",
            "EPOCH  26 batch  40 : Loss: 2.4159 Perplexity: 11.1993\n",
            "Avg 10 batchs takes  0.54  min\n",
            "EPOCH  26 batch  50 : Loss: 2.3702 Perplexity: 10.6992\n",
            "Avg 10 batchs takes  0.64  min\n",
            "EPOCH  26 batch  60 : Loss: 2.4529 Perplexity: 11.6218\n",
            "Avg 10 batchs takes  0.75  min\n",
            "EPOCH  26 batch  70 : Loss: 2.4124 Perplexity: 11.1613\n",
            "Avg 10 batchs takes  0.86  min\n",
            "EPOCH  26 batch  80 : Loss: 2.3847 Perplexity: 10.8554\n",
            "Avg 10 batchs takes  0.96  min\n",
            "EPOCH  26 batch  90 : Loss: 2.3451 Perplexity: 10.4347\n",
            "Avg 10 batchs takes  1.07  min\n",
            "EPOCH  26 batch  100 : Loss: 2.3443 Perplexity: 10.4256\n",
            "Avg 10 batchs takes  1.17  min\n",
            "EPOCH  26 batch  110 : Loss: 2.4193 Perplexity: 11.2378\n",
            "Avg 10 batchs takes  1.28  min\n",
            "EPOCH  26 batch  120 : Loss: 2.3646 Perplexity: 10.6394\n",
            "Avg 10 batchs takes  1.38  min\n",
            "EPOCH  26 batch  130 : Loss: 2.3652 Perplexity: 10.6464\n",
            "Avg 10 batchs takes  1.49  min\n",
            "EPOCH  26 batch  140 : Loss: 2.3957 Perplexity: 10.9758\n",
            "Avg 10 batchs takes  1.6  min\n",
            "EPOCH  26 batch  150 : Loss: 2.3495 Perplexity: 10.4807\n",
            "Avg 10 batchs takes  1.7  min\n",
            "EPOCH  26 batch  160 : Loss: 2.3603 Perplexity: 10.5938\n",
            "Avg 10 batchs takes  1.8  min\n",
            "EPOCH  26 batch  170 : Loss: 2.3648 Perplexity: 10.6424\n",
            "Avg 10 batchs takes  1.91  min\n",
            "EPOCH  26 batch  180 : Loss: 2.3674 Perplexity: 10.6694\n",
            "Avg 10 batchs takes  2.01  min\n",
            "EPOCH  26 batch  190 : Loss: 2.3479 Perplexity: 10.4633\n",
            "Avg 10 batchs takes  2.12  min\n",
            "EPOCH  26 batch  200 : Loss: 2.4133 Perplexity: 11.1702\n",
            "Avg 10 batchs takes  2.22  min\n",
            "EPOCH  26 batch  210 : Loss: 2.4213 Perplexity: 11.2606\n",
            "Avg 10 batchs takes  2.32  min\n",
            "EPOCH  26 batch  220 : Loss: 2.3469 Perplexity: 10.4527\n",
            "Avg 10 batchs takes  2.43  min\n",
            "EPOCH  26 batch  230 : Loss: 2.3857 Perplexity: 10.8662\n",
            "Avg 10 batchs takes  2.53  min\n",
            "EPOCH  26 batch  240 : Loss: 2.373 Perplexity: 10.7292\n",
            "Avg 10 batchs takes  2.63  min\n",
            "EPOCH  26 batch  250 : Loss: 2.3587 Perplexity: 10.5774\n",
            "Avg 10 batchs takes  2.74  min\n",
            "EPOCH  26 batch  260 : Loss: 2.3993 Perplexity: 11.0151\n",
            "Avg 10 batchs takes  2.84  min\n",
            "EPOCH  26 batch  270 : Loss: 2.3586 Perplexity: 10.5757\n",
            "Avg 10 batchs takes  2.95  min\n",
            "EPOCH  26 batch  280 : Loss: 2.3543 Perplexity: 10.5307\n",
            "Avg 10 batchs takes  3.05  min\n",
            "EPOCH  26 batch  290 : Loss: 2.3574 Perplexity: 10.5638\n",
            "Avg 10 batchs takes  3.15  min\n",
            "EPOCH  26 batch  300 : Loss: 2.358 Perplexity: 10.5696\n",
            " ********* Epoch  26  Average Levenshtein Distance is ****:  288.34375\n",
            "Avg 10 batchs takes  3.27  min\n",
            "EPOCH  26 batch  310 : Loss: 2.3419 Perplexity: 10.4009\n",
            "Avg 10 batchs takes  3.38  min\n",
            "EPOCH  26 batch  320 : Loss: 2.3515 Perplexity: 10.5013\n",
            "Avg 10 batchs takes  3.48  min\n",
            "EPOCH  26 batch  330 : Loss: 2.3746 Perplexity: 10.7469\n",
            "Avg 10 batchs takes  3.59  min\n",
            "EPOCH  26 batch  340 : Loss: 2.3343 Perplexity: 10.3218\n",
            "Avg 10 batchs takes  3.7  min\n",
            "EPOCH  26 batch  350 : Loss: 2.3159 Perplexity: 10.1344\n",
            "Avg 10 batchs takes  3.8  min\n",
            "EPOCH  26 batch  360 : Loss: 2.421 Perplexity: 11.2569\n",
            "Avg 10 batchs takes  3.91  min\n",
            "EPOCH  26 batch  370 : Loss: 2.4078 Perplexity: 11.109\n",
            "Avg 10 batchs takes  4.01  min\n",
            "EPOCH  26 batch  380 : Loss: 2.3038 Perplexity: 10.0118\n",
            "Avg 10 batchs takes  4.11  min\n",
            "EPOCH  26 batch  390 : Loss: 2.3841 Perplexity: 10.8493\n",
            "Avg 10 batchs takes  4.22  min\n",
            "EPOCH  26 batch  400 : Loss: 2.3813 Perplexity: 10.8189\n",
            "Avg 10 batchs takes  4.33  min\n",
            "EPOCH  26 batch  410 : Loss: 2.325 Perplexity: 10.2268\n",
            "Avg 10 batchs takes  4.43  min\n",
            "EPOCH  26 batch  420 : Loss: 2.363 Perplexity: 10.6231\n",
            "Avg 10 batchs takes  4.53  min\n",
            "EPOCH  26 batch  430 : Loss: 2.3084 Perplexity: 10.0586\n",
            "Avg 10 batchs takes  4.64  min\n",
            "EPOCH  26 batch  440 : Loss: 2.4123 Perplexity: 11.1592\n",
            "Avg 10 batchs takes  4.75  min\n",
            "EPOCH  26 batch  450 : Loss: 2.3552 Perplexity: 10.5407\n",
            "Avg 10 batchs takes  4.85  min\n",
            "EPOCH  26 batch  460 : Loss: 2.3445 Perplexity: 10.4282\n",
            "Avg 10 batchs takes  4.96  min\n",
            "EPOCH  26 batch  470 : Loss: 2.3931 Perplexity: 10.9478\n",
            "Avg 10 batchs takes  5.06  min\n",
            "EPOCH  26 batch  480 : Loss: 2.4051 Perplexity: 11.0792\n",
            "Avg 10 batchs takes  5.17  min\n",
            "EPOCH  26 batch  490 : Loss: 2.3324 Perplexity: 10.303\n",
            "Avg 10 batchs takes  5.28  min\n",
            "EPOCH  26 batch  500 : Loss: 2.3883 Perplexity: 10.8944\n",
            "Avg 10 batchs takes  5.38  min\n",
            "EPOCH  26 batch  510 : Loss: 2.3048 Perplexity: 10.0225\n",
            "Avg 10 batchs takes  5.48  min\n",
            "EPOCH  26 batch  520 : Loss: 2.3728 Perplexity: 10.7275\n",
            "Avg 10 batchs takes  5.59  min\n",
            "EPOCH  26 batch  530 : Loss: 2.3417 Perplexity: 10.3987\n",
            "Avg 10 batchs takes  5.69  min\n",
            "EPOCH  26 batch  540 : Loss: 2.3376 Perplexity: 10.3561\n",
            "Avg 10 batchs takes  5.8  min\n",
            "EPOCH  26 batch  550 : Loss: 2.2867 Perplexity: 9.8426\n",
            "Avg 10 batchs takes  5.9  min\n",
            "EPOCH  26 batch  560 : Loss: 2.3725 Perplexity: 10.7247\n",
            "Avg 10 batchs takes  6.01  min\n",
            "EPOCH  26 batch  570 : Loss: 2.4058 Perplexity: 11.0869\n",
            "Avg 10 batchs takes  6.11  min\n",
            "EPOCH  26 batch  580 : Loss: 2.3116 Perplexity: 10.091\n",
            "Avg 10 batchs takes  6.22  min\n",
            "EPOCH  26 batch  590 : Loss: 2.3954 Perplexity: 10.9727\n",
            "Avg 10 batchs takes  6.32  min\n",
            "EPOCH  26 batch  600 : Loss: 2.4518 Perplexity: 11.6088\n",
            " ********* Epoch  26  Average Levenshtein Distance is ****:  209.0\n",
            "Avg 10 batchs takes  6.43  min\n",
            "EPOCH  26 batch  610 : Loss: 2.3949 Perplexity: 10.9674\n",
            "Avg 10 batchs takes  6.54  min\n",
            "EPOCH  26 batch  620 : Loss: 2.417 Perplexity: 11.2124\n",
            "Avg 10 batchs takes  6.64  min\n",
            "EPOCH  26 batch  630 : Loss: 2.3957 Perplexity: 10.9755\n",
            "Avg 10 batchs takes  6.75  min\n",
            "EPOCH  26 batch  640 : Loss: 2.3691 Perplexity: 10.6879\n",
            "Avg 10 batchs takes  6.85  min\n",
            "EPOCH  26 batch  650 : Loss: 2.3787 Perplexity: 10.791\n",
            "Avg 10 batchs takes  6.95  min\n",
            "EPOCH  26 batch  660 : Loss: 2.3827 Perplexity: 10.8338\n",
            "Avg 10 batchs takes  7.06  min\n",
            "EPOCH  26 batch  670 : Loss: 2.3834 Perplexity: 10.842\n",
            "Avg 10 batchs takes  7.17  min\n",
            "EPOCH  26 batch  680 : Loss: 2.4069 Perplexity: 11.0994\n",
            "Avg 10 batchs takes  7.27  min\n",
            "EPOCH  26 batch  690 : Loss: 2.3441 Perplexity: 10.4236\n",
            "Avg 10 batchs takes  7.38  min\n",
            "EPOCH  26 batch  700 : Loss: 2.3094 Perplexity: 10.0687\n",
            "Avg 10 batchs takes  7.48  min\n",
            "EPOCH  26 batch  710 : Loss: 2.3876 Perplexity: 10.8878\n",
            "Avg 10 batchs takes  7.58  min\n",
            "EPOCH  26 batch  720 : Loss: 2.3072 Perplexity: 10.0458\n",
            "Avg 10 batchs takes  7.69  min\n",
            "EPOCH  26 batch  730 : Loss: 2.3015 Perplexity: 9.9888\n",
            "Avg 10 batchs takes  7.8  min\n",
            "EPOCH  26 batch  740 : Loss: 2.3168 Perplexity: 10.1434\n",
            "Avg 10 batchs takes  7.91  min\n",
            "EPOCH  26 batch  750 : Loss: 2.3645 Perplexity: 10.6386\n",
            "Avg 10 batchs takes  8.02  min\n",
            "EPOCH  26 batch  760 : Loss: 2.319 Perplexity: 10.1651\n",
            "Avg 10 batchs takes  8.12  min\n",
            "EPOCH  26 batch  770 : Loss: 2.2871 Perplexity: 9.8459\n",
            "Training loss after one epoch is: 2.3624680042266846\n",
            "Time take for an epoch is: 8.14  min\n",
            " ********************* Epoch  26  ******************\n",
            "Test loss   26  is  3.746354648045131\n",
            "Perplexity is:  42.36635987031633\n",
            "Avg distance is:  227.8123015873016\n",
            "Learning rate for epoch  27  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  27 batch  0 : Loss: 2.3333 Perplexity: 10.3124\n",
            " ********* Epoch  27  Average Levenshtein Distance is ****:  275.75\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  27 batch  10 : Loss: 2.3803 Perplexity: 10.8085\n",
            "Avg 10 batchs takes  0.22  min\n",
            "EPOCH  27 batch  20 : Loss: 2.3651 Perplexity: 10.6449\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  27 batch  30 : Loss: 2.2665 Perplexity: 9.6454\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  27 batch  40 : Loss: 2.3151 Perplexity: 10.1258\n",
            "Avg 10 batchs takes  0.55  min\n",
            "EPOCH  27 batch  50 : Loss: 2.3564 Perplexity: 10.5524\n",
            "Avg 10 batchs takes  0.65  min\n",
            "EPOCH  27 batch  60 : Loss: 2.3439 Perplexity: 10.4221\n",
            "Avg 10 batchs takes  0.76  min\n",
            "EPOCH  27 batch  70 : Loss: 2.3333 Perplexity: 10.312\n",
            "Avg 10 batchs takes  0.86  min\n",
            "EPOCH  27 batch  80 : Loss: 2.4044 Perplexity: 11.0713\n",
            "Avg 10 batchs takes  0.96  min\n",
            "EPOCH  27 batch  90 : Loss: 2.4193 Perplexity: 11.2374\n",
            "Avg 10 batchs takes  1.07  min\n",
            "EPOCH  27 batch  100 : Loss: 2.3773 Perplexity: 10.776\n",
            "Avg 10 batchs takes  1.17  min\n",
            "EPOCH  27 batch  110 : Loss: 2.3578 Perplexity: 10.568\n",
            "Avg 10 batchs takes  1.28  min\n",
            "EPOCH  27 batch  120 : Loss: 2.3399 Perplexity: 10.3803\n",
            "Avg 10 batchs takes  1.39  min\n",
            "EPOCH  27 batch  130 : Loss: 2.3669 Perplexity: 10.664\n",
            "Avg 10 batchs takes  1.49  min\n",
            "EPOCH  27 batch  140 : Loss: 2.3563 Perplexity: 10.5521\n",
            "Avg 10 batchs takes  1.6  min\n",
            "EPOCH  27 batch  150 : Loss: 2.3205 Perplexity: 10.1803\n",
            "Avg 10 batchs takes  1.71  min\n",
            "EPOCH  27 batch  160 : Loss: 2.2788 Perplexity: 9.7648\n",
            "Avg 10 batchs takes  1.81  min\n",
            "EPOCH  27 batch  170 : Loss: 2.4447 Perplexity: 11.5272\n",
            "Avg 10 batchs takes  1.92  min\n",
            "EPOCH  27 batch  180 : Loss: 2.412 Perplexity: 11.156\n",
            "Avg 10 batchs takes  2.03  min\n",
            "EPOCH  27 batch  190 : Loss: 2.2934 Perplexity: 9.9086\n",
            "Avg 10 batchs takes  2.13  min\n",
            "EPOCH  27 batch  200 : Loss: 2.3686 Perplexity: 10.6828\n",
            "Avg 10 batchs takes  2.23  min\n",
            "EPOCH  27 batch  210 : Loss: 2.3625 Perplexity: 10.617\n",
            "Avg 10 batchs takes  2.34  min\n",
            "EPOCH  27 batch  220 : Loss: 2.4019 Perplexity: 11.0441\n",
            "Avg 10 batchs takes  2.45  min\n",
            "EPOCH  27 batch  230 : Loss: 2.365 Perplexity: 10.6445\n",
            "Avg 10 batchs takes  2.55  min\n",
            "EPOCH  27 batch  240 : Loss: 2.3203 Perplexity: 10.1791\n",
            "Avg 10 batchs takes  2.66  min\n",
            "EPOCH  27 batch  250 : Loss: 2.3521 Perplexity: 10.5071\n",
            "Avg 10 batchs takes  2.77  min\n",
            "EPOCH  27 batch  260 : Loss: 2.4111 Perplexity: 11.1467\n",
            "Avg 10 batchs takes  2.87  min\n",
            "EPOCH  27 batch  270 : Loss: 2.3918 Perplexity: 10.9332\n",
            "Avg 10 batchs takes  2.98  min\n",
            "EPOCH  27 batch  280 : Loss: 2.3672 Perplexity: 10.667\n",
            "Avg 10 batchs takes  3.08  min\n",
            "EPOCH  27 batch  290 : Loss: 2.3895 Perplexity: 10.9084\n",
            "Avg 10 batchs takes  3.19  min\n",
            "EPOCH  27 batch  300 : Loss: 2.3605 Perplexity: 10.5967\n",
            " ********* Epoch  27  Average Levenshtein Distance is ****:  295.8125\n",
            "Avg 10 batchs takes  3.3  min\n",
            "EPOCH  27 batch  310 : Loss: 2.4126 Perplexity: 11.1628\n",
            "Avg 10 batchs takes  3.41  min\n",
            "EPOCH  27 batch  320 : Loss: 2.3204 Perplexity: 10.1801\n",
            "Avg 10 batchs takes  3.52  min\n",
            "EPOCH  27 batch  330 : Loss: 2.2815 Perplexity: 9.7911\n",
            "Avg 10 batchs takes  3.63  min\n",
            "EPOCH  27 batch  340 : Loss: 2.3842 Perplexity: 10.8508\n",
            "Avg 10 batchs takes  3.73  min\n",
            "EPOCH  27 batch  350 : Loss: 2.4444 Perplexity: 11.5236\n",
            "Avg 10 batchs takes  3.84  min\n",
            "EPOCH  27 batch  360 : Loss: 2.3299 Perplexity: 10.2764\n",
            "Avg 10 batchs takes  3.96  min\n",
            "EPOCH  27 batch  370 : Loss: 2.3493 Perplexity: 10.4783\n",
            "Avg 10 batchs takes  4.06  min\n",
            "EPOCH  27 batch  380 : Loss: 2.3775 Perplexity: 10.7783\n",
            "Avg 10 batchs takes  4.16  min\n",
            "EPOCH  27 batch  390 : Loss: 2.3329 Perplexity: 10.3079\n",
            "Avg 10 batchs takes  4.27  min\n",
            "EPOCH  27 batch  400 : Loss: 2.3497 Perplexity: 10.4823\n",
            "Avg 10 batchs takes  4.38  min\n",
            "EPOCH  27 batch  410 : Loss: 2.2537 Perplexity: 9.5226\n",
            "Avg 10 batchs takes  4.49  min\n",
            "EPOCH  27 batch  420 : Loss: 2.3015 Perplexity: 9.9887\n",
            "Avg 10 batchs takes  4.6  min\n",
            "EPOCH  27 batch  430 : Loss: 2.4068 Perplexity: 11.0988\n",
            "Avg 10 batchs takes  4.71  min\n",
            "EPOCH  27 batch  440 : Loss: 2.3706 Perplexity: 10.7043\n",
            "Avg 10 batchs takes  4.81  min\n",
            "EPOCH  27 batch  450 : Loss: 2.3927 Perplexity: 10.9431\n",
            "Avg 10 batchs takes  4.92  min\n",
            "EPOCH  27 batch  460 : Loss: 2.3813 Perplexity: 10.8187\n",
            "Avg 10 batchs takes  5.02  min\n",
            "EPOCH  27 batch  470 : Loss: 2.3452 Perplexity: 10.4357\n",
            "Avg 10 batchs takes  5.14  min\n",
            "EPOCH  27 batch  480 : Loss: 2.2782 Perplexity: 9.7588\n",
            "Avg 10 batchs takes  5.24  min\n",
            "EPOCH  27 batch  490 : Loss: 2.3049 Perplexity: 10.0232\n",
            "Avg 10 batchs takes  5.35  min\n",
            "EPOCH  27 batch  500 : Loss: 2.3414 Perplexity: 10.3955\n",
            "Avg 10 batchs takes  5.46  min\n",
            "EPOCH  27 batch  510 : Loss: 2.3008 Perplexity: 9.9818\n",
            "Avg 10 batchs takes  5.57  min\n",
            "EPOCH  27 batch  520 : Loss: 2.3443 Perplexity: 10.4259\n",
            "Avg 10 batchs takes  5.68  min\n",
            "EPOCH  27 batch  530 : Loss: 2.3943 Perplexity: 10.961\n",
            "Avg 10 batchs takes  5.79  min\n",
            "EPOCH  27 batch  540 : Loss: 2.4574 Perplexity: 11.6741\n",
            "Avg 10 batchs takes  5.9  min\n",
            "EPOCH  27 batch  550 : Loss: 2.3115 Perplexity: 10.0896\n",
            "Avg 10 batchs takes  6.0  min\n",
            "EPOCH  27 batch  560 : Loss: 2.3476 Perplexity: 10.4602\n",
            "Avg 10 batchs takes  6.11  min\n",
            "EPOCH  27 batch  570 : Loss: 2.3648 Perplexity: 10.642\n",
            "Avg 10 batchs takes  6.21  min\n",
            "EPOCH  27 batch  580 : Loss: 2.3367 Perplexity: 10.3468\n",
            "Avg 10 batchs takes  6.32  min\n",
            "EPOCH  27 batch  590 : Loss: 2.358 Perplexity: 10.5699\n",
            "Avg 10 batchs takes  6.43  min\n",
            "EPOCH  27 batch  600 : Loss: 2.3616 Perplexity: 10.6074\n",
            " ********* Epoch  27  Average Levenshtein Distance is ****:  342.46875\n",
            "Avg 10 batchs takes  6.54  min\n",
            "EPOCH  27 batch  610 : Loss: 2.3412 Perplexity: 10.3939\n",
            "Avg 10 batchs takes  6.64  min\n",
            "EPOCH  27 batch  620 : Loss: 2.3384 Perplexity: 10.3647\n",
            "Avg 10 batchs takes  6.75  min\n",
            "EPOCH  27 batch  630 : Loss: 2.377 Perplexity: 10.7725\n",
            "Avg 10 batchs takes  6.86  min\n",
            "EPOCH  27 batch  640 : Loss: 2.3405 Perplexity: 10.3864\n",
            "Avg 10 batchs takes  6.96  min\n",
            "EPOCH  27 batch  650 : Loss: 2.3152 Perplexity: 10.1267\n",
            "Avg 10 batchs takes  7.07  min\n",
            "EPOCH  27 batch  660 : Loss: 2.4257 Perplexity: 11.3099\n",
            "Avg 10 batchs takes  7.17  min\n",
            "EPOCH  27 batch  670 : Loss: 2.3973 Perplexity: 10.9935\n",
            "Avg 10 batchs takes  7.28  min\n",
            "EPOCH  27 batch  680 : Loss: 2.2473 Perplexity: 9.4622\n",
            "Avg 10 batchs takes  7.39  min\n",
            "EPOCH  27 batch  690 : Loss: 2.4335 Perplexity: 11.3992\n",
            "Avg 10 batchs takes  7.49  min\n",
            "EPOCH  27 batch  700 : Loss: 2.3791 Perplexity: 10.7956\n",
            "Avg 10 batchs takes  7.6  min\n",
            "EPOCH  27 batch  710 : Loss: 2.3829 Perplexity: 10.836\n",
            "Avg 10 batchs takes  7.71  min\n",
            "EPOCH  27 batch  720 : Loss: 2.4492 Perplexity: 11.5792\n",
            "Avg 10 batchs takes  7.81  min\n",
            "EPOCH  27 batch  730 : Loss: 2.2834 Perplexity: 9.8103\n",
            "Avg 10 batchs takes  7.92  min\n",
            "EPOCH  27 batch  740 : Loss: 2.3343 Perplexity: 10.3226\n",
            "Avg 10 batchs takes  8.02  min\n",
            "EPOCH  27 batch  750 : Loss: 2.3014 Perplexity: 9.9879\n",
            "Avg 10 batchs takes  8.13  min\n",
            "EPOCH  27 batch  760 : Loss: 2.2611 Perplexity: 9.5937\n",
            "Avg 10 batchs takes  8.23  min\n",
            "EPOCH  27 batch  770 : Loss: 2.3754 Perplexity: 10.7558\n",
            "Training loss after one epoch is: 2.349471092224121\n",
            "Time take for an epoch is: 8.25  min\n",
            " ********************* Epoch  27  ******************\n",
            "Test loss   27  is  4.219498722893851\n",
            "Perplexity is:  67.99938920754978\n",
            "Avg distance is:  221.16478174603176\n",
            "Learning rate for epoch  28  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  28 batch  0 : Loss: 2.3894 Perplexity: 10.9066\n",
            " ********* Epoch  28  Average Levenshtein Distance is ****:  256.34375\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  28 batch  10 : Loss: 2.3475 Perplexity: 10.4593\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  28 batch  20 : Loss: 2.3656 Perplexity: 10.65\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  28 batch  30 : Loss: 2.3923 Perplexity: 10.9386\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  28 batch  40 : Loss: 2.2523 Perplexity: 9.5099\n",
            "Avg 10 batchs takes  0.55  min\n",
            "EPOCH  28 batch  50 : Loss: 2.3177 Perplexity: 10.1522\n",
            "Avg 10 batchs takes  0.65  min\n",
            "EPOCH  28 batch  60 : Loss: 2.3509 Perplexity: 10.4954\n",
            "Avg 10 batchs takes  0.76  min\n",
            "EPOCH  28 batch  70 : Loss: 2.359 Perplexity: 10.5798\n",
            "Avg 10 batchs takes  0.87  min\n",
            "EPOCH  28 batch  80 : Loss: 2.3163 Perplexity: 10.1385\n",
            "Avg 10 batchs takes  0.98  min\n",
            "EPOCH  28 batch  90 : Loss: 2.3317 Perplexity: 10.2955\n",
            "Avg 10 batchs takes  1.08  min\n",
            "EPOCH  28 batch  100 : Loss: 2.3846 Perplexity: 10.8547\n",
            "Avg 10 batchs takes  1.19  min\n",
            "EPOCH  28 batch  110 : Loss: 2.4102 Perplexity: 11.136\n",
            "Avg 10 batchs takes  1.29  min\n",
            "EPOCH  28 batch  120 : Loss: 2.3848 Perplexity: 10.8568\n",
            "Avg 10 batchs takes  1.4  min\n",
            "EPOCH  28 batch  130 : Loss: 2.322 Perplexity: 10.1962\n",
            "Avg 10 batchs takes  1.5  min\n",
            "EPOCH  28 batch  140 : Loss: 2.3874 Perplexity: 10.8851\n",
            "Avg 10 batchs takes  1.61  min\n",
            "EPOCH  28 batch  150 : Loss: 2.3598 Perplexity: 10.5888\n",
            "Avg 10 batchs takes  1.71  min\n",
            "EPOCH  28 batch  160 : Loss: 2.4081 Perplexity: 11.1129\n",
            "Avg 10 batchs takes  1.82  min\n",
            "EPOCH  28 batch  170 : Loss: 2.4075 Perplexity: 11.1063\n",
            "Avg 10 batchs takes  1.93  min\n",
            "EPOCH  28 batch  180 : Loss: 2.3242 Perplexity: 10.2187\n",
            "Avg 10 batchs takes  2.03  min\n",
            "EPOCH  28 batch  190 : Loss: 2.2984 Perplexity: 9.9579\n",
            "Avg 10 batchs takes  2.14  min\n",
            "EPOCH  28 batch  200 : Loss: 2.3015 Perplexity: 9.9889\n",
            "Avg 10 batchs takes  2.25  min\n",
            "EPOCH  28 batch  210 : Loss: 2.4083 Perplexity: 11.1155\n",
            "Avg 10 batchs takes  2.36  min\n",
            "EPOCH  28 batch  220 : Loss: 2.2459 Perplexity: 9.4488\n",
            "Avg 10 batchs takes  2.47  min\n",
            "EPOCH  28 batch  230 : Loss: 2.3019 Perplexity: 9.9932\n",
            "Avg 10 batchs takes  2.57  min\n",
            "EPOCH  28 batch  240 : Loss: 2.3527 Perplexity: 10.514\n",
            "Avg 10 batchs takes  2.67  min\n",
            "EPOCH  28 batch  250 : Loss: 2.4189 Perplexity: 11.2334\n",
            "Avg 10 batchs takes  2.79  min\n",
            "EPOCH  28 batch  260 : Loss: 2.3254 Perplexity: 10.2306\n",
            "Avg 10 batchs takes  2.89  min\n",
            "EPOCH  28 batch  270 : Loss: 2.3279 Perplexity: 10.2561\n",
            "Avg 10 batchs takes  3.0  min\n",
            "EPOCH  28 batch  280 : Loss: 2.3206 Perplexity: 10.1815\n",
            "Avg 10 batchs takes  3.1  min\n",
            "EPOCH  28 batch  290 : Loss: 2.3385 Perplexity: 10.3658\n",
            "Avg 10 batchs takes  3.21  min\n",
            "EPOCH  28 batch  300 : Loss: 2.301 Perplexity: 9.9846\n",
            " ********* Epoch  28  Average Levenshtein Distance is ****:  309.15625\n",
            "Avg 10 batchs takes  3.33  min\n",
            "EPOCH  28 batch  310 : Loss: 2.3597 Perplexity: 10.5877\n",
            "Avg 10 batchs takes  3.44  min\n",
            "EPOCH  28 batch  320 : Loss: 2.32 Perplexity: 10.176\n",
            "Avg 10 batchs takes  3.54  min\n",
            "EPOCH  28 batch  330 : Loss: 2.3168 Perplexity: 10.1435\n",
            "Avg 10 batchs takes  3.65  min\n",
            "EPOCH  28 batch  340 : Loss: 2.4021 Perplexity: 11.046\n",
            "Avg 10 batchs takes  3.75  min\n",
            "EPOCH  28 batch  350 : Loss: 2.2328 Perplexity: 9.326\n",
            "Avg 10 batchs takes  3.87  min\n",
            "EPOCH  28 batch  360 : Loss: 2.3134 Perplexity: 10.109\n",
            "Avg 10 batchs takes  3.97  min\n",
            "EPOCH  28 batch  370 : Loss: 2.4142 Perplexity: 11.1804\n",
            "Avg 10 batchs takes  4.08  min\n",
            "EPOCH  28 batch  380 : Loss: 2.2877 Perplexity: 9.8525\n",
            "Avg 10 batchs takes  4.19  min\n",
            "EPOCH  28 batch  390 : Loss: 2.3627 Perplexity: 10.6191\n",
            "Avg 10 batchs takes  4.29  min\n",
            "EPOCH  28 batch  400 : Loss: 2.3653 Perplexity: 10.6475\n",
            "Avg 10 batchs takes  4.4  min\n",
            "EPOCH  28 batch  410 : Loss: 2.3375 Perplexity: 10.3557\n",
            "Avg 10 batchs takes  4.5  min\n",
            "EPOCH  28 batch  420 : Loss: 2.3627 Perplexity: 10.6195\n",
            "Avg 10 batchs takes  4.61  min\n",
            "EPOCH  28 batch  430 : Loss: 2.3651 Perplexity: 10.6448\n",
            "Avg 10 batchs takes  4.72  min\n",
            "EPOCH  28 batch  440 : Loss: 2.4504 Perplexity: 11.5926\n",
            "Avg 10 batchs takes  4.82  min\n",
            "EPOCH  28 batch  450 : Loss: 2.3518 Perplexity: 10.5041\n",
            "Avg 10 batchs takes  4.93  min\n",
            "EPOCH  28 batch  460 : Loss: 2.3553 Perplexity: 10.5408\n",
            "Avg 10 batchs takes  5.04  min\n",
            "EPOCH  28 batch  470 : Loss: 2.358 Perplexity: 10.5694\n",
            "Avg 10 batchs takes  5.14  min\n",
            "EPOCH  28 batch  480 : Loss: 2.3858 Perplexity: 10.8681\n",
            "Avg 10 batchs takes  5.25  min\n",
            "EPOCH  28 batch  490 : Loss: 2.3259 Perplexity: 10.2357\n",
            "Avg 10 batchs takes  5.36  min\n",
            "EPOCH  28 batch  500 : Loss: 2.3469 Perplexity: 10.4529\n",
            "Avg 10 batchs takes  5.46  min\n",
            "EPOCH  28 batch  510 : Loss: 2.4092 Perplexity: 11.1255\n",
            "Avg 10 batchs takes  5.57  min\n",
            "EPOCH  28 batch  520 : Loss: 2.4093 Perplexity: 11.1258\n",
            "Avg 10 batchs takes  5.68  min\n",
            "EPOCH  28 batch  530 : Loss: 2.3441 Perplexity: 10.4241\n",
            "Avg 10 batchs takes  5.79  min\n",
            "EPOCH  28 batch  540 : Loss: 2.3751 Perplexity: 10.752\n",
            "Avg 10 batchs takes  5.9  min\n",
            "EPOCH  28 batch  550 : Loss: 2.2397 Perplexity: 9.3906\n",
            "Avg 10 batchs takes  6.0  min\n",
            "EPOCH  28 batch  560 : Loss: 2.3985 Perplexity: 11.0065\n",
            "Avg 10 batchs takes  6.12  min\n",
            "EPOCH  28 batch  570 : Loss: 2.3154 Perplexity: 10.1287\n",
            "Avg 10 batchs takes  6.22  min\n",
            "EPOCH  28 batch  580 : Loss: 2.3809 Perplexity: 10.8146\n",
            "Avg 10 batchs takes  6.33  min\n",
            "EPOCH  28 batch  590 : Loss: 2.3635 Perplexity: 10.6285\n",
            "Avg 10 batchs takes  6.44  min\n",
            "EPOCH  28 batch  600 : Loss: 2.3368 Perplexity: 10.3486\n",
            " ********* Epoch  28  Average Levenshtein Distance is ****:  283.96875\n",
            "Avg 10 batchs takes  6.54  min\n",
            "EPOCH  28 batch  610 : Loss: 2.2553 Perplexity: 9.5377\n",
            "Avg 10 batchs takes  6.65  min\n",
            "EPOCH  28 batch  620 : Loss: 2.363 Perplexity: 10.6226\n",
            "Avg 10 batchs takes  6.75  min\n",
            "EPOCH  28 batch  630 : Loss: 2.3247 Perplexity: 10.2232\n",
            "Avg 10 batchs takes  6.86  min\n",
            "EPOCH  28 batch  640 : Loss: 2.4021 Perplexity: 11.046\n",
            "Avg 10 batchs takes  6.97  min\n",
            "EPOCH  28 batch  650 : Loss: 2.3477 Perplexity: 10.4611\n",
            "Avg 10 batchs takes  7.08  min\n",
            "EPOCH  28 batch  660 : Loss: 2.3815 Perplexity: 10.8216\n",
            "Avg 10 batchs takes  7.19  min\n",
            "EPOCH  28 batch  670 : Loss: 2.2909 Perplexity: 9.8838\n",
            "Avg 10 batchs takes  7.3  min\n",
            "EPOCH  28 batch  680 : Loss: 2.3619 Perplexity: 10.6112\n",
            "Avg 10 batchs takes  7.41  min\n",
            "EPOCH  28 batch  690 : Loss: 2.3955 Perplexity: 10.9737\n",
            "Avg 10 batchs takes  7.52  min\n",
            "EPOCH  28 batch  700 : Loss: 2.3505 Perplexity: 10.4904\n",
            "Avg 10 batchs takes  7.62  min\n",
            "EPOCH  28 batch  710 : Loss: 2.3818 Perplexity: 10.8242\n",
            "Avg 10 batchs takes  7.73  min\n",
            "EPOCH  28 batch  720 : Loss: 2.3695 Perplexity: 10.6918\n",
            "Avg 10 batchs takes  7.84  min\n",
            "EPOCH  28 batch  730 : Loss: 2.3298 Perplexity: 10.2764\n",
            "Avg 10 batchs takes  7.95  min\n",
            "EPOCH  28 batch  740 : Loss: 2.3726 Perplexity: 10.7254\n",
            "Avg 10 batchs takes  8.06  min\n",
            "EPOCH  28 batch  750 : Loss: 2.3738 Perplexity: 10.7376\n",
            "Avg 10 batchs takes  8.17  min\n",
            "EPOCH  28 batch  760 : Loss: 2.3379 Perplexity: 10.3599\n",
            "Avg 10 batchs takes  8.27  min\n",
            "EPOCH  28 batch  770 : Loss: 2.3035 Perplexity: 10.0094\n",
            "Training loss after one epoch is: 2.362028121948242\n",
            "Time take for an epoch is: 8.29  min\n",
            " ********************* Epoch  28  ******************\n",
            "Test loss   28  is  4.391519893918718\n",
            "Perplexity is:  80.76307705290505\n",
            "Avg distance is:  276.11577380952383\n",
            "Learning rate for epoch  29  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  29 batch  0 : Loss: 2.392 Perplexity: 10.9352\n",
            " ********* Epoch  29  Average Levenshtein Distance is ****:  255.53125\n",
            "Avg 10 batchs takes  0.13  min\n",
            "EPOCH  29 batch  10 : Loss: 2.3885 Perplexity: 10.8967\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  29 batch  20 : Loss: 2.3412 Perplexity: 10.3933\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  29 batch  30 : Loss: 2.3448 Perplexity: 10.4315\n",
            "Avg 10 batchs takes  0.45  min\n",
            "EPOCH  29 batch  40 : Loss: 2.3765 Perplexity: 10.7672\n",
            "Avg 10 batchs takes  0.55  min\n",
            "EPOCH  29 batch  50 : Loss: 2.3307 Perplexity: 10.2848\n",
            "Avg 10 batchs takes  0.66  min\n",
            "EPOCH  29 batch  60 : Loss: 2.3791 Perplexity: 10.7949\n",
            "Avg 10 batchs takes  0.77  min\n",
            "EPOCH  29 batch  70 : Loss: 2.3978 Perplexity: 10.9986\n",
            "Avg 10 batchs takes  0.88  min\n",
            "EPOCH  29 batch  80 : Loss: 2.2724 Perplexity: 9.7029\n",
            "Avg 10 batchs takes  0.98  min\n",
            "EPOCH  29 batch  90 : Loss: 2.4162 Perplexity: 11.2033\n",
            "Avg 10 batchs takes  1.09  min\n",
            "EPOCH  29 batch  100 : Loss: 2.2977 Perplexity: 9.951\n",
            "Avg 10 batchs takes  1.2  min\n",
            "EPOCH  29 batch  110 : Loss: 2.396 Perplexity: 10.9789\n",
            "Avg 10 batchs takes  1.31  min\n",
            "EPOCH  29 batch  120 : Loss: 2.3678 Perplexity: 10.6743\n",
            "Avg 10 batchs takes  1.42  min\n",
            "EPOCH  29 batch  130 : Loss: 2.3553 Perplexity: 10.541\n",
            "Avg 10 batchs takes  1.53  min\n",
            "EPOCH  29 batch  140 : Loss: 2.3604 Perplexity: 10.5948\n",
            "Avg 10 batchs takes  1.64  min\n",
            "EPOCH  29 batch  150 : Loss: 2.3007 Perplexity: 9.9815\n",
            "Avg 10 batchs takes  1.74  min\n",
            "EPOCH  29 batch  160 : Loss: 2.37 Perplexity: 10.6972\n",
            "Avg 10 batchs takes  1.84  min\n",
            "EPOCH  29 batch  170 : Loss: 2.2933 Perplexity: 9.9077\n",
            "Avg 10 batchs takes  1.95  min\n",
            "EPOCH  29 batch  180 : Loss: 2.3723 Perplexity: 10.7216\n",
            "Avg 10 batchs takes  2.06  min\n",
            "EPOCH  29 batch  190 : Loss: 2.3255 Perplexity: 10.2318\n",
            "Avg 10 batchs takes  2.16  min\n",
            "EPOCH  29 batch  200 : Loss: 2.3407 Perplexity: 10.389\n",
            "Avg 10 batchs takes  2.27  min\n",
            "EPOCH  29 batch  210 : Loss: 2.3879 Perplexity: 10.8907\n",
            "Avg 10 batchs takes  2.38  min\n",
            "EPOCH  29 batch  220 : Loss: 2.3109 Perplexity: 10.0831\n",
            "Avg 10 batchs takes  2.49  min\n",
            "EPOCH  29 batch  230 : Loss: 2.3505 Perplexity: 10.4905\n",
            "Avg 10 batchs takes  2.6  min\n",
            "EPOCH  29 batch  240 : Loss: 2.3188 Perplexity: 10.1638\n",
            "Avg 10 batchs takes  2.71  min\n",
            "EPOCH  29 batch  250 : Loss: 2.3681 Perplexity: 10.6773\n",
            "Avg 10 batchs takes  2.82  min\n",
            "EPOCH  29 batch  260 : Loss: 2.3251 Perplexity: 10.2274\n",
            "Avg 10 batchs takes  2.92  min\n",
            "EPOCH  29 batch  270 : Loss: 2.3332 Perplexity: 10.3104\n",
            "Avg 10 batchs takes  3.03  min\n",
            "EPOCH  29 batch  280 : Loss: 2.3886 Perplexity: 10.8986\n",
            "Avg 10 batchs takes  3.13  min\n",
            "EPOCH  29 batch  290 : Loss: 2.2666 Perplexity: 9.6465\n",
            "Avg 10 batchs takes  3.24  min\n",
            "EPOCH  29 batch  300 : Loss: 2.3849 Perplexity: 10.8576\n",
            " ********* Epoch  29  Average Levenshtein Distance is ****:  325.875\n",
            "Avg 10 batchs takes  3.35  min\n",
            "EPOCH  29 batch  310 : Loss: 2.4944 Perplexity: 12.1141\n",
            "Avg 10 batchs takes  3.46  min\n",
            "EPOCH  29 batch  320 : Loss: 2.2585 Perplexity: 9.569\n",
            "Avg 10 batchs takes  3.57  min\n",
            "EPOCH  29 batch  330 : Loss: 2.3612 Perplexity: 10.6037\n",
            "Avg 10 batchs takes  3.68  min\n",
            "EPOCH  29 batch  340 : Loss: 2.3363 Perplexity: 10.3433\n",
            "Avg 10 batchs takes  3.78  min\n",
            "EPOCH  29 batch  350 : Loss: 2.3208 Perplexity: 10.1843\n",
            "Avg 10 batchs takes  3.89  min\n",
            "EPOCH  29 batch  360 : Loss: 2.3818 Perplexity: 10.8247\n",
            "Avg 10 batchs takes  4.0  min\n",
            "EPOCH  29 batch  370 : Loss: 2.2767 Perplexity: 9.7442\n",
            "Avg 10 batchs takes  4.11  min\n",
            "EPOCH  29 batch  380 : Loss: 2.3213 Perplexity: 10.1893\n",
            "Avg 10 batchs takes  4.22  min\n",
            "EPOCH  29 batch  390 : Loss: 2.4152 Perplexity: 11.1919\n",
            "Avg 10 batchs takes  4.33  min\n",
            "EPOCH  29 batch  400 : Loss: 2.4201 Perplexity: 11.2472\n",
            "Avg 10 batchs takes  4.43  min\n",
            "EPOCH  29 batch  410 : Loss: 2.3209 Perplexity: 10.1846\n",
            "Avg 10 batchs takes  4.54  min\n",
            "EPOCH  29 batch  420 : Loss: 2.399 Perplexity: 11.012\n",
            "Avg 10 batchs takes  4.65  min\n",
            "EPOCH  29 batch  430 : Loss: 2.3208 Perplexity: 10.1833\n",
            "Avg 10 batchs takes  4.76  min\n",
            "EPOCH  29 batch  440 : Loss: 2.4052 Perplexity: 11.0808\n",
            "Avg 10 batchs takes  4.87  min\n",
            "EPOCH  29 batch  450 : Loss: 2.3781 Perplexity: 10.7839\n",
            "Avg 10 batchs takes  4.98  min\n",
            "EPOCH  29 batch  460 : Loss: 2.4152 Perplexity: 11.1916\n",
            "Avg 10 batchs takes  5.08  min\n",
            "EPOCH  29 batch  470 : Loss: 2.3817 Perplexity: 10.8235\n",
            "Avg 10 batchs takes  5.19  min\n",
            "EPOCH  29 batch  480 : Loss: 2.3283 Perplexity: 10.2603\n",
            "Avg 10 batchs takes  5.29  min\n",
            "EPOCH  29 batch  490 : Loss: 2.3137 Perplexity: 10.1115\n",
            "Avg 10 batchs takes  5.4  min\n",
            "EPOCH  29 batch  500 : Loss: 2.2967 Perplexity: 9.9414\n",
            "Avg 10 batchs takes  5.51  min\n",
            "EPOCH  29 batch  510 : Loss: 2.3601 Perplexity: 10.5918\n",
            "Avg 10 batchs takes  5.61  min\n",
            "EPOCH  29 batch  520 : Loss: 2.39 Perplexity: 10.9132\n",
            "Avg 10 batchs takes  5.73  min\n",
            "EPOCH  29 batch  530 : Loss: 2.2942 Perplexity: 9.9169\n",
            "Avg 10 batchs takes  5.83  min\n",
            "EPOCH  29 batch  540 : Loss: 2.3627 Perplexity: 10.6193\n",
            "Avg 10 batchs takes  5.94  min\n",
            "EPOCH  29 batch  550 : Loss: 2.3553 Perplexity: 10.5411\n",
            "Avg 10 batchs takes  6.06  min\n",
            "EPOCH  29 batch  560 : Loss: 2.3027 Perplexity: 10.0009\n",
            "Avg 10 batchs takes  6.16  min\n",
            "EPOCH  29 batch  570 : Loss: 2.3843 Perplexity: 10.8518\n",
            "Avg 10 batchs takes  6.27  min\n",
            "EPOCH  29 batch  580 : Loss: 2.3121 Perplexity: 10.0959\n",
            "Avg 10 batchs takes  6.38  min\n",
            "EPOCH  29 batch  590 : Loss: 2.2617 Perplexity: 9.5996\n",
            "Avg 10 batchs takes  6.49  min\n",
            "EPOCH  29 batch  600 : Loss: 2.442 Perplexity: 11.4957\n",
            " ********* Epoch  29  Average Levenshtein Distance is ****:  240.78125\n",
            "Avg 10 batchs takes  6.6  min\n",
            "EPOCH  29 batch  610 : Loss: 2.293 Perplexity: 9.9042\n",
            "Avg 10 batchs takes  6.71  min\n",
            "EPOCH  29 batch  620 : Loss: 2.3902 Perplexity: 10.916\n",
            "Avg 10 batchs takes  6.82  min\n",
            "EPOCH  29 batch  630 : Loss: 2.4159 Perplexity: 11.2002\n",
            "Avg 10 batchs takes  6.94  min\n",
            "EPOCH  29 batch  640 : Loss: 2.3133 Perplexity: 10.1073\n",
            "Avg 10 batchs takes  7.05  min\n",
            "EPOCH  29 batch  650 : Loss: 2.3317 Perplexity: 10.2953\n",
            "Avg 10 batchs takes  7.15  min\n",
            "EPOCH  29 batch  660 : Loss: 2.3209 Perplexity: 10.1851\n",
            "Avg 10 batchs takes  7.26  min\n",
            "EPOCH  29 batch  670 : Loss: 2.3926 Perplexity: 10.9415\n",
            "Avg 10 batchs takes  7.37  min\n",
            "EPOCH  29 batch  680 : Loss: 2.3845 Perplexity: 10.8535\n",
            "Avg 10 batchs takes  7.48  min\n",
            "EPOCH  29 batch  690 : Loss: 2.3073 Perplexity: 10.047\n",
            "Avg 10 batchs takes  7.59  min\n",
            "EPOCH  29 batch  700 : Loss: 2.3293 Perplexity: 10.2709\n",
            "Avg 10 batchs takes  7.7  min\n",
            "EPOCH  29 batch  710 : Loss: 2.3007 Perplexity: 9.9814\n",
            "Avg 10 batchs takes  7.8  min\n",
            "EPOCH  29 batch  720 : Loss: 2.3328 Perplexity: 10.3071\n",
            "Avg 10 batchs takes  7.91  min\n",
            "EPOCH  29 batch  730 : Loss: 2.2747 Perplexity: 9.7251\n",
            "Avg 10 batchs takes  8.02  min\n",
            "EPOCH  29 batch  740 : Loss: 2.3386 Perplexity: 10.3669\n",
            "Avg 10 batchs takes  8.12  min\n",
            "EPOCH  29 batch  750 : Loss: 2.3413 Perplexity: 10.3943\n",
            "Avg 10 batchs takes  8.24  min\n",
            "EPOCH  29 batch  760 : Loss: 2.3801 Perplexity: 10.8058\n",
            "Avg 10 batchs takes  8.34  min\n",
            "EPOCH  29 batch  770 : Loss: 2.3385 Perplexity: 10.3661\n",
            "Training loss after one epoch is: 2.3185272216796875\n",
            "Time take for an epoch is: 8.36  min\n",
            " ********************* Epoch  29  ******************\n",
            "Test loss   29  is  3.7610435213361466\n",
            "Perplexity is:  42.993266969822145\n",
            "Avg distance is:  171.56398809523807\n",
            "Learning rate for epoch  30  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  30 batch  0 : Loss: 2.2834 Perplexity: 9.8102\n",
            " ********* Epoch  30  Average Levenshtein Distance is ****:  159.40625\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  30 batch  10 : Loss: 2.2971 Perplexity: 9.945\n",
            "Avg 10 batchs takes  0.24  min\n",
            "EPOCH  30 batch  20 : Loss: 2.3949 Perplexity: 10.9666\n",
            "Avg 10 batchs takes  0.35  min\n",
            "EPOCH  30 batch  30 : Loss: 2.3883 Perplexity: 10.8953\n",
            "Avg 10 batchs takes  0.46  min\n",
            "EPOCH  30 batch  40 : Loss: 2.3752 Perplexity: 10.7527\n",
            "Avg 10 batchs takes  0.56  min\n",
            "EPOCH  30 batch  50 : Loss: 2.4039 Perplexity: 11.0667\n",
            "Avg 10 batchs takes  0.67  min\n",
            "EPOCH  30 batch  60 : Loss: 2.3659 Perplexity: 10.6539\n",
            "Avg 10 batchs takes  0.78  min\n",
            "EPOCH  30 batch  70 : Loss: 2.3957 Perplexity: 10.9762\n",
            "Avg 10 batchs takes  0.89  min\n",
            "EPOCH  30 batch  80 : Loss: 2.2289 Perplexity: 9.2897\n",
            "Avg 10 batchs takes  1.0  min\n",
            "EPOCH  30 batch  90 : Loss: 2.3621 Perplexity: 10.6137\n",
            "Avg 10 batchs takes  1.11  min\n",
            "EPOCH  30 batch  100 : Loss: 2.3393 Perplexity: 10.3742\n",
            "Avg 10 batchs takes  1.21  min\n",
            "EPOCH  30 batch  110 : Loss: 2.3073 Perplexity: 10.0473\n",
            "Avg 10 batchs takes  1.32  min\n",
            "EPOCH  30 batch  120 : Loss: 2.3482 Perplexity: 10.4668\n",
            "Avg 10 batchs takes  1.43  min\n",
            "EPOCH  30 batch  130 : Loss: 2.3611 Perplexity: 10.6029\n",
            "Avg 10 batchs takes  1.53  min\n",
            "EPOCH  30 batch  140 : Loss: 2.2357 Perplexity: 9.3529\n",
            "Avg 10 batchs takes  1.65  min\n",
            "EPOCH  30 batch  150 : Loss: 2.262 Perplexity: 9.6024\n",
            "Avg 10 batchs takes  1.75  min\n",
            "EPOCH  30 batch  160 : Loss: 2.3975 Perplexity: 10.9961\n",
            "Avg 10 batchs takes  1.86  min\n",
            "EPOCH  30 batch  170 : Loss: 2.3907 Perplexity: 10.9214\n",
            "Avg 10 batchs takes  1.97  min\n",
            "EPOCH  30 batch  180 : Loss: 2.3524 Perplexity: 10.5105\n",
            "Avg 10 batchs takes  2.07  min\n",
            "EPOCH  30 batch  190 : Loss: 2.2808 Perplexity: 9.7844\n",
            "Avg 10 batchs takes  2.18  min\n",
            "EPOCH  30 batch  200 : Loss: 2.3457 Perplexity: 10.4403\n",
            "Avg 10 batchs takes  2.29  min\n",
            "EPOCH  30 batch  210 : Loss: 2.3322 Perplexity: 10.3008\n",
            "Avg 10 batchs takes  2.4  min\n",
            "EPOCH  30 batch  220 : Loss: 2.379 Perplexity: 10.7945\n",
            "Avg 10 batchs takes  2.5  min\n",
            "EPOCH  30 batch  230 : Loss: 2.372 Perplexity: 10.7183\n",
            "Avg 10 batchs takes  2.61  min\n",
            "EPOCH  30 batch  240 : Loss: 2.4348 Perplexity: 11.4141\n",
            "Avg 10 batchs takes  2.72  min\n",
            "EPOCH  30 batch  250 : Loss: 2.3616 Perplexity: 10.6077\n",
            "Avg 10 batchs takes  2.83  min\n",
            "EPOCH  30 batch  260 : Loss: 2.303 Perplexity: 10.0045\n",
            "Avg 10 batchs takes  2.94  min\n",
            "EPOCH  30 batch  270 : Loss: 2.3254 Perplexity: 10.2313\n",
            "Avg 10 batchs takes  3.05  min\n",
            "EPOCH  30 batch  280 : Loss: 2.3572 Perplexity: 10.5617\n",
            "Avg 10 batchs takes  3.16  min\n",
            "EPOCH  30 batch  290 : Loss: 2.3723 Perplexity: 10.7223\n",
            "Avg 10 batchs takes  3.27  min\n",
            "EPOCH  30 batch  300 : Loss: 2.3857 Perplexity: 10.8663\n",
            " ********* Epoch  30  Average Levenshtein Distance is ****:  265.96875\n",
            "Avg 10 batchs takes  3.38  min\n",
            "EPOCH  30 batch  310 : Loss: 2.3443 Perplexity: 10.4255\n",
            "Avg 10 batchs takes  3.49  min\n",
            "EPOCH  30 batch  320 : Loss: 2.4426 Perplexity: 11.5029\n",
            "Avg 10 batchs takes  3.6  min\n",
            "EPOCH  30 batch  330 : Loss: 2.3201 Perplexity: 10.177\n",
            "Avg 10 batchs takes  3.71  min\n",
            "EPOCH  30 batch  340 : Loss: 2.3283 Perplexity: 10.2607\n",
            "Avg 10 batchs takes  3.82  min\n",
            "EPOCH  30 batch  350 : Loss: 2.2528 Perplexity: 9.5144\n",
            "Avg 10 batchs takes  3.93  min\n",
            "EPOCH  30 batch  360 : Loss: 2.3374 Perplexity: 10.3545\n",
            "Avg 10 batchs takes  4.04  min\n",
            "EPOCH  30 batch  370 : Loss: 2.2959 Perplexity: 9.9338\n",
            "Avg 10 batchs takes  4.15  min\n",
            "EPOCH  30 batch  380 : Loss: 2.3652 Perplexity: 10.6466\n",
            "Avg 10 batchs takes  4.25  min\n",
            "EPOCH  30 batch  390 : Loss: 2.3372 Perplexity: 10.352\n",
            "Avg 10 batchs takes  4.36  min\n",
            "EPOCH  30 batch  400 : Loss: 2.2775 Perplexity: 9.7518\n",
            "Avg 10 batchs takes  4.47  min\n",
            "EPOCH  30 batch  410 : Loss: 2.3277 Perplexity: 10.254\n",
            "Avg 10 batchs takes  4.58  min\n",
            "EPOCH  30 batch  420 : Loss: 2.3359 Perplexity: 10.3386\n",
            "Avg 10 batchs takes  4.69  min\n",
            "EPOCH  30 batch  430 : Loss: 2.2011 Perplexity: 9.0346\n",
            "Avg 10 batchs takes  4.8  min\n",
            "EPOCH  30 batch  440 : Loss: 2.3718 Perplexity: 10.7172\n",
            "Avg 10 batchs takes  4.9  min\n",
            "EPOCH  30 batch  450 : Loss: 2.2987 Perplexity: 9.9615\n",
            "Avg 10 batchs takes  5.01  min\n",
            "EPOCH  30 batch  460 : Loss: 2.3586 Perplexity: 10.5757\n",
            "Avg 10 batchs takes  5.12  min\n",
            "EPOCH  30 batch  470 : Loss: 2.2584 Perplexity: 9.5677\n",
            "Avg 10 batchs takes  5.23  min\n",
            "EPOCH  30 batch  480 : Loss: 2.3153 Perplexity: 10.1275\n",
            "Avg 10 batchs takes  5.34  min\n",
            "EPOCH  30 batch  490 : Loss: 2.3515 Perplexity: 10.5015\n",
            "Avg 10 batchs takes  5.45  min\n",
            "EPOCH  30 batch  500 : Loss: 2.3881 Perplexity: 10.8926\n",
            "Avg 10 batchs takes  5.55  min\n",
            "EPOCH  30 batch  510 : Loss: 2.3328 Perplexity: 10.3071\n",
            "Avg 10 batchs takes  5.66  min\n",
            "EPOCH  30 batch  520 : Loss: 2.3531 Perplexity: 10.5178\n",
            "Avg 10 batchs takes  5.77  min\n",
            "EPOCH  30 batch  530 : Loss: 2.2548 Perplexity: 9.5333\n",
            "Avg 10 batchs takes  5.88  min\n",
            "EPOCH  30 batch  540 : Loss: 2.3585 Perplexity: 10.575\n",
            "Avg 10 batchs takes  5.99  min\n",
            "EPOCH  30 batch  550 : Loss: 2.3803 Perplexity: 10.8084\n",
            "Avg 10 batchs takes  6.1  min\n",
            "EPOCH  30 batch  560 : Loss: 2.3859 Perplexity: 10.8692\n",
            "Avg 10 batchs takes  6.21  min\n",
            "EPOCH  30 batch  570 : Loss: 2.3672 Perplexity: 10.667\n",
            "Avg 10 batchs takes  6.32  min\n",
            "EPOCH  30 batch  580 : Loss: 2.3051 Perplexity: 10.0254\n",
            "Avg 10 batchs takes  6.43  min\n",
            "EPOCH  30 batch  590 : Loss: 2.3341 Perplexity: 10.3197\n",
            "Avg 10 batchs takes  6.54  min\n",
            "EPOCH  30 batch  600 : Loss: 2.2467 Perplexity: 9.4566\n",
            " ********* Epoch  30  Average Levenshtein Distance is ****:  292.34375\n",
            "Avg 10 batchs takes  6.65  min\n",
            "EPOCH  30 batch  610 : Loss: 2.3456 Perplexity: 10.4396\n",
            "Avg 10 batchs takes  6.77  min\n",
            "EPOCH  30 batch  620 : Loss: 2.3653 Perplexity: 10.6468\n",
            "Avg 10 batchs takes  6.88  min\n",
            "EPOCH  30 batch  630 : Loss: 2.3594 Perplexity: 10.5842\n",
            "Avg 10 batchs takes  6.99  min\n",
            "EPOCH  30 batch  640 : Loss: 2.3766 Perplexity: 10.7685\n",
            "Avg 10 batchs takes  7.1  min\n",
            "EPOCH  30 batch  650 : Loss: 2.3574 Perplexity: 10.5636\n",
            "Avg 10 batchs takes  7.2  min\n",
            "EPOCH  30 batch  660 : Loss: 2.3188 Perplexity: 10.1632\n",
            "Avg 10 batchs takes  7.31  min\n",
            "EPOCH  30 batch  670 : Loss: 2.3329 Perplexity: 10.3082\n",
            "Avg 10 batchs takes  7.42  min\n",
            "EPOCH  30 batch  680 : Loss: 2.3396 Perplexity: 10.3775\n",
            "Avg 10 batchs takes  7.53  min\n",
            "EPOCH  30 batch  690 : Loss: 2.3766 Perplexity: 10.7683\n",
            "Avg 10 batchs takes  7.64  min\n",
            "EPOCH  30 batch  700 : Loss: 2.3602 Perplexity: 10.5936\n",
            "Avg 10 batchs takes  7.74  min\n",
            "EPOCH  30 batch  710 : Loss: 2.4202 Perplexity: 11.2481\n",
            "Avg 10 batchs takes  7.86  min\n",
            "EPOCH  30 batch  720 : Loss: 2.2925 Perplexity: 9.8995\n",
            "Avg 10 batchs takes  7.97  min\n",
            "EPOCH  30 batch  730 : Loss: 2.3511 Perplexity: 10.4966\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  30 batch  740 : Loss: 2.3235 Perplexity: 10.2112\n",
            "Avg 10 batchs takes  8.18  min\n",
            "EPOCH  30 batch  750 : Loss: 2.3486 Perplexity: 10.471\n",
            "Avg 10 batchs takes  8.29  min\n",
            "EPOCH  30 batch  760 : Loss: 2.2874 Perplexity: 9.8495\n",
            "Avg 10 batchs takes  8.4  min\n",
            "EPOCH  30 batch  770 : Loss: 2.3375 Perplexity: 10.355\n",
            "Training loss after one epoch is: 2.3712995052337646\n",
            "Time take for an epoch is: 8.42  min\n",
            " ********************* Epoch  30  ******************\n",
            "Test loss   30  is  3.8380865505763464\n",
            "Perplexity is:  46.43653541483616\n",
            "Avg distance is:  216.55813492063493\n",
            "Learning rate for epoch  31  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  31 batch  0 : Loss: 2.3717 Perplexity: 10.7158\n",
            " ********* Epoch  31  Average Levenshtein Distance is ****:  180.8125\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  31 batch  10 : Loss: 2.3854 Perplexity: 10.863\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  31 batch  20 : Loss: 2.2592 Perplexity: 9.575\n",
            "Avg 10 batchs takes  0.35  min\n",
            "EPOCH  31 batch  30 : Loss: 2.3542 Perplexity: 10.5294\n",
            "Avg 10 batchs takes  0.46  min\n",
            "EPOCH  31 batch  40 : Loss: 2.3954 Perplexity: 10.9729\n",
            "Avg 10 batchs takes  0.57  min\n",
            "EPOCH  31 batch  50 : Loss: 2.3387 Perplexity: 10.3675\n",
            "Avg 10 batchs takes  0.68  min\n",
            "EPOCH  31 batch  60 : Loss: 2.3328 Perplexity: 10.3068\n",
            "Avg 10 batchs takes  0.79  min\n",
            "EPOCH  31 batch  70 : Loss: 2.2933 Perplexity: 9.908\n",
            "Avg 10 batchs takes  0.9  min\n",
            "EPOCH  31 batch  80 : Loss: 2.3924 Perplexity: 10.9401\n",
            "Avg 10 batchs takes  1.01  min\n",
            "EPOCH  31 batch  90 : Loss: 2.3325 Perplexity: 10.3033\n",
            "Avg 10 batchs takes  1.11  min\n",
            "EPOCH  31 batch  100 : Loss: 2.4089 Perplexity: 11.1222\n",
            "Avg 10 batchs takes  1.22  min\n",
            "EPOCH  31 batch  110 : Loss: 2.332 Perplexity: 10.2988\n",
            "Avg 10 batchs takes  1.33  min\n",
            "EPOCH  31 batch  120 : Loss: 2.4082 Perplexity: 11.1134\n",
            "Avg 10 batchs takes  1.44  min\n",
            "EPOCH  31 batch  130 : Loss: 2.2749 Perplexity: 9.7271\n",
            "Avg 10 batchs takes  1.55  min\n",
            "EPOCH  31 batch  140 : Loss: 2.332 Perplexity: 10.2989\n",
            "Avg 10 batchs takes  1.66  min\n",
            "EPOCH  31 batch  150 : Loss: 2.3686 Perplexity: 10.6823\n",
            "Avg 10 batchs takes  1.77  min\n",
            "EPOCH  31 batch  160 : Loss: 2.3872 Perplexity: 10.8835\n",
            "Avg 10 batchs takes  1.88  min\n",
            "EPOCH  31 batch  170 : Loss: 2.2965 Perplexity: 9.9388\n",
            "Avg 10 batchs takes  2.0  min\n",
            "EPOCH  31 batch  180 : Loss: 2.3178 Perplexity: 10.1529\n",
            "Avg 10 batchs takes  2.1  min\n",
            "EPOCH  31 batch  190 : Loss: 2.3512 Perplexity: 10.4978\n",
            "Avg 10 batchs takes  2.21  min\n",
            "EPOCH  31 batch  200 : Loss: 2.3747 Perplexity: 10.7476\n",
            "Avg 10 batchs takes  2.32  min\n",
            "EPOCH  31 batch  210 : Loss: 2.3037 Perplexity: 10.0115\n",
            "Avg 10 batchs takes  2.42  min\n",
            "EPOCH  31 batch  220 : Loss: 2.3029 Perplexity: 10.003\n",
            "Avg 10 batchs takes  2.53  min\n",
            "EPOCH  31 batch  230 : Loss: 2.3123 Perplexity: 10.0977\n",
            "Avg 10 batchs takes  2.64  min\n",
            "EPOCH  31 batch  240 : Loss: 2.3808 Perplexity: 10.8132\n",
            "Avg 10 batchs takes  2.75  min\n",
            "EPOCH  31 batch  250 : Loss: 2.3652 Perplexity: 10.6466\n",
            "Avg 10 batchs takes  2.86  min\n",
            "EPOCH  31 batch  260 : Loss: 2.3745 Perplexity: 10.7461\n",
            "Avg 10 batchs takes  2.97  min\n",
            "EPOCH  31 batch  270 : Loss: 2.3769 Perplexity: 10.7715\n",
            "Avg 10 batchs takes  3.08  min\n",
            "EPOCH  31 batch  280 : Loss: 2.337 Perplexity: 10.3507\n",
            "Avg 10 batchs takes  3.19  min\n",
            "EPOCH  31 batch  290 : Loss: 2.2954 Perplexity: 9.9281\n",
            "Avg 10 batchs takes  3.3  min\n",
            "EPOCH  31 batch  300 : Loss: 2.2624 Perplexity: 9.6064\n",
            " ********* Epoch  31  Average Levenshtein Distance is ****:  227.09375\n",
            "Avg 10 batchs takes  3.41  min\n",
            "EPOCH  31 batch  310 : Loss: 2.3044 Perplexity: 10.0177\n",
            "Avg 10 batchs takes  3.52  min\n",
            "EPOCH  31 batch  320 : Loss: 2.2989 Perplexity: 9.9637\n",
            "Avg 10 batchs takes  3.62  min\n",
            "EPOCH  31 batch  330 : Loss: 2.3116 Perplexity: 10.0908\n",
            "Avg 10 batchs takes  3.74  min\n",
            "EPOCH  31 batch  340 : Loss: 2.4076 Perplexity: 11.1073\n",
            "Avg 10 batchs takes  3.84  min\n",
            "EPOCH  31 batch  350 : Loss: 2.3475 Perplexity: 10.4593\n",
            "Avg 10 batchs takes  3.95  min\n",
            "EPOCH  31 batch  360 : Loss: 2.2829 Perplexity: 9.8049\n",
            "Avg 10 batchs takes  4.06  min\n",
            "EPOCH  31 batch  370 : Loss: 2.3955 Perplexity: 10.9734\n",
            "Avg 10 batchs takes  4.16  min\n",
            "EPOCH  31 batch  380 : Loss: 2.3306 Perplexity: 10.2844\n",
            "Avg 10 batchs takes  4.26  min\n",
            "EPOCH  31 batch  390 : Loss: 2.2871 Perplexity: 9.8463\n",
            "Avg 10 batchs takes  4.37  min\n",
            "EPOCH  31 batch  400 : Loss: 2.3843 Perplexity: 10.851\n",
            "Avg 10 batchs takes  4.47  min\n",
            "EPOCH  31 batch  410 : Loss: 2.3297 Perplexity: 10.2748\n",
            "Avg 10 batchs takes  4.58  min\n",
            "EPOCH  31 batch  420 : Loss: 2.3543 Perplexity: 10.5309\n",
            "Avg 10 batchs takes  4.68  min\n",
            "EPOCH  31 batch  430 : Loss: 2.3519 Perplexity: 10.5055\n",
            "Avg 10 batchs takes  4.79  min\n",
            "EPOCH  31 batch  440 : Loss: 2.2664 Perplexity: 9.6448\n",
            "Avg 10 batchs takes  4.9  min\n",
            "EPOCH  31 batch  450 : Loss: 2.3975 Perplexity: 10.9959\n",
            "Avg 10 batchs takes  5.01  min\n",
            "EPOCH  31 batch  460 : Loss: 2.2808 Perplexity: 9.7841\n",
            "Avg 10 batchs takes  5.12  min\n",
            "EPOCH  31 batch  470 : Loss: 2.4457 Perplexity: 11.5391\n",
            "Avg 10 batchs takes  5.22  min\n",
            "EPOCH  31 batch  480 : Loss: 2.3508 Perplexity: 10.4937\n",
            "Avg 10 batchs takes  5.33  min\n",
            "EPOCH  31 batch  490 : Loss: 2.292 Perplexity: 9.8943\n",
            "Avg 10 batchs takes  5.44  min\n",
            "EPOCH  31 batch  500 : Loss: 2.287 Perplexity: 9.8451\n",
            "Avg 10 batchs takes  5.55  min\n",
            "EPOCH  31 batch  510 : Loss: 2.3152 Perplexity: 10.1267\n",
            "Avg 10 batchs takes  5.65  min\n",
            "EPOCH  31 batch  520 : Loss: 2.3864 Perplexity: 10.8742\n",
            "Avg 10 batchs takes  5.76  min\n",
            "EPOCH  31 batch  530 : Loss: 2.3441 Perplexity: 10.4236\n",
            "Avg 10 batchs takes  5.87  min\n",
            "EPOCH  31 batch  540 : Loss: 2.2934 Perplexity: 9.9086\n",
            "Avg 10 batchs takes  5.98  min\n",
            "EPOCH  31 batch  550 : Loss: 2.2683 Perplexity: 9.6632\n",
            "Avg 10 batchs takes  6.08  min\n",
            "EPOCH  31 batch  560 : Loss: 2.4202 Perplexity: 11.2478\n",
            "Avg 10 batchs takes  6.2  min\n",
            "EPOCH  31 batch  570 : Loss: 2.3329 Perplexity: 10.3078\n",
            "Avg 10 batchs takes  6.31  min\n",
            "EPOCH  31 batch  580 : Loss: 2.3651 Perplexity: 10.645\n",
            "Avg 10 batchs takes  6.41  min\n",
            "EPOCH  31 batch  590 : Loss: 2.3407 Perplexity: 10.388\n",
            "Avg 10 batchs takes  6.52  min\n",
            "EPOCH  31 batch  600 : Loss: 2.3013 Perplexity: 9.9871\n",
            " ********* Epoch  31  Average Levenshtein Distance is ****:  149.0625\n",
            "Avg 10 batchs takes  6.63  min\n",
            "EPOCH  31 batch  610 : Loss: 2.4065 Perplexity: 11.0954\n",
            "Avg 10 batchs takes  6.75  min\n",
            "EPOCH  31 batch  620 : Loss: 2.3652 Perplexity: 10.6458\n",
            "Avg 10 batchs takes  6.85  min\n",
            "EPOCH  31 batch  630 : Loss: 2.2903 Perplexity: 9.8783\n",
            "Avg 10 batchs takes  6.96  min\n",
            "EPOCH  31 batch  640 : Loss: 2.2767 Perplexity: 9.7442\n",
            "Avg 10 batchs takes  7.07  min\n",
            "EPOCH  31 batch  650 : Loss: 2.3229 Perplexity: 10.2049\n",
            "Avg 10 batchs takes  7.18  min\n",
            "EPOCH  31 batch  660 : Loss: 2.3453 Perplexity: 10.4366\n",
            "Avg 10 batchs takes  7.28  min\n",
            "EPOCH  31 batch  670 : Loss: 2.2934 Perplexity: 9.9083\n",
            "Avg 10 batchs takes  7.39  min\n",
            "EPOCH  31 batch  680 : Loss: 2.258 Perplexity: 9.5641\n",
            "Avg 10 batchs takes  7.5  min\n",
            "EPOCH  31 batch  690 : Loss: 2.3292 Perplexity: 10.27\n",
            "Avg 10 batchs takes  7.61  min\n",
            "EPOCH  31 batch  700 : Loss: 2.2905 Perplexity: 9.8802\n",
            "Avg 10 batchs takes  7.72  min\n",
            "EPOCH  31 batch  710 : Loss: 2.3733 Perplexity: 10.7332\n",
            "Avg 10 batchs takes  7.83  min\n",
            "EPOCH  31 batch  720 : Loss: 2.3553 Perplexity: 10.5416\n",
            "Avg 10 batchs takes  7.94  min\n",
            "EPOCH  31 batch  730 : Loss: 2.3633 Perplexity: 10.6263\n",
            "Avg 10 batchs takes  8.05  min\n",
            "EPOCH  31 batch  740 : Loss: 2.2488 Perplexity: 9.476\n",
            "Avg 10 batchs takes  8.17  min\n",
            "EPOCH  31 batch  750 : Loss: 2.2525 Perplexity: 9.5111\n",
            "Avg 10 batchs takes  8.28  min\n",
            "EPOCH  31 batch  760 : Loss: 2.3003 Perplexity: 9.9768\n",
            "Avg 10 batchs takes  8.4  min\n",
            "EPOCH  31 batch  770 : Loss: 2.262 Perplexity: 9.602\n",
            "Training loss after one epoch is: 2.3655245304107666\n",
            "Time take for an epoch is: 8.42  min\n",
            " ********************* Epoch  31  ******************\n",
            "Test loss   31  is  3.6770417349679128\n",
            "Perplexity is:  39.52928283977837\n",
            "Avg distance is:  199.21041666666665\n",
            "Learning rate for epoch  32  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  32 batch  0 : Loss: 2.3416 Perplexity: 10.3975\n",
            " ********* Epoch  32  Average Levenshtein Distance is ****:  384.28125\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  32 batch  10 : Loss: 2.3021 Perplexity: 9.9956\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  32 batch  20 : Loss: 2.4915 Perplexity: 12.08\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  32 batch  30 : Loss: 2.331 Perplexity: 10.2883\n",
            "Avg 10 batchs takes  0.45  min\n",
            "EPOCH  32 batch  40 : Loss: 2.3328 Perplexity: 10.3069\n",
            "Avg 10 batchs takes  0.56  min\n",
            "EPOCH  32 batch  50 : Loss: 2.3898 Perplexity: 10.9113\n",
            "Avg 10 batchs takes  0.67  min\n",
            "EPOCH  32 batch  60 : Loss: 2.2167 Perplexity: 9.1766\n",
            "Avg 10 batchs takes  0.78  min\n",
            "EPOCH  32 batch  70 : Loss: 2.3262 Perplexity: 10.2395\n",
            "Avg 10 batchs takes  0.88  min\n",
            "EPOCH  32 batch  80 : Loss: 2.3599 Perplexity: 10.5898\n",
            "Avg 10 batchs takes  1.0  min\n",
            "EPOCH  32 batch  90 : Loss: 2.2525 Perplexity: 9.5118\n",
            "Avg 10 batchs takes  1.1  min\n",
            "EPOCH  32 batch  100 : Loss: 2.3457 Perplexity: 10.4404\n",
            "Avg 10 batchs takes  1.21  min\n",
            "EPOCH  32 batch  110 : Loss: 2.317 Perplexity: 10.1455\n",
            "Avg 10 batchs takes  1.32  min\n",
            "EPOCH  32 batch  120 : Loss: 2.3434 Perplexity: 10.4166\n",
            "Avg 10 batchs takes  1.43  min\n",
            "EPOCH  32 batch  130 : Loss: 2.3701 Perplexity: 10.6989\n",
            "Avg 10 batchs takes  1.54  min\n",
            "EPOCH  32 batch  140 : Loss: 2.3092 Perplexity: 10.0659\n",
            "Avg 10 batchs takes  1.66  min\n",
            "EPOCH  32 batch  150 : Loss: 2.3009 Perplexity: 9.9831\n",
            "Avg 10 batchs takes  1.77  min\n",
            "EPOCH  32 batch  160 : Loss: 2.3213 Perplexity: 10.1884\n",
            "Avg 10 batchs takes  1.88  min\n",
            "EPOCH  32 batch  170 : Loss: 2.521 Perplexity: 12.4416\n",
            "Avg 10 batchs takes  1.99  min\n",
            "EPOCH  32 batch  180 : Loss: 2.3809 Perplexity: 10.8147\n",
            "Avg 10 batchs takes  2.1  min\n",
            "EPOCH  32 batch  190 : Loss: 2.3936 Perplexity: 10.953\n",
            "Avg 10 batchs takes  2.21  min\n",
            "EPOCH  32 batch  200 : Loss: 2.3512 Perplexity: 10.4982\n",
            "Avg 10 batchs takes  2.32  min\n",
            "EPOCH  32 batch  210 : Loss: 2.2784 Perplexity: 9.7606\n",
            "Avg 10 batchs takes  2.44  min\n",
            "EPOCH  32 batch  220 : Loss: 2.3219 Perplexity: 10.1948\n",
            "Avg 10 batchs takes  2.55  min\n",
            "EPOCH  32 batch  230 : Loss: 2.347 Perplexity: 10.4541\n",
            "Avg 10 batchs takes  2.66  min\n",
            "EPOCH  32 batch  240 : Loss: 2.2782 Perplexity: 9.7588\n",
            "Avg 10 batchs takes  2.77  min\n",
            "EPOCH  32 batch  250 : Loss: 2.2836 Perplexity: 9.812\n",
            "Avg 10 batchs takes  2.88  min\n",
            "EPOCH  32 batch  260 : Loss: 2.3423 Perplexity: 10.4051\n",
            "Avg 10 batchs takes  2.99  min\n",
            "EPOCH  32 batch  270 : Loss: 2.4049 Perplexity: 11.0772\n",
            "Avg 10 batchs takes  3.09  min\n",
            "EPOCH  32 batch  280 : Loss: 2.3625 Perplexity: 10.6172\n",
            "Avg 10 batchs takes  3.21  min\n",
            "EPOCH  32 batch  290 : Loss: 2.2972 Perplexity: 9.9466\n",
            "Avg 10 batchs takes  3.32  min\n",
            "EPOCH  32 batch  300 : Loss: 2.3735 Perplexity: 10.7349\n",
            " ********* Epoch  32  Average Levenshtein Distance is ****:  257.9375\n",
            "Avg 10 batchs takes  3.44  min\n",
            "EPOCH  32 batch  310 : Loss: 2.3091 Perplexity: 10.065\n",
            "Avg 10 batchs takes  3.55  min\n",
            "EPOCH  32 batch  320 : Loss: 2.3078 Perplexity: 10.0521\n",
            "Avg 10 batchs takes  3.65  min\n",
            "EPOCH  32 batch  330 : Loss: 2.2938 Perplexity: 9.913\n",
            "Avg 10 batchs takes  3.76  min\n",
            "EPOCH  32 batch  340 : Loss: 2.377 Perplexity: 10.773\n",
            "Avg 10 batchs takes  3.87  min\n",
            "EPOCH  32 batch  350 : Loss: 2.3254 Perplexity: 10.2304\n",
            "Avg 10 batchs takes  3.98  min\n",
            "EPOCH  32 batch  360 : Loss: 2.3893 Perplexity: 10.9062\n",
            "Avg 10 batchs takes  4.09  min\n",
            "EPOCH  32 batch  370 : Loss: 2.3007 Perplexity: 9.9814\n",
            "Avg 10 batchs takes  4.2  min\n",
            "EPOCH  32 batch  380 : Loss: 2.2041 Perplexity: 9.0621\n",
            "Avg 10 batchs takes  4.31  min\n",
            "EPOCH  32 batch  390 : Loss: 2.3159 Perplexity: 10.1339\n",
            "Avg 10 batchs takes  4.42  min\n",
            "EPOCH  32 batch  400 : Loss: 2.3326 Perplexity: 10.3048\n",
            "Avg 10 batchs takes  4.53  min\n",
            "EPOCH  32 batch  410 : Loss: 2.3285 Perplexity: 10.2626\n",
            "Avg 10 batchs takes  4.64  min\n",
            "EPOCH  32 batch  420 : Loss: 2.3049 Perplexity: 10.0228\n",
            "Avg 10 batchs takes  4.76  min\n",
            "EPOCH  32 batch  430 : Loss: 2.3895 Perplexity: 10.9084\n",
            "Avg 10 batchs takes  4.87  min\n",
            "EPOCH  32 batch  440 : Loss: 2.2762 Perplexity: 9.7396\n",
            "Avg 10 batchs takes  4.97  min\n",
            "EPOCH  32 batch  450 : Loss: 2.3521 Perplexity: 10.5076\n",
            "Avg 10 batchs takes  5.08  min\n",
            "EPOCH  32 batch  460 : Loss: 2.2945 Perplexity: 9.9196\n",
            "Avg 10 batchs takes  5.19  min\n",
            "EPOCH  32 batch  470 : Loss: 2.2583 Perplexity: 9.5664\n",
            "Avg 10 batchs takes  5.3  min\n",
            "EPOCH  32 batch  480 : Loss: 2.3593 Perplexity: 10.5838\n",
            "Avg 10 batchs takes  5.4  min\n",
            "EPOCH  32 batch  490 : Loss: 2.3026 Perplexity: 10.0006\n",
            "Avg 10 batchs takes  5.51  min\n",
            "EPOCH  32 batch  500 : Loss: 2.3746 Perplexity: 10.7466\n",
            "Avg 10 batchs takes  5.62  min\n",
            "EPOCH  32 batch  510 : Loss: 2.3026 Perplexity: 9.9998\n",
            "Avg 10 batchs takes  5.73  min\n",
            "EPOCH  32 batch  520 : Loss: 2.4325 Perplexity: 11.3872\n",
            "Avg 10 batchs takes  5.85  min\n",
            "EPOCH  32 batch  530 : Loss: 2.3276 Perplexity: 10.2536\n",
            "Avg 10 batchs takes  5.96  min\n",
            "EPOCH  32 batch  540 : Loss: 2.321 Perplexity: 10.1858\n",
            "Avg 10 batchs takes  6.07  min\n",
            "EPOCH  32 batch  550 : Loss: 2.2994 Perplexity: 9.9686\n",
            "Avg 10 batchs takes  6.18  min\n",
            "EPOCH  32 batch  560 : Loss: 2.4007 Perplexity: 11.0304\n",
            "Avg 10 batchs takes  6.29  min\n",
            "EPOCH  32 batch  570 : Loss: 2.2531 Perplexity: 9.5169\n",
            "Avg 10 batchs takes  6.39  min\n",
            "EPOCH  32 batch  580 : Loss: 2.3131 Perplexity: 10.1061\n",
            "Avg 10 batchs takes  6.5  min\n",
            "EPOCH  32 batch  590 : Loss: 2.3537 Perplexity: 10.5243\n",
            "Avg 10 batchs takes  6.61  min\n",
            "EPOCH  32 batch  600 : Loss: 2.2938 Perplexity: 9.9129\n",
            " ********* Epoch  32  Average Levenshtein Distance is ****:  189.3125\n",
            "Avg 10 batchs takes  6.72  min\n",
            "EPOCH  32 batch  610 : Loss: 2.2429 Perplexity: 9.4203\n",
            "Avg 10 batchs takes  6.83  min\n",
            "EPOCH  32 batch  620 : Loss: 2.3876 Perplexity: 10.8877\n",
            "Avg 10 batchs takes  6.94  min\n",
            "EPOCH  32 batch  630 : Loss: 2.2347 Perplexity: 9.3433\n",
            "Avg 10 batchs takes  7.05  min\n",
            "EPOCH  32 batch  640 : Loss: 2.26 Perplexity: 9.5835\n",
            "Avg 10 batchs takes  7.16  min\n",
            "EPOCH  32 batch  650 : Loss: 2.2914 Perplexity: 9.8887\n",
            "Avg 10 batchs takes  7.27  min\n",
            "EPOCH  32 batch  660 : Loss: 2.3376 Perplexity: 10.3567\n",
            "Avg 10 batchs takes  7.38  min\n",
            "EPOCH  32 batch  670 : Loss: 2.3394 Perplexity: 10.3746\n",
            "Avg 10 batchs takes  7.49  min\n",
            "EPOCH  32 batch  680 : Loss: 2.2992 Perplexity: 9.9664\n",
            "Avg 10 batchs takes  7.59  min\n",
            "EPOCH  32 batch  690 : Loss: 2.2783 Perplexity: 9.7602\n",
            "Avg 10 batchs takes  7.7  min\n",
            "EPOCH  32 batch  700 : Loss: 2.2737 Perplexity: 9.7153\n",
            "Avg 10 batchs takes  7.82  min\n",
            "EPOCH  32 batch  710 : Loss: 2.3286 Perplexity: 10.2631\n",
            "Avg 10 batchs takes  7.92  min\n",
            "EPOCH  32 batch  720 : Loss: 2.3401 Perplexity: 10.3823\n",
            "Avg 10 batchs takes  8.03  min\n",
            "EPOCH  32 batch  730 : Loss: 2.299 Perplexity: 9.9641\n",
            "Avg 10 batchs takes  8.14  min\n",
            "EPOCH  32 batch  740 : Loss: 2.3833 Perplexity: 10.8401\n",
            "Avg 10 batchs takes  8.26  min\n",
            "EPOCH  32 batch  750 : Loss: 2.3642 Perplexity: 10.6359\n",
            "Avg 10 batchs takes  8.37  min\n",
            "EPOCH  32 batch  760 : Loss: 2.3728 Perplexity: 10.7277\n",
            "Avg 10 batchs takes  8.48  min\n",
            "EPOCH  32 batch  770 : Loss: 2.3192 Perplexity: 10.1678\n",
            "Training loss after one epoch is: 2.304434061050415\n",
            "Time take for an epoch is: 8.5  min\n",
            " ********************* Epoch  32  ******************\n",
            "Test loss   32  is  3.786850779397147\n",
            "Perplexity is:  44.11724633665845\n",
            "Avg distance is:  255.43561507936508\n",
            "Learning rate for epoch  33  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  33 batch  0 : Loss: 2.2756 Perplexity: 9.7338\n",
            " ********* Epoch  33  Average Levenshtein Distance is ****:  339.09375\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  33 batch  10 : Loss: 2.2995 Perplexity: 9.9689\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  33 batch  20 : Loss: 2.2933 Perplexity: 9.9078\n",
            "Avg 10 batchs takes  0.35  min\n",
            "EPOCH  33 batch  30 : Loss: 2.2994 Perplexity: 9.9678\n",
            "Avg 10 batchs takes  0.45  min\n",
            "EPOCH  33 batch  40 : Loss: 2.3582 Perplexity: 10.5714\n",
            "Avg 10 batchs takes  0.56  min\n",
            "EPOCH  33 batch  50 : Loss: 2.385 Perplexity: 10.8587\n",
            "Avg 10 batchs takes  0.67  min\n",
            "EPOCH  33 batch  60 : Loss: 2.2826 Perplexity: 9.802\n",
            "Avg 10 batchs takes  0.78  min\n",
            "EPOCH  33 batch  70 : Loss: 2.4353 Perplexity: 11.4194\n",
            "Avg 10 batchs takes  0.89  min\n",
            "EPOCH  33 batch  80 : Loss: 2.3262 Perplexity: 10.2388\n",
            "Avg 10 batchs takes  1.01  min\n",
            "EPOCH  33 batch  90 : Loss: 2.3138 Perplexity: 10.1124\n",
            "Avg 10 batchs takes  1.11  min\n",
            "EPOCH  33 batch  100 : Loss: 2.3556 Perplexity: 10.5442\n",
            "Avg 10 batchs takes  1.23  min\n",
            "EPOCH  33 batch  110 : Loss: 2.3531 Perplexity: 10.5178\n",
            "Avg 10 batchs takes  1.34  min\n",
            "EPOCH  33 batch  120 : Loss: 2.2593 Perplexity: 9.5767\n",
            "Avg 10 batchs takes  1.45  min\n",
            "EPOCH  33 batch  130 : Loss: 2.3571 Perplexity: 10.5601\n",
            "Avg 10 batchs takes  1.55  min\n",
            "EPOCH  33 batch  140 : Loss: 2.3434 Perplexity: 10.4168\n",
            "Avg 10 batchs takes  1.66  min\n",
            "EPOCH  33 batch  150 : Loss: 2.3646 Perplexity: 10.64\n",
            "Avg 10 batchs takes  1.77  min\n",
            "EPOCH  33 batch  160 : Loss: 2.2897 Perplexity: 9.8722\n",
            "Avg 10 batchs takes  1.88  min\n",
            "EPOCH  33 batch  170 : Loss: 2.3144 Perplexity: 10.1192\n",
            "Avg 10 batchs takes  1.99  min\n",
            "EPOCH  33 batch  180 : Loss: 2.2587 Perplexity: 9.5708\n",
            "Avg 10 batchs takes  2.1  min\n",
            "EPOCH  33 batch  190 : Loss: 2.3258 Perplexity: 10.235\n",
            "Avg 10 batchs takes  2.21  min\n",
            "EPOCH  33 batch  200 : Loss: 2.2796 Perplexity: 9.7725\n",
            "Avg 10 batchs takes  2.32  min\n",
            "EPOCH  33 batch  210 : Loss: 2.249 Perplexity: 9.4783\n",
            "Avg 10 batchs takes  2.42  min\n",
            "EPOCH  33 batch  220 : Loss: 2.2954 Perplexity: 9.9283\n",
            "Avg 10 batchs takes  2.53  min\n",
            "EPOCH  33 batch  230 : Loss: 2.2938 Perplexity: 9.9125\n",
            "Avg 10 batchs takes  2.63  min\n",
            "EPOCH  33 batch  240 : Loss: 2.3378 Perplexity: 10.3587\n",
            "Avg 10 batchs takes  2.74  min\n",
            "EPOCH  33 batch  250 : Loss: 2.3461 Perplexity: 10.4446\n",
            "Avg 10 batchs takes  2.85  min\n",
            "EPOCH  33 batch  260 : Loss: 2.248 Perplexity: 9.4688\n",
            "Avg 10 batchs takes  2.96  min\n",
            "EPOCH  33 batch  270 : Loss: 2.2925 Perplexity: 9.8996\n",
            "Avg 10 batchs takes  3.07  min\n",
            "EPOCH  33 batch  280 : Loss: 2.2302 Perplexity: 9.3018\n",
            "Avg 10 batchs takes  3.17  min\n",
            "EPOCH  33 batch  290 : Loss: 2.423 Perplexity: 11.2801\n",
            "Avg 10 batchs takes  3.28  min\n",
            "EPOCH  33 batch  300 : Loss: 2.3206 Perplexity: 10.1816\n",
            " ********* Epoch  33  Average Levenshtein Distance is ****:  295.1875\n",
            "Avg 10 batchs takes  3.4  min\n",
            "EPOCH  33 batch  310 : Loss: 2.3522 Perplexity: 10.5082\n",
            "Avg 10 batchs takes  3.51  min\n",
            "EPOCH  33 batch  320 : Loss: 2.4133 Perplexity: 11.1707\n",
            "Avg 10 batchs takes  3.62  min\n",
            "EPOCH  33 batch  330 : Loss: 2.2106 Perplexity: 9.1211\n",
            "Avg 10 batchs takes  3.73  min\n",
            "EPOCH  33 batch  340 : Loss: 2.2271 Perplexity: 9.2732\n",
            "Avg 10 batchs takes  3.84  min\n",
            "EPOCH  33 batch  350 : Loss: 2.3164 Perplexity: 10.1395\n",
            "Avg 10 batchs takes  3.95  min\n",
            "EPOCH  33 batch  360 : Loss: 2.2654 Perplexity: 9.6345\n",
            "Avg 10 batchs takes  4.06  min\n",
            "EPOCH  33 batch  370 : Loss: 2.2795 Perplexity: 9.7721\n",
            "Avg 10 batchs takes  4.17  min\n",
            "EPOCH  33 batch  380 : Loss: 2.2722 Perplexity: 9.7007\n",
            "Avg 10 batchs takes  4.28  min\n",
            "EPOCH  33 batch  390 : Loss: 2.3193 Perplexity: 10.1681\n",
            "Avg 10 batchs takes  4.39  min\n",
            "EPOCH  33 batch  400 : Loss: 2.3837 Perplexity: 10.8455\n",
            "Avg 10 batchs takes  4.5  min\n",
            "EPOCH  33 batch  410 : Loss: 2.3145 Perplexity: 10.1202\n",
            "Avg 10 batchs takes  4.6  min\n",
            "EPOCH  33 batch  420 : Loss: 2.3598 Perplexity: 10.5893\n",
            "Avg 10 batchs takes  4.71  min\n",
            "EPOCH  33 batch  430 : Loss: 2.3774 Perplexity: 10.7765\n",
            "Avg 10 batchs takes  4.82  min\n",
            "EPOCH  33 batch  440 : Loss: 2.2974 Perplexity: 9.9484\n",
            "Avg 10 batchs takes  4.92  min\n",
            "EPOCH  33 batch  450 : Loss: 2.3019 Perplexity: 9.993\n",
            "Avg 10 batchs takes  5.03  min\n",
            "EPOCH  33 batch  460 : Loss: 2.2603 Perplexity: 9.5857\n",
            "Avg 10 batchs takes  5.14  min\n",
            "EPOCH  33 batch  470 : Loss: 2.3203 Perplexity: 10.1791\n",
            "Avg 10 batchs takes  5.25  min\n",
            "EPOCH  33 batch  480 : Loss: 2.3312 Perplexity: 10.2899\n",
            "Avg 10 batchs takes  5.35  min\n",
            "EPOCH  33 batch  490 : Loss: 2.3147 Perplexity: 10.1216\n",
            "Avg 10 batchs takes  5.46  min\n",
            "EPOCH  33 batch  500 : Loss: 2.3122 Perplexity: 10.0968\n",
            "Avg 10 batchs takes  5.57  min\n",
            "EPOCH  33 batch  510 : Loss: 2.2584 Perplexity: 9.5677\n",
            "Avg 10 batchs takes  5.68  min\n",
            "EPOCH  33 batch  520 : Loss: 2.3339 Perplexity: 10.3178\n",
            "Avg 10 batchs takes  5.79  min\n",
            "EPOCH  33 batch  530 : Loss: 2.3107 Perplexity: 10.0819\n",
            "Avg 10 batchs takes  5.9  min\n",
            "EPOCH  33 batch  540 : Loss: 2.2955 Perplexity: 9.9293\n",
            "Avg 10 batchs takes  6.01  min\n",
            "EPOCH  33 batch  550 : Loss: 2.2895 Perplexity: 9.8701\n",
            "Avg 10 batchs takes  6.12  min\n",
            "EPOCH  33 batch  560 : Loss: 2.371 Perplexity: 10.7083\n",
            "Avg 10 batchs takes  6.22  min\n",
            "EPOCH  33 batch  570 : Loss: 2.284 Perplexity: 9.8159\n",
            "Avg 10 batchs takes  6.34  min\n",
            "EPOCH  33 batch  580 : Loss: 2.3075 Perplexity: 10.0495\n",
            "Avg 10 batchs takes  6.45  min\n",
            "EPOCH  33 batch  590 : Loss: 2.305 Perplexity: 10.0241\n",
            "Avg 10 batchs takes  6.56  min\n",
            "EPOCH  33 batch  600 : Loss: 2.3171 Perplexity: 10.1462\n",
            " ********* Epoch  33  Average Levenshtein Distance is ****:  269.53125\n",
            "Avg 10 batchs takes  6.67  min\n",
            "EPOCH  33 batch  610 : Loss: 2.3594 Perplexity: 10.585\n",
            "Avg 10 batchs takes  6.78  min\n",
            "EPOCH  33 batch  620 : Loss: 2.2505 Perplexity: 9.4929\n",
            "Avg 10 batchs takes  6.89  min\n",
            "EPOCH  33 batch  630 : Loss: 2.4172 Perplexity: 11.2143\n",
            "Avg 10 batchs takes  7.0  min\n",
            "EPOCH  33 batch  640 : Loss: 2.3338 Perplexity: 10.3167\n",
            "Avg 10 batchs takes  7.11  min\n",
            "EPOCH  33 batch  650 : Loss: 2.2462 Perplexity: 9.4514\n",
            "Avg 10 batchs takes  7.22  min\n",
            "EPOCH  33 batch  660 : Loss: 2.2884 Perplexity: 9.8587\n",
            "Avg 10 batchs takes  7.33  min\n",
            "EPOCH  33 batch  670 : Loss: 2.4044 Perplexity: 11.0722\n",
            "Avg 10 batchs takes  7.44  min\n",
            "EPOCH  33 batch  680 : Loss: 2.3045 Perplexity: 10.019\n",
            "Avg 10 batchs takes  7.55  min\n",
            "EPOCH  33 batch  690 : Loss: 2.3367 Perplexity: 10.3468\n",
            "Avg 10 batchs takes  7.66  min\n",
            "EPOCH  33 batch  700 : Loss: 2.2685 Perplexity: 9.6653\n",
            "Avg 10 batchs takes  7.78  min\n",
            "EPOCH  33 batch  710 : Loss: 2.3035 Perplexity: 10.0088\n",
            "Avg 10 batchs takes  7.89  min\n",
            "EPOCH  33 batch  720 : Loss: 2.3466 Perplexity: 10.4503\n",
            "Avg 10 batchs takes  7.99  min\n",
            "EPOCH  33 batch  730 : Loss: 2.3081 Perplexity: 10.0554\n",
            "Avg 10 batchs takes  8.1  min\n",
            "EPOCH  33 batch  740 : Loss: 2.3073 Perplexity: 10.0477\n",
            "Avg 10 batchs takes  8.21  min\n",
            "EPOCH  33 batch  750 : Loss: 2.3411 Perplexity: 10.393\n",
            "Avg 10 batchs takes  8.31  min\n",
            "EPOCH  33 batch  760 : Loss: 2.3432 Perplexity: 10.4149\n",
            "Avg 10 batchs takes  8.43  min\n",
            "EPOCH  33 batch  770 : Loss: 2.2696 Perplexity: 9.6754\n",
            "Training loss after one epoch is: 2.396517753601074\n",
            "Time take for an epoch is: 8.45  min\n",
            " ********************* Epoch  33  ******************\n",
            "Test loss   33  is  3.493871293749128\n",
            "Perplexity is:  32.91311773809714\n",
            "Avg distance is:  171.03809523809522\n",
            "Learning rate for epoch  34  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  34 batch  0 : Loss: 2.2698 Perplexity: 9.6775\n",
            " ********* Epoch  34  Average Levenshtein Distance is ****:  346.125\n",
            "Avg 10 batchs takes  0.13  min\n",
            "EPOCH  34 batch  10 : Loss: 2.3614 Perplexity: 10.6055\n",
            "Avg 10 batchs takes  0.24  min\n",
            "EPOCH  34 batch  20 : Loss: 2.246 Perplexity: 9.4502\n",
            "Avg 10 batchs takes  0.35  min\n",
            "EPOCH  34 batch  30 : Loss: 2.4238 Perplexity: 11.2883\n",
            "Avg 10 batchs takes  0.45  min\n",
            "EPOCH  34 batch  40 : Loss: 2.3005 Perplexity: 9.9787\n",
            "Avg 10 batchs takes  0.56  min\n",
            "EPOCH  34 batch  50 : Loss: 2.2709 Perplexity: 9.6882\n",
            "Avg 10 batchs takes  0.67  min\n",
            "EPOCH  34 batch  60 : Loss: 2.2739 Perplexity: 9.7173\n",
            "Avg 10 batchs takes  0.77  min\n",
            "EPOCH  34 batch  70 : Loss: 2.3041 Perplexity: 10.0151\n",
            "Avg 10 batchs takes  0.89  min\n",
            "EPOCH  34 batch  80 : Loss: 2.3155 Perplexity: 10.1301\n",
            "Avg 10 batchs takes  0.99  min\n",
            "EPOCH  34 batch  90 : Loss: 2.2621 Perplexity: 9.6036\n",
            "Avg 10 batchs takes  1.1  min\n",
            "EPOCH  34 batch  100 : Loss: 2.3391 Perplexity: 10.3714\n",
            "Avg 10 batchs takes  1.21  min\n",
            "EPOCH  34 batch  110 : Loss: 2.3564 Perplexity: 10.5531\n",
            "Avg 10 batchs takes  1.31  min\n",
            "EPOCH  34 batch  120 : Loss: 2.3307 Perplexity: 10.2846\n",
            "Avg 10 batchs takes  1.42  min\n",
            "EPOCH  34 batch  130 : Loss: 2.2745 Perplexity: 9.7227\n",
            "Avg 10 batchs takes  1.53  min\n",
            "EPOCH  34 batch  140 : Loss: 2.366 Perplexity: 10.6551\n",
            "Avg 10 batchs takes  1.64  min\n",
            "EPOCH  34 batch  150 : Loss: 2.2071 Perplexity: 9.0892\n",
            "Avg 10 batchs takes  1.75  min\n",
            "EPOCH  34 batch  160 : Loss: 2.3892 Perplexity: 10.9053\n",
            "Avg 10 batchs takes  1.86  min\n",
            "EPOCH  34 batch  170 : Loss: 2.3225 Perplexity: 10.2015\n",
            "Avg 10 batchs takes  1.97  min\n",
            "EPOCH  34 batch  180 : Loss: 2.2907 Perplexity: 9.8822\n",
            "Avg 10 batchs takes  2.08  min\n",
            "EPOCH  34 batch  190 : Loss: 2.3574 Perplexity: 10.5638\n",
            "Avg 10 batchs takes  2.19  min\n",
            "EPOCH  34 batch  200 : Loss: 2.3724 Perplexity: 10.7234\n",
            "Avg 10 batchs takes  2.3  min\n",
            "EPOCH  34 batch  210 : Loss: 2.3318 Perplexity: 10.2966\n",
            "Avg 10 batchs takes  2.41  min\n",
            "EPOCH  34 batch  220 : Loss: 2.3269 Perplexity: 10.2458\n",
            "Avg 10 batchs takes  2.52  min\n",
            "EPOCH  34 batch  230 : Loss: 2.3483 Perplexity: 10.4681\n",
            "Avg 10 batchs takes  2.63  min\n",
            "EPOCH  34 batch  240 : Loss: 2.3114 Perplexity: 10.0889\n",
            "Avg 10 batchs takes  2.74  min\n",
            "EPOCH  34 batch  250 : Loss: 2.3426 Perplexity: 10.4084\n",
            "Avg 10 batchs takes  2.85  min\n",
            "EPOCH  34 batch  260 : Loss: 2.2881 Perplexity: 9.856\n",
            "Avg 10 batchs takes  2.96  min\n",
            "EPOCH  34 batch  270 : Loss: 2.2583 Perplexity: 9.5671\n",
            "Avg 10 batchs takes  3.06  min\n",
            "EPOCH  34 batch  280 : Loss: 2.4178 Perplexity: 11.2216\n",
            "Avg 10 batchs takes  3.17  min\n",
            "EPOCH  34 batch  290 : Loss: 2.2581 Perplexity: 9.5649\n",
            "Avg 10 batchs takes  3.28  min\n",
            "EPOCH  34 batch  300 : Loss: 2.3192 Perplexity: 10.1674\n",
            " ********* Epoch  34  Average Levenshtein Distance is ****:  155.0\n",
            "Avg 10 batchs takes  3.39  min\n",
            "EPOCH  34 batch  310 : Loss: 2.3866 Perplexity: 10.877\n",
            "Avg 10 batchs takes  3.5  min\n",
            "EPOCH  34 batch  320 : Loss: 2.3786 Perplexity: 10.7895\n",
            "Avg 10 batchs takes  3.61  min\n",
            "EPOCH  34 batch  330 : Loss: 2.2691 Perplexity: 9.6705\n",
            "Avg 10 batchs takes  3.72  min\n",
            "EPOCH  34 batch  340 : Loss: 2.3015 Perplexity: 9.9893\n",
            "Avg 10 batchs takes  3.83  min\n",
            "EPOCH  34 batch  350 : Loss: 2.333 Perplexity: 10.3088\n",
            "Avg 10 batchs takes  3.94  min\n",
            "EPOCH  34 batch  360 : Loss: 2.4118 Perplexity: 11.1542\n",
            "Avg 10 batchs takes  4.04  min\n",
            "EPOCH  34 batch  370 : Loss: 2.2669 Perplexity: 9.6496\n",
            "Avg 10 batchs takes  4.15  min\n",
            "EPOCH  34 batch  380 : Loss: 2.3207 Perplexity: 10.1828\n",
            "Avg 10 batchs takes  4.26  min\n",
            "EPOCH  34 batch  390 : Loss: 2.4385 Perplexity: 11.4559\n",
            "Avg 10 batchs takes  4.38  min\n",
            "EPOCH  34 batch  400 : Loss: 2.2673 Perplexity: 9.6532\n",
            "Avg 10 batchs takes  4.49  min\n",
            "EPOCH  34 batch  410 : Loss: 2.2492 Perplexity: 9.48\n",
            "Avg 10 batchs takes  4.6  min\n",
            "EPOCH  34 batch  420 : Loss: 2.321 Perplexity: 10.1858\n",
            "Avg 10 batchs takes  4.71  min\n",
            "EPOCH  34 batch  430 : Loss: 2.4052 Perplexity: 11.0805\n",
            "Avg 10 batchs takes  4.83  min\n",
            "EPOCH  34 batch  440 : Loss: 2.3294 Perplexity: 10.2714\n",
            "Avg 10 batchs takes  4.93  min\n",
            "EPOCH  34 batch  450 : Loss: 2.3475 Perplexity: 10.4589\n",
            "Avg 10 batchs takes  5.05  min\n",
            "EPOCH  34 batch  460 : Loss: 2.3233 Perplexity: 10.2098\n",
            "Avg 10 batchs takes  5.15  min\n",
            "EPOCH  34 batch  470 : Loss: 2.2907 Perplexity: 9.8821\n",
            "Avg 10 batchs takes  5.26  min\n",
            "EPOCH  34 batch  480 : Loss: 2.316 Perplexity: 10.1347\n",
            "Avg 10 batchs takes  5.37  min\n",
            "EPOCH  34 batch  490 : Loss: 2.2098 Perplexity: 9.1139\n",
            "Avg 10 batchs takes  5.48  min\n",
            "EPOCH  34 batch  500 : Loss: 2.2684 Perplexity: 9.6637\n",
            "Avg 10 batchs takes  5.59  min\n",
            "EPOCH  34 batch  510 : Loss: 2.2316 Perplexity: 9.3144\n",
            "Avg 10 batchs takes  5.7  min\n",
            "EPOCH  34 batch  520 : Loss: 2.2821 Perplexity: 9.7969\n",
            "Avg 10 batchs takes  5.81  min\n",
            "EPOCH  34 batch  530 : Loss: 2.3121 Perplexity: 10.0954\n",
            "Avg 10 batchs takes  5.92  min\n",
            "EPOCH  34 batch  540 : Loss: 2.3797 Perplexity: 10.8017\n",
            "Avg 10 batchs takes  6.03  min\n",
            "EPOCH  34 batch  550 : Loss: 2.3336 Perplexity: 10.3152\n",
            "Avg 10 batchs takes  6.14  min\n",
            "EPOCH  34 batch  560 : Loss: 2.3415 Perplexity: 10.397\n",
            "Avg 10 batchs takes  6.25  min\n",
            "EPOCH  34 batch  570 : Loss: 2.293 Perplexity: 9.9045\n",
            "Avg 10 batchs takes  6.36  min\n",
            "EPOCH  34 batch  580 : Loss: 2.2927 Perplexity: 9.9015\n",
            "Avg 10 batchs takes  6.47  min\n",
            "EPOCH  34 batch  590 : Loss: 2.4063 Perplexity: 11.0934\n",
            "Avg 10 batchs takes  6.58  min\n",
            "EPOCH  34 batch  600 : Loss: 2.3123 Perplexity: 10.0978\n",
            " ********* Epoch  34  Average Levenshtein Distance is ****:  369.1875\n",
            "Avg 10 batchs takes  6.7  min\n",
            "EPOCH  34 batch  610 : Loss: 2.3491 Perplexity: 10.4764\n",
            "Avg 10 batchs takes  6.81  min\n",
            "EPOCH  34 batch  620 : Loss: 2.2996 Perplexity: 9.97\n",
            "Avg 10 batchs takes  6.92  min\n",
            "EPOCH  34 batch  630 : Loss: 2.3595 Perplexity: 10.586\n",
            "Avg 10 batchs takes  7.03  min\n",
            "EPOCH  34 batch  640 : Loss: 2.3699 Perplexity: 10.696\n",
            "Avg 10 batchs takes  7.14  min\n",
            "EPOCH  34 batch  650 : Loss: 2.3544 Perplexity: 10.5318\n",
            "Avg 10 batchs takes  7.25  min\n",
            "EPOCH  34 batch  660 : Loss: 2.3102 Perplexity: 10.0761\n",
            "Avg 10 batchs takes  7.36  min\n",
            "EPOCH  34 batch  670 : Loss: 2.3468 Perplexity: 10.4516\n",
            "Avg 10 batchs takes  7.47  min\n",
            "EPOCH  34 batch  680 : Loss: 2.3134 Perplexity: 10.1083\n",
            "Avg 10 batchs takes  7.58  min\n",
            "EPOCH  34 batch  690 : Loss: 2.3259 Perplexity: 10.2362\n",
            "Avg 10 batchs takes  7.69  min\n",
            "EPOCH  34 batch  700 : Loss: 2.2522 Perplexity: 9.509\n",
            "Avg 10 batchs takes  7.8  min\n",
            "EPOCH  34 batch  710 : Loss: 2.3887 Perplexity: 10.8996\n",
            "Avg 10 batchs takes  7.91  min\n",
            "EPOCH  34 batch  720 : Loss: 2.2393 Perplexity: 9.3864\n",
            "Avg 10 batchs takes  8.03  min\n",
            "EPOCH  34 batch  730 : Loss: 2.2271 Perplexity: 9.2731\n",
            "Avg 10 batchs takes  8.14  min\n",
            "EPOCH  34 batch  740 : Loss: 2.269 Perplexity: 9.6697\n",
            "Avg 10 batchs takes  8.25  min\n",
            "EPOCH  34 batch  750 : Loss: 2.3161 Perplexity: 10.1359\n",
            "Avg 10 batchs takes  8.36  min\n",
            "EPOCH  34 batch  760 : Loss: 2.4042 Perplexity: 11.0694\n",
            "Avg 10 batchs takes  8.46  min\n",
            "EPOCH  34 batch  770 : Loss: 2.2947 Perplexity: 9.9213\n",
            "Training loss after one epoch is: 2.1748108863830566\n",
            "Time take for an epoch is: 8.49  min\n",
            " ********************* Epoch  34  ******************\n",
            "Test loss   34  is  3.585805654525757\n",
            "Perplexity is:  36.08241597355961\n",
            "Avg distance is:  194.0843253968254\n",
            "Learning rate for epoch  35  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  35 batch  0 : Loss: 2.4033 Perplexity: 11.0591\n",
            " ********* Epoch  35  Average Levenshtein Distance is ****:  273.0625\n",
            "Avg 10 batchs takes  0.13  min\n",
            "EPOCH  35 batch  10 : Loss: 2.3208 Perplexity: 10.1842\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  35 batch  20 : Loss: 2.3232 Perplexity: 10.2084\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  35 batch  30 : Loss: 2.3258 Perplexity: 10.2353\n",
            "Avg 10 batchs takes  0.45  min\n",
            "EPOCH  35 batch  40 : Loss: 2.3007 Perplexity: 9.9814\n",
            "Avg 10 batchs takes  0.56  min\n",
            "EPOCH  35 batch  50 : Loss: 2.3387 Perplexity: 10.3683\n",
            "Avg 10 batchs takes  0.67  min\n",
            "EPOCH  35 batch  60 : Loss: 2.4099 Perplexity: 11.1325\n",
            "Avg 10 batchs takes  0.78  min\n",
            "EPOCH  35 batch  70 : Loss: 2.244 Perplexity: 9.4312\n",
            "Avg 10 batchs takes  0.89  min\n",
            "EPOCH  35 batch  80 : Loss: 2.3477 Perplexity: 10.4611\n",
            "Avg 10 batchs takes  1.01  min\n",
            "EPOCH  35 batch  90 : Loss: 2.2534 Perplexity: 9.5204\n",
            "Avg 10 batchs takes  1.12  min\n",
            "EPOCH  35 batch  100 : Loss: 2.2884 Perplexity: 9.8588\n",
            "Avg 10 batchs takes  1.22  min\n",
            "EPOCH  35 batch  110 : Loss: 2.3676 Perplexity: 10.6717\n",
            "Avg 10 batchs takes  1.33  min\n",
            "EPOCH  35 batch  120 : Loss: 2.2666 Perplexity: 9.6461\n",
            "Avg 10 batchs takes  1.44  min\n",
            "EPOCH  35 batch  130 : Loss: 2.2532 Perplexity: 9.5182\n",
            "Avg 10 batchs takes  1.55  min\n",
            "EPOCH  35 batch  140 : Loss: 2.3352 Perplexity: 10.3311\n",
            "Avg 10 batchs takes  1.65  min\n",
            "EPOCH  35 batch  150 : Loss: 2.4054 Perplexity: 11.083\n",
            "Avg 10 batchs takes  1.76  min\n",
            "EPOCH  35 batch  160 : Loss: 2.2857 Perplexity: 9.833\n",
            "Avg 10 batchs takes  1.87  min\n",
            "EPOCH  35 batch  170 : Loss: 2.3404 Perplexity: 10.3854\n",
            "Avg 10 batchs takes  1.98  min\n",
            "EPOCH  35 batch  180 : Loss: 2.2456 Perplexity: 9.4464\n",
            "Avg 10 batchs takes  2.09  min\n",
            "EPOCH  35 batch  190 : Loss: 2.2289 Perplexity: 9.2895\n",
            "Avg 10 batchs takes  2.2  min\n",
            "EPOCH  35 batch  200 : Loss: 2.3643 Perplexity: 10.6365\n",
            "Avg 10 batchs takes  2.31  min\n",
            "EPOCH  35 batch  210 : Loss: 2.3032 Perplexity: 10.0061\n",
            "Avg 10 batchs takes  2.42  min\n",
            "EPOCH  35 batch  220 : Loss: 2.3349 Perplexity: 10.328\n",
            "Avg 10 batchs takes  2.53  min\n",
            "EPOCH  35 batch  230 : Loss: 2.2375 Perplexity: 9.3701\n",
            "Avg 10 batchs takes  2.64  min\n",
            "EPOCH  35 batch  240 : Loss: 2.3791 Perplexity: 10.7955\n",
            "Avg 10 batchs takes  2.74  min\n",
            "EPOCH  35 batch  250 : Loss: 2.3437 Perplexity: 10.4193\n",
            "Avg 10 batchs takes  2.85  min\n",
            "EPOCH  35 batch  260 : Loss: 2.2941 Perplexity: 9.915\n",
            "Avg 10 batchs takes  2.95  min\n",
            "EPOCH  35 batch  270 : Loss: 2.2288 Perplexity: 9.2885\n",
            "Avg 10 batchs takes  3.06  min\n",
            "EPOCH  35 batch  280 : Loss: 2.3826 Perplexity: 10.8327\n",
            "Avg 10 batchs takes  3.16  min\n",
            "EPOCH  35 batch  290 : Loss: 2.3151 Perplexity: 10.1255\n",
            "Avg 10 batchs takes  3.27  min\n",
            "EPOCH  35 batch  300 : Loss: 2.2953 Perplexity: 9.9269\n",
            " ********* Epoch  35  Average Levenshtein Distance is ****:  233.5\n",
            "Avg 10 batchs takes  3.39  min\n",
            "EPOCH  35 batch  310 : Loss: 2.3232 Perplexity: 10.2085\n",
            "Avg 10 batchs takes  3.5  min\n",
            "EPOCH  35 batch  320 : Loss: 2.302 Perplexity: 9.9937\n",
            "Avg 10 batchs takes  3.61  min\n",
            "EPOCH  35 batch  330 : Loss: 2.3144 Perplexity: 10.1186\n",
            "Avg 10 batchs takes  3.72  min\n",
            "EPOCH  35 batch  340 : Loss: 2.2594 Perplexity: 9.5776\n",
            "Avg 10 batchs takes  3.82  min\n",
            "EPOCH  35 batch  350 : Loss: 2.3921 Perplexity: 10.9359\n",
            "Avg 10 batchs takes  3.93  min\n",
            "EPOCH  35 batch  360 : Loss: 2.3306 Perplexity: 10.284\n",
            "Avg 10 batchs takes  4.05  min\n",
            "EPOCH  35 batch  370 : Loss: 2.3025 Perplexity: 9.9989\n",
            "Avg 10 batchs takes  4.16  min\n",
            "EPOCH  35 batch  380 : Loss: 2.3146 Perplexity: 10.1213\n",
            "Avg 10 batchs takes  4.27  min\n",
            "EPOCH  35 batch  390 : Loss: 2.27 Perplexity: 9.6797\n",
            "Avg 10 batchs takes  4.37  min\n",
            "EPOCH  35 batch  400 : Loss: 2.3056 Perplexity: 10.0299\n",
            "Avg 10 batchs takes  4.48  min\n",
            "EPOCH  35 batch  410 : Loss: 2.3206 Perplexity: 10.1821\n",
            "Avg 10 batchs takes  4.59  min\n",
            "EPOCH  35 batch  420 : Loss: 2.3038 Perplexity: 10.0125\n",
            "Avg 10 batchs takes  4.7  min\n",
            "EPOCH  35 batch  430 : Loss: 2.3612 Perplexity: 10.6036\n",
            "Avg 10 batchs takes  4.82  min\n",
            "EPOCH  35 batch  440 : Loss: 2.3518 Perplexity: 10.5045\n",
            "Avg 10 batchs takes  4.93  min\n",
            "EPOCH  35 batch  450 : Loss: 2.2799 Perplexity: 9.7756\n",
            "Avg 10 batchs takes  5.04  min\n",
            "EPOCH  35 batch  460 : Loss: 2.3233 Perplexity: 10.2094\n",
            "Avg 10 batchs takes  5.15  min\n",
            "EPOCH  35 batch  470 : Loss: 2.2905 Perplexity: 9.8795\n",
            "Avg 10 batchs takes  5.25  min\n",
            "EPOCH  35 batch  480 : Loss: 2.2735 Perplexity: 9.713\n",
            "Avg 10 batchs takes  5.36  min\n",
            "EPOCH  35 batch  490 : Loss: 2.2508 Perplexity: 9.4952\n",
            "Avg 10 batchs takes  5.47  min\n",
            "EPOCH  35 batch  500 : Loss: 2.276 Perplexity: 9.7374\n",
            "Avg 10 batchs takes  5.58  min\n",
            "EPOCH  35 batch  510 : Loss: 2.3377 Perplexity: 10.357\n",
            "Avg 10 batchs takes  5.69  min\n",
            "EPOCH  35 batch  520 : Loss: 2.2201 Perplexity: 9.2086\n",
            "Avg 10 batchs takes  5.8  min\n",
            "EPOCH  35 batch  530 : Loss: 2.3377 Perplexity: 10.3574\n",
            "Avg 10 batchs takes  5.91  min\n",
            "EPOCH  35 batch  540 : Loss: 2.1861 Perplexity: 8.9002\n",
            "Avg 10 batchs takes  6.02  min\n",
            "EPOCH  35 batch  550 : Loss: 2.33 Perplexity: 10.2783\n",
            "Avg 10 batchs takes  6.13  min\n",
            "EPOCH  35 batch  560 : Loss: 2.2701 Perplexity: 9.6805\n",
            "Avg 10 batchs takes  6.24  min\n",
            "EPOCH  35 batch  570 : Loss: 2.3527 Perplexity: 10.5141\n",
            "Avg 10 batchs takes  6.35  min\n",
            "EPOCH  35 batch  580 : Loss: 2.3348 Perplexity: 10.3273\n",
            "Avg 10 batchs takes  6.46  min\n",
            "EPOCH  35 batch  590 : Loss: 2.2437 Perplexity: 9.4278\n",
            "Avg 10 batchs takes  6.57  min\n",
            "EPOCH  35 batch  600 : Loss: 2.3331 Perplexity: 10.3098\n",
            " ********* Epoch  35  Average Levenshtein Distance is ****:  279.875\n",
            "Avg 10 batchs takes  6.69  min\n",
            "EPOCH  35 batch  610 : Loss: 2.33 Perplexity: 10.2777\n",
            "Avg 10 batchs takes  6.79  min\n",
            "EPOCH  35 batch  620 : Loss: 2.3268 Perplexity: 10.2448\n",
            "Avg 10 batchs takes  6.9  min\n",
            "EPOCH  35 batch  630 : Loss: 2.3821 Perplexity: 10.8275\n",
            "Avg 10 batchs takes  7.01  min\n",
            "EPOCH  35 batch  640 : Loss: 2.2256 Perplexity: 9.2587\n",
            "Avg 10 batchs takes  7.12  min\n",
            "EPOCH  35 batch  650 : Loss: 2.3506 Perplexity: 10.4919\n",
            "Avg 10 batchs takes  7.23  min\n",
            "EPOCH  35 batch  660 : Loss: 2.353 Perplexity: 10.5169\n",
            "Avg 10 batchs takes  7.34  min\n",
            "EPOCH  35 batch  670 : Loss: 2.2673 Perplexity: 9.6534\n",
            "Avg 10 batchs takes  7.45  min\n",
            "EPOCH  35 batch  680 : Loss: 2.3056 Perplexity: 10.0298\n",
            "Avg 10 batchs takes  7.56  min\n",
            "EPOCH  35 batch  690 : Loss: 2.2821 Perplexity: 9.797\n",
            "Avg 10 batchs takes  7.67  min\n",
            "EPOCH  35 batch  700 : Loss: 2.3397 Perplexity: 10.3783\n",
            "Avg 10 batchs takes  7.78  min\n",
            "EPOCH  35 batch  710 : Loss: 2.2877 Perplexity: 9.8524\n",
            "Avg 10 batchs takes  7.88  min\n",
            "EPOCH  35 batch  720 : Loss: 2.4078 Perplexity: 11.109\n",
            "Avg 10 batchs takes  7.99  min\n",
            "EPOCH  35 batch  730 : Loss: 2.3581 Perplexity: 10.5712\n",
            "Avg 10 batchs takes  8.1  min\n",
            "EPOCH  35 batch  740 : Loss: 2.1886 Perplexity: 8.9224\n",
            "Avg 10 batchs takes  8.2  min\n",
            "EPOCH  35 batch  750 : Loss: 2.2153 Perplexity: 9.1643\n",
            "Avg 10 batchs takes  8.31  min\n",
            "EPOCH  35 batch  760 : Loss: 2.3231 Perplexity: 10.2072\n",
            "Avg 10 batchs takes  8.42  min\n",
            "EPOCH  35 batch  770 : Loss: 2.3515 Perplexity: 10.5017\n",
            "Training loss after one epoch is: 2.2016987800598145\n",
            "Time take for an epoch is: 8.44  min\n",
            " ********************* Epoch  35  ******************\n",
            "Test loss   35  is  4.040522575378418\n",
            "Perplexity is:  56.85604661362812\n",
            "Avg distance is:  170.50684523809522\n",
            "Learning rate for epoch  36  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  36 batch  0 : Loss: 2.2388 Perplexity: 9.3824\n",
            " ********* Epoch  36  Average Levenshtein Distance is ****:  255.53125\n",
            "Avg 10 batchs takes  0.13  min\n",
            "EPOCH  36 batch  10 : Loss: 2.2958 Perplexity: 9.9324\n",
            "Avg 10 batchs takes  0.24  min\n",
            "EPOCH  36 batch  20 : Loss: 2.2409 Perplexity: 9.4018\n",
            "Avg 10 batchs takes  0.35  min\n",
            "EPOCH  36 batch  30 : Loss: 2.357 Perplexity: 10.5594\n",
            "Avg 10 batchs takes  0.46  min\n",
            "EPOCH  36 batch  40 : Loss: 2.3661 Perplexity: 10.6554\n",
            "Avg 10 batchs takes  0.57  min\n",
            "EPOCH  36 batch  50 : Loss: 2.2703 Perplexity: 9.682\n",
            "Avg 10 batchs takes  0.67  min\n",
            "EPOCH  36 batch  60 : Loss: 2.281 Perplexity: 9.7866\n",
            "Avg 10 batchs takes  0.78  min\n",
            "EPOCH  36 batch  70 : Loss: 2.2819 Perplexity: 9.7948\n",
            "Avg 10 batchs takes  0.89  min\n",
            "EPOCH  36 batch  80 : Loss: 2.2176 Perplexity: 9.1853\n",
            "Avg 10 batchs takes  1.0  min\n",
            "EPOCH  36 batch  90 : Loss: 2.2797 Perplexity: 9.7737\n",
            "Avg 10 batchs takes  1.1  min\n",
            "EPOCH  36 batch  100 : Loss: 2.2559 Perplexity: 9.5437\n",
            "Avg 10 batchs takes  1.21  min\n",
            "EPOCH  36 batch  110 : Loss: 2.2826 Perplexity: 9.802\n",
            "Avg 10 batchs takes  1.32  min\n",
            "EPOCH  36 batch  120 : Loss: 2.3366 Perplexity: 10.3464\n",
            "Avg 10 batchs takes  1.43  min\n",
            "EPOCH  36 batch  130 : Loss: 2.2915 Perplexity: 9.8894\n",
            "Avg 10 batchs takes  1.54  min\n",
            "EPOCH  36 batch  140 : Loss: 2.2924 Perplexity: 9.8984\n",
            "Avg 10 batchs takes  1.65  min\n",
            "EPOCH  36 batch  150 : Loss: 2.3962 Perplexity: 10.981\n",
            "Avg 10 batchs takes  1.76  min\n",
            "EPOCH  36 batch  160 : Loss: 2.2566 Perplexity: 9.5503\n",
            "Avg 10 batchs takes  1.87  min\n",
            "EPOCH  36 batch  170 : Loss: 2.29 Perplexity: 9.8751\n",
            "Avg 10 batchs takes  1.99  min\n",
            "EPOCH  36 batch  180 : Loss: 2.3174 Perplexity: 10.149\n",
            "Avg 10 batchs takes  2.1  min\n",
            "EPOCH  36 batch  190 : Loss: 2.3215 Perplexity: 10.1907\n",
            "Avg 10 batchs takes  2.21  min\n",
            "EPOCH  36 batch  200 : Loss: 2.3208 Perplexity: 10.1837\n",
            "Avg 10 batchs takes  2.32  min\n",
            "EPOCH  36 batch  210 : Loss: 2.3242 Perplexity: 10.2188\n",
            "Avg 10 batchs takes  2.43  min\n",
            "EPOCH  36 batch  220 : Loss: 2.274 Perplexity: 9.7181\n",
            "Avg 10 batchs takes  2.54  min\n",
            "EPOCH  36 batch  230 : Loss: 2.2534 Perplexity: 9.5205\n",
            "Avg 10 batchs takes  2.65  min\n",
            "EPOCH  36 batch  240 : Loss: 2.2611 Perplexity: 9.5936\n",
            "Avg 10 batchs takes  2.76  min\n",
            "EPOCH  36 batch  250 : Loss: 2.3452 Perplexity: 10.4354\n",
            "Avg 10 batchs takes  2.86  min\n",
            "EPOCH  36 batch  260 : Loss: 2.3314 Perplexity: 10.2919\n",
            "Avg 10 batchs takes  2.97  min\n",
            "EPOCH  36 batch  270 : Loss: 2.3549 Perplexity: 10.5368\n",
            "Avg 10 batchs takes  3.08  min\n",
            "EPOCH  36 batch  280 : Loss: 2.3244 Perplexity: 10.2202\n",
            "Avg 10 batchs takes  3.19  min\n",
            "EPOCH  36 batch  290 : Loss: 2.2535 Perplexity: 9.5214\n",
            "Avg 10 batchs takes  3.3  min\n",
            "EPOCH  36 batch  300 : Loss: 2.2553 Perplexity: 9.5383\n",
            " ********* Epoch  36  Average Levenshtein Distance is ****:  201.15625\n",
            "Avg 10 batchs takes  3.41  min\n",
            "EPOCH  36 batch  310 : Loss: 2.2461 Perplexity: 9.4509\n",
            "Avg 10 batchs takes  3.52  min\n",
            "EPOCH  36 batch  320 : Loss: 2.2717 Perplexity: 9.6959\n",
            "Avg 10 batchs takes  3.63  min\n",
            "EPOCH  36 batch  330 : Loss: 2.2953 Perplexity: 9.9269\n",
            "Avg 10 batchs takes  3.74  min\n",
            "EPOCH  36 batch  340 : Loss: 2.3584 Perplexity: 10.5745\n",
            "Avg 10 batchs takes  3.85  min\n",
            "EPOCH  36 batch  350 : Loss: 2.3383 Perplexity: 10.3635\n",
            "Avg 10 batchs takes  3.96  min\n",
            "EPOCH  36 batch  360 : Loss: 2.347 Perplexity: 10.4542\n",
            "Avg 10 batchs takes  4.07  min\n",
            "EPOCH  36 batch  370 : Loss: 2.2222 Perplexity: 9.2273\n",
            "Avg 10 batchs takes  4.17  min\n",
            "EPOCH  36 batch  380 : Loss: 2.3127 Perplexity: 10.1019\n",
            "Avg 10 batchs takes  4.28  min\n",
            "EPOCH  36 batch  390 : Loss: 2.4199 Perplexity: 11.2447\n",
            "Avg 10 batchs takes  4.39  min\n",
            "EPOCH  36 batch  400 : Loss: 2.2882 Perplexity: 9.8573\n",
            "Avg 10 batchs takes  4.5  min\n",
            "EPOCH  36 batch  410 : Loss: 2.2583 Perplexity: 9.5671\n",
            "Avg 10 batchs takes  4.61  min\n",
            "EPOCH  36 batch  420 : Loss: 2.3395 Perplexity: 10.3756\n",
            "Avg 10 batchs takes  4.72  min\n",
            "EPOCH  36 batch  430 : Loss: 2.2949 Perplexity: 9.9236\n",
            "Avg 10 batchs takes  4.83  min\n",
            "EPOCH  36 batch  440 : Loss: 2.2277 Perplexity: 9.2783\n",
            "Avg 10 batchs takes  4.94  min\n",
            "EPOCH  36 batch  450 : Loss: 2.2772 Perplexity: 9.7494\n",
            "Avg 10 batchs takes  5.05  min\n",
            "EPOCH  36 batch  460 : Loss: 2.2408 Perplexity: 9.401\n",
            "Avg 10 batchs takes  5.16  min\n",
            "EPOCH  36 batch  470 : Loss: 2.3247 Perplexity: 10.2232\n",
            "Avg 10 batchs takes  5.27  min\n",
            "EPOCH  36 batch  480 : Loss: 2.3907 Perplexity: 10.9214\n",
            "Avg 10 batchs takes  5.38  min\n",
            "EPOCH  36 batch  490 : Loss: 2.2809 Perplexity: 9.7857\n",
            "Avg 10 batchs takes  5.49  min\n",
            "EPOCH  36 batch  500 : Loss: 2.3506 Perplexity: 10.4914\n",
            "Avg 10 batchs takes  5.6  min\n",
            "EPOCH  36 batch  510 : Loss: 2.3289 Perplexity: 10.267\n",
            "Avg 10 batchs takes  5.72  min\n",
            "EPOCH  36 batch  520 : Loss: 2.3176 Perplexity: 10.1514\n",
            "Avg 10 batchs takes  5.83  min\n",
            "EPOCH  36 batch  530 : Loss: 2.2638 Perplexity: 9.6196\n",
            "Avg 10 batchs takes  5.94  min\n",
            "EPOCH  36 batch  540 : Loss: 2.2314 Perplexity: 9.3128\n",
            "Avg 10 batchs takes  6.05  min\n",
            "EPOCH  36 batch  550 : Loss: 2.2261 Perplexity: 9.2633\n",
            "Avg 10 batchs takes  6.15  min\n",
            "EPOCH  36 batch  560 : Loss: 2.2893 Perplexity: 9.868\n",
            "Avg 10 batchs takes  6.26  min\n",
            "EPOCH  36 batch  570 : Loss: 2.2495 Perplexity: 9.4826\n",
            "Avg 10 batchs takes  6.37  min\n",
            "EPOCH  36 batch  580 : Loss: 2.3215 Perplexity: 10.1911\n",
            "Avg 10 batchs takes  6.48  min\n",
            "EPOCH  36 batch  590 : Loss: 2.3227 Perplexity: 10.2035\n",
            "Avg 10 batchs takes  6.59  min\n",
            "EPOCH  36 batch  600 : Loss: 2.2764 Perplexity: 9.7415\n",
            " ********* Epoch  36  Average Levenshtein Distance is ****:  258.65625\n",
            "Avg 10 batchs takes  6.71  min\n",
            "EPOCH  36 batch  610 : Loss: 2.3544 Perplexity: 10.5317\n",
            "Avg 10 batchs takes  6.81  min\n",
            "EPOCH  36 batch  620 : Loss: 2.2632 Perplexity: 9.6137\n",
            "Avg 10 batchs takes  6.92  min\n",
            "EPOCH  36 batch  630 : Loss: 2.2843 Perplexity: 9.8188\n",
            "Avg 10 batchs takes  7.03  min\n",
            "EPOCH  36 batch  640 : Loss: 2.285 Perplexity: 9.8253\n",
            "Avg 10 batchs takes  7.14  min\n",
            "EPOCH  36 batch  650 : Loss: 2.2962 Perplexity: 9.9368\n",
            "Avg 10 batchs takes  7.24  min\n",
            "EPOCH  36 batch  660 : Loss: 2.3873 Perplexity: 10.8843\n",
            "Avg 10 batchs takes  7.35  min\n",
            "EPOCH  36 batch  670 : Loss: 2.2668 Perplexity: 9.6485\n",
            "Avg 10 batchs takes  7.45  min\n",
            "EPOCH  36 batch  680 : Loss: 2.2794 Perplexity: 9.7713\n",
            "Avg 10 batchs takes  7.57  min\n",
            "EPOCH  36 batch  690 : Loss: 2.2036 Perplexity: 9.058\n",
            "Avg 10 batchs takes  7.67  min\n",
            "EPOCH  36 batch  700 : Loss: 2.3343 Perplexity: 10.3227\n",
            "Avg 10 batchs takes  7.78  min\n",
            "EPOCH  36 batch  710 : Loss: 2.3434 Perplexity: 10.4169\n",
            "Avg 10 batchs takes  7.9  min\n",
            "EPOCH  36 batch  720 : Loss: 2.2907 Perplexity: 9.8815\n",
            "Avg 10 batchs takes  8.01  min\n",
            "EPOCH  36 batch  730 : Loss: 2.2849 Perplexity: 9.8246\n",
            "Avg 10 batchs takes  8.12  min\n",
            "EPOCH  36 batch  740 : Loss: 2.3269 Perplexity: 10.2458\n",
            "Avg 10 batchs takes  8.23  min\n",
            "EPOCH  36 batch  750 : Loss: 2.3055 Perplexity: 10.0289\n",
            "Avg 10 batchs takes  8.33  min\n",
            "EPOCH  36 batch  760 : Loss: 2.2983 Perplexity: 9.9574\n",
            "Avg 10 batchs takes  8.44  min\n",
            "EPOCH  36 batch  770 : Loss: 2.3113 Perplexity: 10.0875\n",
            "Training loss after one epoch is: 2.4199178218841553\n",
            "Time take for an epoch is: 8.46  min\n",
            " ********************* Epoch  36  ******************\n",
            "Test loss   36  is  3.946850619997297\n",
            "Perplexity is:  51.772059921833055\n",
            "Avg distance is:  192.32251984126984\n",
            "Learning rate for epoch  37  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  37 batch  0 : Loss: 2.272 Perplexity: 9.6991\n",
            " ********* Epoch  37  Average Levenshtein Distance is ****:  325.75\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  37 batch  10 : Loss: 2.2612 Perplexity: 9.5946\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  37 batch  20 : Loss: 2.2519 Perplexity: 9.5055\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  37 batch  30 : Loss: 2.3575 Perplexity: 10.5647\n",
            "Avg 10 batchs takes  0.45  min\n",
            "EPOCH  37 batch  40 : Loss: 2.3452 Perplexity: 10.4359\n",
            "Avg 10 batchs takes  0.56  min\n",
            "EPOCH  37 batch  50 : Loss: 2.3009 Perplexity: 9.9834\n",
            "Avg 10 batchs takes  0.66  min\n",
            "EPOCH  37 batch  60 : Loss: 2.3558 Perplexity: 10.5468\n",
            "Avg 10 batchs takes  0.78  min\n",
            "EPOCH  37 batch  70 : Loss: 2.2997 Perplexity: 9.9711\n",
            "Avg 10 batchs takes  0.88  min\n",
            "EPOCH  37 batch  80 : Loss: 2.2878 Perplexity: 9.8528\n",
            "Avg 10 batchs takes  0.99  min\n",
            "EPOCH  37 batch  90 : Loss: 2.3581 Perplexity: 10.5709\n",
            "Avg 10 batchs takes  1.1  min\n",
            "EPOCH  37 batch  100 : Loss: 2.2762 Perplexity: 9.74\n",
            "Avg 10 batchs takes  1.2  min\n",
            "EPOCH  37 batch  110 : Loss: 2.3789 Perplexity: 10.7934\n",
            "Avg 10 batchs takes  1.3  min\n",
            "EPOCH  37 batch  120 : Loss: 2.1783 Perplexity: 8.8311\n",
            "Avg 10 batchs takes  1.41  min\n",
            "EPOCH  37 batch  130 : Loss: 2.3261 Perplexity: 10.2377\n",
            "Avg 10 batchs takes  1.52  min\n",
            "EPOCH  37 batch  140 : Loss: 2.2967 Perplexity: 9.941\n",
            "Avg 10 batchs takes  1.63  min\n",
            "EPOCH  37 batch  150 : Loss: 2.2915 Perplexity: 9.8899\n",
            "Avg 10 batchs takes  1.73  min\n",
            "EPOCH  37 batch  160 : Loss: 2.2945 Perplexity: 9.9191\n",
            "Avg 10 batchs takes  1.84  min\n",
            "EPOCH  37 batch  170 : Loss: 2.2953 Perplexity: 9.9278\n",
            "Avg 10 batchs takes  1.95  min\n",
            "EPOCH  37 batch  180 : Loss: 2.3798 Perplexity: 10.8027\n",
            "Avg 10 batchs takes  2.05  min\n",
            "EPOCH  37 batch  190 : Loss: 2.3395 Perplexity: 10.3758\n",
            "Avg 10 batchs takes  2.16  min\n",
            "EPOCH  37 batch  200 : Loss: 2.266 Perplexity: 9.6404\n",
            "Avg 10 batchs takes  2.27  min\n",
            "EPOCH  37 batch  210 : Loss: 2.2767 Perplexity: 9.7448\n",
            "Avg 10 batchs takes  2.38  min\n",
            "EPOCH  37 batch  220 : Loss: 2.2436 Perplexity: 9.4272\n",
            "Avg 10 batchs takes  2.49  min\n",
            "EPOCH  37 batch  230 : Loss: 2.2449 Perplexity: 9.4395\n",
            "Avg 10 batchs takes  2.59  min\n",
            "EPOCH  37 batch  240 : Loss: 2.2457 Perplexity: 9.4475\n",
            "Avg 10 batchs takes  2.7  min\n",
            "EPOCH  37 batch  250 : Loss: 2.34 Perplexity: 10.3814\n",
            "Avg 10 batchs takes  2.81  min\n",
            "EPOCH  37 batch  260 : Loss: 2.3274 Perplexity: 10.251\n",
            "Avg 10 batchs takes  2.92  min\n",
            "EPOCH  37 batch  270 : Loss: 2.2273 Perplexity: 9.2749\n",
            "Avg 10 batchs takes  3.02  min\n",
            "EPOCH  37 batch  280 : Loss: 2.3673 Perplexity: 10.669\n",
            "Avg 10 batchs takes  3.13  min\n",
            "EPOCH  37 batch  290 : Loss: 2.316 Perplexity: 10.1354\n",
            "Avg 10 batchs takes  3.23  min\n",
            "EPOCH  37 batch  300 : Loss: 2.3362 Perplexity: 10.3415\n",
            " ********* Epoch  37  Average Levenshtein Distance is ****:  284.28125\n",
            "Avg 10 batchs takes  3.34  min\n",
            "EPOCH  37 batch  310 : Loss: 2.2941 Perplexity: 9.9153\n",
            "Avg 10 batchs takes  3.45  min\n",
            "EPOCH  37 batch  320 : Loss: 2.2861 Perplexity: 9.8364\n",
            "Avg 10 batchs takes  3.56  min\n",
            "EPOCH  37 batch  330 : Loss: 2.3114 Perplexity: 10.0885\n",
            "Avg 10 batchs takes  3.67  min\n",
            "EPOCH  37 batch  340 : Loss: 2.347 Perplexity: 10.4544\n",
            "Avg 10 batchs takes  3.77  min\n",
            "EPOCH  37 batch  350 : Loss: 2.3116 Perplexity: 10.0907\n",
            "Avg 10 batchs takes  3.88  min\n",
            "EPOCH  37 batch  360 : Loss: 2.4087 Perplexity: 11.1196\n",
            "Avg 10 batchs takes  3.99  min\n",
            "EPOCH  37 batch  370 : Loss: 2.3201 Perplexity: 10.1769\n",
            "Avg 10 batchs takes  4.1  min\n",
            "EPOCH  37 batch  380 : Loss: 2.2279 Perplexity: 9.2802\n",
            "Avg 10 batchs takes  4.2  min\n",
            "EPOCH  37 batch  390 : Loss: 2.337 Perplexity: 10.3505\n",
            "Avg 10 batchs takes  4.31  min\n",
            "EPOCH  37 batch  400 : Loss: 2.3613 Perplexity: 10.6043\n",
            "Avg 10 batchs takes  4.42  min\n",
            "EPOCH  37 batch  410 : Loss: 2.3124 Perplexity: 10.0983\n",
            "Avg 10 batchs takes  4.52  min\n",
            "EPOCH  37 batch  420 : Loss: 2.3214 Perplexity: 10.1896\n",
            "Avg 10 batchs takes  4.63  min\n",
            "EPOCH  37 batch  430 : Loss: 2.2218 Perplexity: 9.224\n",
            "Avg 10 batchs takes  4.73  min\n",
            "EPOCH  37 batch  440 : Loss: 2.4144 Perplexity: 11.1831\n",
            "Avg 10 batchs takes  4.84  min\n",
            "EPOCH  37 batch  450 : Loss: 2.2944 Perplexity: 9.9183\n",
            "Avg 10 batchs takes  4.95  min\n",
            "EPOCH  37 batch  460 : Loss: 2.2904 Perplexity: 9.8792\n",
            "Avg 10 batchs takes  5.06  min\n",
            "EPOCH  37 batch  470 : Loss: 2.2223 Perplexity: 9.2286\n",
            "Avg 10 batchs takes  5.17  min\n",
            "EPOCH  37 batch  480 : Loss: 2.3297 Perplexity: 10.2749\n",
            "Avg 10 batchs takes  5.28  min\n",
            "EPOCH  37 batch  490 : Loss: 2.3158 Perplexity: 10.1329\n",
            "Avg 10 batchs takes  5.38  min\n",
            "EPOCH  37 batch  500 : Loss: 2.2817 Perplexity: 9.7933\n",
            "Avg 10 batchs takes  5.49  min\n",
            "EPOCH  37 batch  510 : Loss: 2.2836 Perplexity: 9.8115\n",
            "Avg 10 batchs takes  5.6  min\n",
            "EPOCH  37 batch  520 : Loss: 2.3478 Perplexity: 10.4628\n",
            "Avg 10 batchs takes  5.7  min\n",
            "EPOCH  37 batch  530 : Loss: 2.3028 Perplexity: 10.0025\n",
            "Avg 10 batchs takes  5.81  min\n",
            "EPOCH  37 batch  540 : Loss: 2.259 Perplexity: 9.5738\n",
            "Avg 10 batchs takes  5.92  min\n",
            "EPOCH  37 batch  550 : Loss: 2.2082 Perplexity: 9.0993\n",
            "Avg 10 batchs takes  6.03  min\n",
            "EPOCH  37 batch  560 : Loss: 2.3086 Perplexity: 10.0599\n",
            "Avg 10 batchs takes  6.14  min\n",
            "EPOCH  37 batch  570 : Loss: 2.3272 Perplexity: 10.2488\n",
            "Avg 10 batchs takes  6.25  min\n",
            "EPOCH  37 batch  580 : Loss: 2.3112 Perplexity: 10.0862\n",
            "Avg 10 batchs takes  6.36  min\n",
            "EPOCH  37 batch  590 : Loss: 2.3271 Perplexity: 10.248\n",
            "Avg 10 batchs takes  6.47  min\n",
            "EPOCH  37 batch  600 : Loss: 2.2475 Perplexity: 9.4641\n",
            " ********* Epoch  37  Average Levenshtein Distance is ****:  300.0625\n",
            "Avg 10 batchs takes  6.58  min\n",
            "EPOCH  37 batch  610 : Loss: 2.3843 Perplexity: 10.8514\n",
            "Avg 10 batchs takes  6.69  min\n",
            "EPOCH  37 batch  620 : Loss: 2.3674 Perplexity: 10.6701\n",
            "Avg 10 batchs takes  6.79  min\n",
            "EPOCH  37 batch  630 : Loss: 2.3655 Perplexity: 10.6495\n",
            "Avg 10 batchs takes  6.9  min\n",
            "EPOCH  37 batch  640 : Loss: 2.2824 Perplexity: 9.8001\n",
            "Avg 10 batchs takes  7.01  min\n",
            "EPOCH  37 batch  650 : Loss: 2.1862 Perplexity: 8.9009\n",
            "Avg 10 batchs takes  7.12  min\n",
            "EPOCH  37 batch  660 : Loss: 2.2448 Perplexity: 9.4388\n",
            "Avg 10 batchs takes  7.22  min\n",
            "EPOCH  37 batch  670 : Loss: 2.2609 Perplexity: 9.5913\n",
            "Avg 10 batchs takes  7.33  min\n",
            "EPOCH  37 batch  680 : Loss: 2.343 Perplexity: 10.4123\n",
            "Avg 10 batchs takes  7.44  min\n",
            "EPOCH  37 batch  690 : Loss: 2.2102 Perplexity: 9.118\n",
            "Avg 10 batchs takes  7.55  min\n",
            "EPOCH  37 batch  700 : Loss: 2.2262 Perplexity: 9.2646\n",
            "Avg 10 batchs takes  7.66  min\n",
            "EPOCH  37 batch  710 : Loss: 2.3354 Perplexity: 10.3337\n",
            "Avg 10 batchs takes  7.77  min\n",
            "EPOCH  37 batch  720 : Loss: 2.3221 Perplexity: 10.1967\n",
            "Avg 10 batchs takes  7.88  min\n",
            "EPOCH  37 batch  730 : Loss: 2.2228 Perplexity: 9.2329\n",
            "Avg 10 batchs takes  7.98  min\n",
            "EPOCH  37 batch  740 : Loss: 2.3684 Perplexity: 10.6798\n",
            "Avg 10 batchs takes  8.09  min\n",
            "EPOCH  37 batch  750 : Loss: 2.2319 Perplexity: 9.3173\n",
            "Avg 10 batchs takes  8.2  min\n",
            "EPOCH  37 batch  760 : Loss: 2.2523 Perplexity: 9.5092\n",
            "Avg 10 batchs takes  8.32  min\n",
            "EPOCH  37 batch  770 : Loss: 2.2349 Perplexity: 9.3457\n",
            "Training loss after one epoch is: 2.335193634033203\n",
            "Time take for an epoch is: 8.34  min\n",
            " ********************* Epoch  37  ******************\n",
            "Test loss   37  is  4.008457851409912\n",
            "Perplexity is:  55.06189143683838\n",
            "Avg distance is:  153.1563492063492\n",
            "Learning rate for epoch  38  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  38 batch  0 : Loss: 2.3128 Perplexity: 10.1024\n",
            " ********* Epoch  38  Average Levenshtein Distance is ****:  229.15625\n",
            "Avg 10 batchs takes  0.13  min\n",
            "EPOCH  38 batch  10 : Loss: 2.2725 Perplexity: 9.7038\n",
            "Avg 10 batchs takes  0.24  min\n",
            "EPOCH  38 batch  20 : Loss: 2.2407 Perplexity: 9.3996\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  38 batch  30 : Loss: 2.3598 Perplexity: 10.5892\n",
            "Avg 10 batchs takes  0.45  min\n",
            "EPOCH  38 batch  40 : Loss: 2.2796 Perplexity: 9.7725\n",
            "Avg 10 batchs takes  0.57  min\n",
            "EPOCH  38 batch  50 : Loss: 2.2836 Perplexity: 9.8115\n",
            "Avg 10 batchs takes  0.67  min\n",
            "EPOCH  38 batch  60 : Loss: 2.222 Perplexity: 9.226\n",
            "Avg 10 batchs takes  0.78  min\n",
            "EPOCH  38 batch  70 : Loss: 2.3099 Perplexity: 10.0738\n",
            "Avg 10 batchs takes  0.89  min\n",
            "EPOCH  38 batch  80 : Loss: 2.2887 Perplexity: 9.8621\n",
            "Avg 10 batchs takes  1.0  min\n",
            "EPOCH  38 batch  90 : Loss: 2.2336 Perplexity: 9.3331\n",
            "Avg 10 batchs takes  1.11  min\n",
            "EPOCH  38 batch  100 : Loss: 2.2768 Perplexity: 9.7455\n",
            "Avg 10 batchs takes  1.22  min\n",
            "EPOCH  38 batch  110 : Loss: 2.2131 Perplexity: 9.144\n",
            "Avg 10 batchs takes  1.33  min\n",
            "EPOCH  38 batch  120 : Loss: 2.2755 Perplexity: 9.7329\n",
            "Avg 10 batchs takes  1.43  min\n",
            "EPOCH  38 batch  130 : Loss: 2.2828 Perplexity: 9.8037\n",
            "Avg 10 batchs takes  1.55  min\n",
            "EPOCH  38 batch  140 : Loss: 2.3017 Perplexity: 9.9909\n",
            "Avg 10 batchs takes  1.65  min\n",
            "EPOCH  38 batch  150 : Loss: 2.3013 Perplexity: 9.9872\n",
            "Avg 10 batchs takes  1.77  min\n",
            "EPOCH  38 batch  160 : Loss: 2.3315 Perplexity: 10.2932\n",
            "Avg 10 batchs takes  1.88  min\n",
            "EPOCH  38 batch  170 : Loss: 2.2885 Perplexity: 9.86\n",
            "Avg 10 batchs takes  2.0  min\n",
            "EPOCH  38 batch  180 : Loss: 2.334 Perplexity: 10.3187\n",
            "Avg 10 batchs takes  2.11  min\n",
            "EPOCH  38 batch  190 : Loss: 2.289 Perplexity: 9.8649\n",
            "Avg 10 batchs takes  2.22  min\n",
            "EPOCH  38 batch  200 : Loss: 2.2554 Perplexity: 9.5394\n",
            "Avg 10 batchs takes  2.33  min\n",
            "EPOCH  38 batch  210 : Loss: 2.2952 Perplexity: 9.926\n",
            "Avg 10 batchs takes  2.44  min\n",
            "EPOCH  38 batch  220 : Loss: 2.2884 Perplexity: 9.8589\n",
            "Avg 10 batchs takes  2.55  min\n",
            "EPOCH  38 batch  230 : Loss: 2.3171 Perplexity: 10.1466\n",
            "Avg 10 batchs takes  2.66  min\n",
            "EPOCH  38 batch  240 : Loss: 2.3293 Perplexity: 10.2704\n",
            "Avg 10 batchs takes  2.77  min\n",
            "EPOCH  38 batch  250 : Loss: 2.2872 Perplexity: 9.8472\n",
            "Avg 10 batchs takes  2.88  min\n",
            "EPOCH  38 batch  260 : Loss: 2.2841 Perplexity: 9.8168\n",
            "Avg 10 batchs takes  2.99  min\n",
            "EPOCH  38 batch  270 : Loss: 2.2378 Perplexity: 9.3724\n",
            "Avg 10 batchs takes  3.11  min\n",
            "EPOCH  38 batch  280 : Loss: 2.2116 Perplexity: 9.1303\n",
            "Avg 10 batchs takes  3.21  min\n",
            "EPOCH  38 batch  290 : Loss: 2.207 Perplexity: 9.0881\n",
            "Avg 10 batchs takes  3.32  min\n",
            "EPOCH  38 batch  300 : Loss: 2.3159 Perplexity: 10.1343\n",
            " ********* Epoch  38  Average Levenshtein Distance is ****:  235.40625\n",
            "Avg 10 batchs takes  3.44  min\n",
            "EPOCH  38 batch  310 : Loss: 2.3529 Perplexity: 10.5157\n",
            "Avg 10 batchs takes  3.55  min\n",
            "EPOCH  38 batch  320 : Loss: 2.2911 Perplexity: 9.8859\n",
            "Avg 10 batchs takes  3.66  min\n",
            "EPOCH  38 batch  330 : Loss: 2.3117 Perplexity: 10.0919\n",
            "Avg 10 batchs takes  3.76  min\n",
            "EPOCH  38 batch  340 : Loss: 2.2587 Perplexity: 9.5704\n",
            "Avg 10 batchs takes  3.87  min\n",
            "EPOCH  38 batch  350 : Loss: 2.2903 Perplexity: 9.878\n",
            "Avg 10 batchs takes  3.98  min\n",
            "EPOCH  38 batch  360 : Loss: 2.3108 Perplexity: 10.0822\n",
            "Avg 10 batchs takes  4.08  min\n",
            "EPOCH  38 batch  370 : Loss: 2.3087 Perplexity: 10.0615\n",
            "Avg 10 batchs takes  4.19  min\n",
            "EPOCH  38 batch  380 : Loss: 2.276 Perplexity: 9.7374\n",
            "Avg 10 batchs takes  4.29  min\n",
            "EPOCH  38 batch  390 : Loss: 2.2937 Perplexity: 9.9114\n",
            "Avg 10 batchs takes  4.4  min\n",
            "EPOCH  38 batch  400 : Loss: 2.1833 Perplexity: 8.8759\n",
            "Avg 10 batchs takes  4.51  min\n",
            "EPOCH  38 batch  410 : Loss: 2.2807 Perplexity: 9.7831\n",
            "Avg 10 batchs takes  4.62  min\n",
            "EPOCH  38 batch  420 : Loss: 2.3566 Perplexity: 10.5553\n",
            "Avg 10 batchs takes  4.72  min\n",
            "EPOCH  38 batch  430 : Loss: 2.2889 Perplexity: 9.8637\n",
            "Avg 10 batchs takes  4.83  min\n",
            "EPOCH  38 batch  440 : Loss: 2.3291 Perplexity: 10.2689\n",
            "Avg 10 batchs takes  4.94  min\n",
            "EPOCH  38 batch  450 : Loss: 2.2274 Perplexity: 9.2761\n",
            "Avg 10 batchs takes  5.05  min\n",
            "EPOCH  38 batch  460 : Loss: 2.2805 Perplexity: 9.7811\n",
            "Avg 10 batchs takes  5.16  min\n",
            "EPOCH  38 batch  470 : Loss: 2.3595 Perplexity: 10.5858\n",
            "Avg 10 batchs takes  5.26  min\n",
            "EPOCH  38 batch  480 : Loss: 2.3007 Perplexity: 9.9809\n",
            "Avg 10 batchs takes  5.37  min\n",
            "EPOCH  38 batch  490 : Loss: 2.2912 Perplexity: 9.8867\n",
            "Avg 10 batchs takes  5.48  min\n",
            "EPOCH  38 batch  500 : Loss: 2.3145 Perplexity: 10.1199\n",
            "Avg 10 batchs takes  5.59  min\n",
            "EPOCH  38 batch  510 : Loss: 2.267 Perplexity: 9.6506\n",
            "Avg 10 batchs takes  5.7  min\n",
            "EPOCH  38 batch  520 : Loss: 2.3384 Perplexity: 10.3646\n",
            "Avg 10 batchs takes  5.8  min\n",
            "EPOCH  38 batch  530 : Loss: 2.292 Perplexity: 9.8947\n",
            "Avg 10 batchs takes  5.91  min\n",
            "EPOCH  38 batch  540 : Loss: 2.3252 Perplexity: 10.2288\n",
            "Avg 10 batchs takes  6.02  min\n",
            "EPOCH  38 batch  550 : Loss: 2.2787 Perplexity: 9.7644\n",
            "Avg 10 batchs takes  6.12  min\n",
            "EPOCH  38 batch  560 : Loss: 2.2142 Perplexity: 9.154\n",
            "Avg 10 batchs takes  6.22  min\n",
            "EPOCH  38 batch  570 : Loss: 2.2312 Perplexity: 9.3111\n",
            "Avg 10 batchs takes  6.33  min\n",
            "EPOCH  38 batch  580 : Loss: 2.2883 Perplexity: 9.8579\n",
            "Avg 10 batchs takes  6.44  min\n",
            "EPOCH  38 batch  590 : Loss: 2.3513 Perplexity: 10.4989\n",
            "Avg 10 batchs takes  6.55  min\n",
            "EPOCH  38 batch  600 : Loss: 2.3321 Perplexity: 10.2998\n",
            " ********* Epoch  38  Average Levenshtein Distance is ****:  263.625\n",
            "Avg 10 batchs takes  6.65  min\n",
            "EPOCH  38 batch  610 : Loss: 2.3255 Perplexity: 10.2316\n",
            "Avg 10 batchs takes  6.76  min\n",
            "EPOCH  38 batch  620 : Loss: 2.2772 Perplexity: 9.7496\n",
            "Avg 10 batchs takes  6.85  min\n",
            "EPOCH  38 batch  630 : Loss: 2.2509 Perplexity: 9.496\n",
            "Avg 10 batchs takes  6.96  min\n",
            "EPOCH  38 batch  640 : Loss: 2.3112 Perplexity: 10.0866\n",
            "Avg 10 batchs takes  7.06  min\n",
            "EPOCH  38 batch  650 : Loss: 2.2164 Perplexity: 9.1743\n",
            "Avg 10 batchs takes  7.18  min\n",
            "EPOCH  38 batch  660 : Loss: 2.3055 Perplexity: 10.0293\n",
            "Avg 10 batchs takes  7.28  min\n",
            "EPOCH  38 batch  670 : Loss: 2.2835 Perplexity: 9.8107\n",
            "Avg 10 batchs takes  7.39  min\n",
            "EPOCH  38 batch  680 : Loss: 2.2483 Perplexity: 9.4712\n",
            "Avg 10 batchs takes  7.5  min\n",
            "EPOCH  38 batch  690 : Loss: 2.3623 Perplexity: 10.6157\n",
            "Avg 10 batchs takes  7.6  min\n",
            "EPOCH  38 batch  700 : Loss: 2.2677 Perplexity: 9.6569\n",
            "Avg 10 batchs takes  7.71  min\n",
            "EPOCH  38 batch  710 : Loss: 2.2394 Perplexity: 9.3876\n",
            "Avg 10 batchs takes  7.81  min\n",
            "EPOCH  38 batch  720 : Loss: 2.3299 Perplexity: 10.2765\n",
            "Avg 10 batchs takes  7.92  min\n",
            "EPOCH  38 batch  730 : Loss: 2.2724 Perplexity: 9.7026\n",
            "Avg 10 batchs takes  8.02  min\n",
            "EPOCH  38 batch  740 : Loss: 2.2991 Perplexity: 9.9652\n",
            "Avg 10 batchs takes  8.13  min\n",
            "EPOCH  38 batch  750 : Loss: 2.3269 Perplexity: 10.2456\n",
            "Avg 10 batchs takes  8.24  min\n",
            "EPOCH  38 batch  760 : Loss: 2.2521 Perplexity: 9.5073\n",
            "Avg 10 batchs takes  8.35  min\n",
            "EPOCH  38 batch  770 : Loss: 2.2596 Perplexity: 9.5795\n",
            "Training loss after one epoch is: 2.2799665927886963\n",
            "Time take for an epoch is: 8.37  min\n",
            " ********************* Epoch  38  ******************\n",
            "Test loss   38  is  3.6645197527749196\n",
            "Perplexity is:  39.03738406526655\n",
            "Avg distance is:  135.38849206349207\n",
            "Learning rate for epoch  39  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  39 batch  0 : Loss: 2.3779 Perplexity: 10.7826\n",
            " ********* Epoch  39  Average Levenshtein Distance is ****:  265.125\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  39 batch  10 : Loss: 2.3059 Perplexity: 10.0333\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  39 batch  20 : Loss: 2.3683 Perplexity: 10.6789\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  39 batch  30 : Loss: 2.2612 Perplexity: 9.5943\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  39 batch  40 : Loss: 2.4084 Perplexity: 11.1166\n",
            "Avg 10 batchs takes  0.55  min\n",
            "EPOCH  39 batch  50 : Loss: 2.3522 Perplexity: 10.5092\n",
            "Avg 10 batchs takes  0.66  min\n",
            "EPOCH  39 batch  60 : Loss: 2.324 Perplexity: 10.2165\n",
            "Avg 10 batchs takes  0.77  min\n",
            "EPOCH  39 batch  70 : Loss: 2.2668 Perplexity: 9.6486\n",
            "Avg 10 batchs takes  0.88  min\n",
            "EPOCH  39 batch  80 : Loss: 2.2445 Perplexity: 9.4354\n",
            "Avg 10 batchs takes  0.99  min\n",
            "EPOCH  39 batch  90 : Loss: 2.1797 Perplexity: 8.8437\n",
            "Avg 10 batchs takes  1.1  min\n",
            "EPOCH  39 batch  100 : Loss: 2.2484 Perplexity: 9.4729\n",
            "Avg 10 batchs takes  1.2  min\n",
            "EPOCH  39 batch  110 : Loss: 2.2519 Perplexity: 9.5058\n",
            "Avg 10 batchs takes  1.32  min\n",
            "EPOCH  39 batch  120 : Loss: 2.2904 Perplexity: 9.8787\n",
            "Avg 10 batchs takes  1.43  min\n",
            "EPOCH  39 batch  130 : Loss: 2.3048 Perplexity: 10.0221\n",
            "Avg 10 batchs takes  1.53  min\n",
            "EPOCH  39 batch  140 : Loss: 2.3004 Perplexity: 9.9777\n",
            "Avg 10 batchs takes  1.63  min\n",
            "EPOCH  39 batch  150 : Loss: 2.3897 Perplexity: 10.9098\n",
            "Avg 10 batchs takes  1.73  min\n",
            "EPOCH  39 batch  160 : Loss: 2.3089 Perplexity: 10.0631\n",
            "Avg 10 batchs takes  1.83  min\n",
            "EPOCH  39 batch  170 : Loss: 2.3344 Perplexity: 10.3231\n",
            "Avg 10 batchs takes  1.94  min\n",
            "EPOCH  39 batch  180 : Loss: 2.2274 Perplexity: 9.2755\n",
            "Avg 10 batchs takes  2.04  min\n",
            "EPOCH  39 batch  190 : Loss: 2.2714 Perplexity: 9.6927\n",
            "Avg 10 batchs takes  2.15  min\n",
            "EPOCH  39 batch  200 : Loss: 2.2656 Perplexity: 9.6373\n",
            "Avg 10 batchs takes  2.26  min\n",
            "EPOCH  39 batch  210 : Loss: 2.2924 Perplexity: 9.8983\n",
            "Avg 10 batchs takes  2.37  min\n",
            "EPOCH  39 batch  220 : Loss: 2.3002 Perplexity: 9.9763\n",
            "Avg 10 batchs takes  2.48  min\n",
            "EPOCH  39 batch  230 : Loss: 2.2895 Perplexity: 9.8695\n",
            "Avg 10 batchs takes  2.59  min\n",
            "EPOCH  39 batch  240 : Loss: 2.2459 Perplexity: 9.4486\n",
            "Avg 10 batchs takes  2.7  min\n",
            "EPOCH  39 batch  250 : Loss: 2.2411 Perplexity: 9.4036\n",
            "Avg 10 batchs takes  2.8  min\n",
            "EPOCH  39 batch  260 : Loss: 2.2792 Perplexity: 9.7689\n",
            "Avg 10 batchs takes  2.91  min\n",
            "EPOCH  39 batch  270 : Loss: 2.3024 Perplexity: 9.9983\n",
            "Avg 10 batchs takes  3.01  min\n",
            "EPOCH  39 batch  280 : Loss: 2.1499 Perplexity: 8.5843\n",
            "Avg 10 batchs takes  3.12  min\n",
            "EPOCH  39 batch  290 : Loss: 2.2801 Perplexity: 9.778\n",
            "Avg 10 batchs takes  3.23  min\n",
            "EPOCH  39 batch  300 : Loss: 2.317 Perplexity: 10.1454\n",
            " ********* Epoch  39  Average Levenshtein Distance is ****:  269.90625\n",
            "Avg 10 batchs takes  3.34  min\n",
            "EPOCH  39 batch  310 : Loss: 2.3389 Perplexity: 10.3694\n",
            "Avg 10 batchs takes  3.45  min\n",
            "EPOCH  39 batch  320 : Loss: 2.2119 Perplexity: 9.133\n",
            "Avg 10 batchs takes  3.56  min\n",
            "EPOCH  39 batch  330 : Loss: 2.3218 Perplexity: 10.1939\n",
            "Avg 10 batchs takes  3.66  min\n",
            "EPOCH  39 batch  340 : Loss: 2.4327 Perplexity: 11.3894\n",
            "Avg 10 batchs takes  3.77  min\n",
            "EPOCH  39 batch  350 : Loss: 2.2412 Perplexity: 9.4047\n",
            "Avg 10 batchs takes  3.88  min\n",
            "EPOCH  39 batch  360 : Loss: 2.3026 Perplexity: 10.0004\n",
            "Avg 10 batchs takes  3.98  min\n",
            "EPOCH  39 batch  370 : Loss: 2.2857 Perplexity: 9.8322\n",
            "Avg 10 batchs takes  4.09  min\n",
            "EPOCH  39 batch  380 : Loss: 2.219 Perplexity: 9.1977\n",
            "Avg 10 batchs takes  4.19  min\n",
            "EPOCH  39 batch  390 : Loss: 2.3002 Perplexity: 9.9766\n",
            "Avg 10 batchs takes  4.3  min\n",
            "EPOCH  39 batch  400 : Loss: 2.2801 Perplexity: 9.7778\n",
            "Avg 10 batchs takes  4.41  min\n",
            "EPOCH  39 batch  410 : Loss: 2.2752 Perplexity: 9.7294\n",
            "Avg 10 batchs takes  4.51  min\n",
            "EPOCH  39 batch  420 : Loss: 2.2154 Perplexity: 9.1648\n",
            "Avg 10 batchs takes  4.62  min\n",
            "EPOCH  39 batch  430 : Loss: 2.2757 Perplexity: 9.7346\n",
            "Avg 10 batchs takes  4.72  min\n",
            "EPOCH  39 batch  440 : Loss: 2.2944 Perplexity: 9.9185\n",
            "Avg 10 batchs takes  4.83  min\n",
            "EPOCH  39 batch  450 : Loss: 2.262 Perplexity: 9.6021\n",
            "Avg 10 batchs takes  4.93  min\n",
            "EPOCH  39 batch  460 : Loss: 2.4036 Perplexity: 11.0629\n",
            "Avg 10 batchs takes  5.03  min\n",
            "EPOCH  39 batch  470 : Loss: 2.2825 Perplexity: 9.8009\n",
            "Avg 10 batchs takes  5.14  min\n",
            "EPOCH  39 batch  480 : Loss: 2.2386 Perplexity: 9.3805\n",
            "Avg 10 batchs takes  5.24  min\n",
            "EPOCH  39 batch  490 : Loss: 2.3584 Perplexity: 10.5744\n",
            "Avg 10 batchs takes  5.34  min\n",
            "EPOCH  39 batch  500 : Loss: 2.1697 Perplexity: 8.7554\n",
            "Avg 10 batchs takes  5.45  min\n",
            "EPOCH  39 batch  510 : Loss: 2.2146 Perplexity: 9.1578\n",
            "Avg 10 batchs takes  5.55  min\n",
            "EPOCH  39 batch  520 : Loss: 2.2495 Perplexity: 9.4829\n",
            "Avg 10 batchs takes  5.65  min\n",
            "EPOCH  39 batch  530 : Loss: 2.2402 Perplexity: 9.3953\n",
            "Avg 10 batchs takes  5.76  min\n",
            "EPOCH  39 batch  540 : Loss: 2.2097 Perplexity: 9.1126\n",
            "Avg 10 batchs takes  5.86  min\n",
            "EPOCH  39 batch  550 : Loss: 2.2831 Perplexity: 9.8073\n",
            "Avg 10 batchs takes  5.97  min\n",
            "EPOCH  39 batch  560 : Loss: 2.304 Perplexity: 10.0137\n",
            "Avg 10 batchs takes  6.07  min\n",
            "EPOCH  39 batch  570 : Loss: 2.204 Perplexity: 9.0612\n",
            "Avg 10 batchs takes  6.17  min\n",
            "EPOCH  39 batch  580 : Loss: 2.2887 Perplexity: 9.8625\n",
            "Avg 10 batchs takes  6.29  min\n",
            "EPOCH  39 batch  590 : Loss: 2.4372 Perplexity: 11.4405\n",
            "Avg 10 batchs takes  6.39  min\n",
            "EPOCH  39 batch  600 : Loss: 2.2828 Perplexity: 9.8044\n",
            " ********* Epoch  39  Average Levenshtein Distance is ****:  186.15625\n",
            "Avg 10 batchs takes  6.51  min\n",
            "EPOCH  39 batch  610 : Loss: 2.2691 Perplexity: 9.6709\n",
            "Avg 10 batchs takes  6.61  min\n",
            "EPOCH  39 batch  620 : Loss: 2.2813 Perplexity: 9.7896\n",
            "Avg 10 batchs takes  6.72  min\n",
            "EPOCH  39 batch  630 : Loss: 2.3092 Perplexity: 10.0665\n",
            "Avg 10 batchs takes  6.82  min\n",
            "EPOCH  39 batch  640 : Loss: 2.2217 Perplexity: 9.2226\n",
            "Avg 10 batchs takes  6.93  min\n",
            "EPOCH  39 batch  650 : Loss: 2.2636 Perplexity: 9.6177\n",
            "Avg 10 batchs takes  7.04  min\n",
            "EPOCH  39 batch  660 : Loss: 2.2835 Perplexity: 9.8111\n",
            "Avg 10 batchs takes  7.15  min\n",
            "EPOCH  39 batch  670 : Loss: 2.3661 Perplexity: 10.6562\n",
            "Avg 10 batchs takes  7.25  min\n",
            "EPOCH  39 batch  680 : Loss: 2.3355 Perplexity: 10.3343\n",
            "Avg 10 batchs takes  7.36  min\n",
            "EPOCH  39 batch  690 : Loss: 2.2818 Perplexity: 9.7938\n",
            "Avg 10 batchs takes  7.47  min\n",
            "EPOCH  39 batch  700 : Loss: 2.3153 Perplexity: 10.1279\n",
            "Avg 10 batchs takes  7.58  min\n",
            "EPOCH  39 batch  710 : Loss: 2.2472 Perplexity: 9.4613\n",
            "Avg 10 batchs takes  7.69  min\n",
            "EPOCH  39 batch  720 : Loss: 2.2902 Perplexity: 9.8772\n",
            "Avg 10 batchs takes  7.79  min\n",
            "EPOCH  39 batch  730 : Loss: 2.2706 Perplexity: 9.6853\n",
            "Avg 10 batchs takes  7.9  min\n",
            "EPOCH  39 batch  740 : Loss: 2.329 Perplexity: 10.2677\n",
            "Avg 10 batchs takes  8.01  min\n",
            "EPOCH  39 batch  750 : Loss: 2.366 Perplexity: 10.6551\n",
            "Avg 10 batchs takes  8.12  min\n",
            "EPOCH  39 batch  760 : Loss: 2.254 Perplexity: 9.5262\n",
            "Avg 10 batchs takes  8.22  min\n",
            "EPOCH  39 batch  770 : Loss: 2.2815 Perplexity: 9.791\n",
            "Training loss after one epoch is: 2.3960134983062744\n",
            "Time take for an epoch is: 8.24  min\n",
            " ********************* Epoch  39  ******************\n",
            "Test loss   39  is  3.8205113070351735\n",
            "Perplexity is:  45.62753203567831\n",
            "Avg distance is:  150.57529761904763\n",
            "Learning rate for epoch  40  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  40 batch  0 : Loss: 2.2186 Perplexity: 9.1949\n",
            " ********* Epoch  40  Average Levenshtein Distance is ****:  279.5\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  40 batch  10 : Loss: 2.1746 Perplexity: 8.7982\n",
            "Avg 10 batchs takes  0.22  min\n",
            "EPOCH  40 batch  20 : Loss: 2.3391 Perplexity: 10.3723\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  40 batch  30 : Loss: 2.3067 Perplexity: 10.0414\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  40 batch  40 : Loss: 2.2726 Perplexity: 9.7044\n",
            "Avg 10 batchs takes  0.56  min\n",
            "EPOCH  40 batch  50 : Loss: 2.2297 Perplexity: 9.2975\n",
            "Avg 10 batchs takes  0.67  min\n",
            "EPOCH  40 batch  60 : Loss: 2.3096 Perplexity: 10.0701\n",
            "Avg 10 batchs takes  0.78  min\n",
            "EPOCH  40 batch  70 : Loss: 2.3375 Perplexity: 10.3549\n",
            "Avg 10 batchs takes  0.89  min\n",
            "EPOCH  40 batch  80 : Loss: 2.3389 Perplexity: 10.3698\n",
            "Avg 10 batchs takes  0.99  min\n",
            "EPOCH  40 batch  90 : Loss: 2.3081 Perplexity: 10.0556\n",
            "Avg 10 batchs takes  1.1  min\n",
            "EPOCH  40 batch  100 : Loss: 2.2614 Perplexity: 9.5967\n",
            "Avg 10 batchs takes  1.22  min\n",
            "EPOCH  40 batch  110 : Loss: 2.2991 Perplexity: 9.9653\n",
            "Avg 10 batchs takes  1.33  min\n",
            "EPOCH  40 batch  120 : Loss: 2.2738 Perplexity: 9.7161\n",
            "Avg 10 batchs takes  1.44  min\n",
            "EPOCH  40 batch  130 : Loss: 2.214 Perplexity: 9.1525\n",
            "Avg 10 batchs takes  1.55  min\n",
            "EPOCH  40 batch  140 : Loss: 2.2973 Perplexity: 9.9477\n",
            "Avg 10 batchs takes  1.65  min\n",
            "EPOCH  40 batch  150 : Loss: 2.29 Perplexity: 9.8747\n",
            "Avg 10 batchs takes  1.75  min\n",
            "EPOCH  40 batch  160 : Loss: 2.3055 Perplexity: 10.0293\n",
            "Avg 10 batchs takes  1.86  min\n",
            "EPOCH  40 batch  170 : Loss: 2.2896 Perplexity: 9.8709\n",
            "Avg 10 batchs takes  1.97  min\n",
            "EPOCH  40 batch  180 : Loss: 2.3903 Perplexity: 10.9164\n",
            "Avg 10 batchs takes  2.07  min\n",
            "EPOCH  40 batch  190 : Loss: 2.3692 Perplexity: 10.6891\n",
            "Avg 10 batchs takes  2.18  min\n",
            "EPOCH  40 batch  200 : Loss: 2.2616 Perplexity: 9.5983\n",
            "Avg 10 batchs takes  2.29  min\n",
            "EPOCH  40 batch  210 : Loss: 2.2788 Perplexity: 9.765\n",
            "Avg 10 batchs takes  2.39  min\n",
            "EPOCH  40 batch  220 : Loss: 2.3758 Perplexity: 10.7596\n",
            "Avg 10 batchs takes  2.5  min\n",
            "EPOCH  40 batch  230 : Loss: 2.2296 Perplexity: 9.2965\n",
            "Avg 10 batchs takes  2.6  min\n",
            "EPOCH  40 batch  240 : Loss: 2.2965 Perplexity: 9.9398\n",
            "Avg 10 batchs takes  2.71  min\n",
            "EPOCH  40 batch  250 : Loss: 2.2413 Perplexity: 9.4058\n",
            "Avg 10 batchs takes  2.82  min\n",
            "EPOCH  40 batch  260 : Loss: 2.2897 Perplexity: 9.872\n",
            "Avg 10 batchs takes  2.93  min\n",
            "EPOCH  40 batch  270 : Loss: 2.2377 Perplexity: 9.3717\n",
            "Avg 10 batchs takes  3.04  min\n",
            "EPOCH  40 batch  280 : Loss: 2.3429 Perplexity: 10.4109\n",
            "Avg 10 batchs takes  3.14  min\n",
            "EPOCH  40 batch  290 : Loss: 2.2648 Perplexity: 9.6287\n",
            "Avg 10 batchs takes  3.24  min\n",
            "EPOCH  40 batch  300 : Loss: 2.3056 Perplexity: 10.0298\n",
            " ********* Epoch  40  Average Levenshtein Distance is ****:  199.21875\n",
            "Avg 10 batchs takes  3.35  min\n",
            "EPOCH  40 batch  310 : Loss: 2.3189 Perplexity: 10.165\n",
            "Avg 10 batchs takes  3.46  min\n",
            "EPOCH  40 batch  320 : Loss: 2.2026 Perplexity: 9.0484\n",
            "Avg 10 batchs takes  3.57  min\n",
            "EPOCH  40 batch  330 : Loss: 2.2769 Perplexity: 9.7468\n",
            "Avg 10 batchs takes  3.67  min\n",
            "EPOCH  40 batch  340 : Loss: 2.1815 Perplexity: 8.8598\n",
            "Avg 10 batchs takes  3.77  min\n",
            "EPOCH  40 batch  350 : Loss: 2.2785 Perplexity: 9.762\n",
            "Avg 10 batchs takes  3.88  min\n",
            "EPOCH  40 batch  360 : Loss: 2.2559 Perplexity: 9.5434\n",
            "Avg 10 batchs takes  3.98  min\n",
            "EPOCH  40 batch  370 : Loss: 2.2961 Perplexity: 9.935\n",
            "Avg 10 batchs takes  4.09  min\n",
            "EPOCH  40 batch  380 : Loss: 2.2803 Perplexity: 9.7798\n",
            "Avg 10 batchs takes  4.2  min\n",
            "EPOCH  40 batch  390 : Loss: 2.2633 Perplexity: 9.6146\n",
            "Avg 10 batchs takes  4.3  min\n",
            "EPOCH  40 batch  400 : Loss: 2.3167 Perplexity: 10.1418\n",
            "Avg 10 batchs takes  4.41  min\n",
            "EPOCH  40 batch  410 : Loss: 2.3311 Perplexity: 10.2893\n",
            "Avg 10 batchs takes  4.51  min\n",
            "EPOCH  40 batch  420 : Loss: 2.2235 Perplexity: 9.2397\n",
            "Avg 10 batchs takes  4.62  min\n",
            "EPOCH  40 batch  430 : Loss: 2.3009 Perplexity: 9.9831\n",
            "Avg 10 batchs takes  4.73  min\n",
            "EPOCH  40 batch  440 : Loss: 2.2579 Perplexity: 9.5631\n",
            "Avg 10 batchs takes  4.83  min\n",
            "EPOCH  40 batch  450 : Loss: 2.2371 Perplexity: 9.3663\n",
            "Avg 10 batchs takes  4.94  min\n",
            "EPOCH  40 batch  460 : Loss: 2.2718 Perplexity: 9.6971\n",
            "Avg 10 batchs takes  5.04  min\n",
            "EPOCH  40 batch  470 : Loss: 2.3226 Perplexity: 10.202\n",
            "Avg 10 batchs takes  5.15  min\n",
            "EPOCH  40 batch  480 : Loss: 2.2714 Perplexity: 9.6928\n",
            "Avg 10 batchs takes  5.25  min\n",
            "EPOCH  40 batch  490 : Loss: 2.321 Perplexity: 10.1857\n",
            "Avg 10 batchs takes  5.36  min\n",
            "EPOCH  40 batch  500 : Loss: 2.289 Perplexity: 9.8655\n",
            "Avg 10 batchs takes  5.46  min\n",
            "EPOCH  40 batch  510 : Loss: 2.2225 Perplexity: 9.2306\n",
            "Avg 10 batchs takes  5.57  min\n",
            "EPOCH  40 batch  520 : Loss: 2.3009 Perplexity: 9.9828\n",
            "Avg 10 batchs takes  5.67  min\n",
            "EPOCH  40 batch  530 : Loss: 2.288 Perplexity: 9.8549\n",
            "Avg 10 batchs takes  5.78  min\n",
            "EPOCH  40 batch  540 : Loss: 2.2078 Perplexity: 9.0957\n",
            "Avg 10 batchs takes  5.89  min\n",
            "EPOCH  40 batch  550 : Loss: 2.3338 Perplexity: 10.3173\n",
            "Avg 10 batchs takes  6.0  min\n",
            "EPOCH  40 batch  560 : Loss: 2.2056 Perplexity: 9.0758\n",
            "Avg 10 batchs takes  6.11  min\n",
            "EPOCH  40 batch  570 : Loss: 2.2421 Perplexity: 9.413\n",
            "Avg 10 batchs takes  6.2  min\n",
            "EPOCH  40 batch  580 : Loss: 2.3892 Perplexity: 10.9044\n",
            "Avg 10 batchs takes  6.31  min\n",
            "EPOCH  40 batch  590 : Loss: 2.368 Perplexity: 10.6761\n",
            "Avg 10 batchs takes  6.42  min\n",
            "EPOCH  40 batch  600 : Loss: 2.2951 Perplexity: 9.9256\n",
            " ********* Epoch  40  Average Levenshtein Distance is ****:  232.28125\n",
            "Avg 10 batchs takes  6.52  min\n",
            "EPOCH  40 batch  610 : Loss: 2.1963 Perplexity: 8.9915\n",
            "Avg 10 batchs takes  6.63  min\n",
            "EPOCH  40 batch  620 : Loss: 2.2156 Perplexity: 9.1669\n",
            "Avg 10 batchs takes  6.73  min\n",
            "EPOCH  40 batch  630 : Loss: 2.3588 Perplexity: 10.5788\n",
            "Avg 10 batchs takes  6.84  min\n",
            "EPOCH  40 batch  640 : Loss: 2.3082 Perplexity: 10.0565\n",
            "Avg 10 batchs takes  6.94  min\n",
            "EPOCH  40 batch  650 : Loss: 2.354 Perplexity: 10.5271\n",
            "Avg 10 batchs takes  7.04  min\n",
            "EPOCH  40 batch  660 : Loss: 2.2737 Perplexity: 9.7154\n",
            "Avg 10 batchs takes  7.15  min\n",
            "EPOCH  40 batch  670 : Loss: 2.2862 Perplexity: 9.8372\n",
            "Avg 10 batchs takes  7.25  min\n",
            "EPOCH  40 batch  680 : Loss: 2.3319 Perplexity: 10.2977\n",
            "Avg 10 batchs takes  7.36  min\n",
            "EPOCH  40 batch  690 : Loss: 2.2854 Perplexity: 9.8292\n",
            "Avg 10 batchs takes  7.46  min\n",
            "EPOCH  40 batch  700 : Loss: 2.3128 Perplexity: 10.1025\n",
            "Avg 10 batchs takes  7.57  min\n",
            "EPOCH  40 batch  710 : Loss: 2.168 Perplexity: 8.7409\n",
            "Avg 10 batchs takes  7.68  min\n",
            "EPOCH  40 batch  720 : Loss: 2.3705 Perplexity: 10.7027\n",
            "Avg 10 batchs takes  7.78  min\n",
            "EPOCH  40 batch  730 : Loss: 2.2786 Perplexity: 9.7631\n",
            "Avg 10 batchs takes  7.89  min\n",
            "EPOCH  40 batch  740 : Loss: 2.2653 Perplexity: 9.6336\n",
            "Avg 10 batchs takes  7.99  min\n",
            "EPOCH  40 batch  750 : Loss: 2.2426 Perplexity: 9.4182\n",
            "Avg 10 batchs takes  8.1  min\n",
            "EPOCH  40 batch  760 : Loss: 2.2052 Perplexity: 9.072\n",
            "Avg 10 batchs takes  8.2  min\n",
            "EPOCH  40 batch  770 : Loss: 2.2058 Perplexity: 9.0772\n",
            "Training loss after one epoch is: 2.2914187908172607\n",
            "Time take for an epoch is: 8.22  min\n",
            " ********************* Epoch  40  ******************\n",
            "Test loss   40  is  3.743466656548636\n",
            "Perplexity is:  42.2441826915183\n",
            "Avg distance is:  218.86259920634922\n",
            "Learning rate for epoch  41  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  41 batch  0 : Loss: 2.319 Perplexity: 10.1652\n",
            " ********* Epoch  41  Average Levenshtein Distance is ****:  276.25\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  41 batch  10 : Loss: 2.3388 Perplexity: 10.3685\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  41 batch  20 : Loss: 2.2289 Perplexity: 9.2898\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  41 batch  30 : Loss: 2.3976 Perplexity: 10.9963\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  41 batch  40 : Loss: 2.2634 Perplexity: 9.6157\n",
            "Avg 10 batchs takes  0.54  min\n",
            "EPOCH  41 batch  50 : Loss: 2.3038 Perplexity: 10.0118\n",
            "Avg 10 batchs takes  0.65  min\n",
            "EPOCH  41 batch  60 : Loss: 2.2183 Perplexity: 9.1917\n",
            "Avg 10 batchs takes  0.77  min\n",
            "EPOCH  41 batch  70 : Loss: 2.2836 Perplexity: 9.8116\n",
            "Avg 10 batchs takes  0.88  min\n",
            "EPOCH  41 batch  80 : Loss: 2.2004 Perplexity: 9.0289\n",
            "Avg 10 batchs takes  0.98  min\n",
            "EPOCH  41 batch  90 : Loss: 2.2749 Perplexity: 9.7268\n",
            "Avg 10 batchs takes  1.08  min\n",
            "EPOCH  41 batch  100 : Loss: 2.2551 Perplexity: 9.5362\n",
            "Avg 10 batchs takes  1.19  min\n",
            "EPOCH  41 batch  110 : Loss: 2.2738 Perplexity: 9.7166\n",
            "Avg 10 batchs takes  1.29  min\n",
            "EPOCH  41 batch  120 : Loss: 2.2606 Perplexity: 9.5887\n",
            "Avg 10 batchs takes  1.4  min\n",
            "EPOCH  41 batch  130 : Loss: 2.2301 Perplexity: 9.3011\n",
            "Avg 10 batchs takes  1.51  min\n",
            "EPOCH  41 batch  140 : Loss: 2.2555 Perplexity: 9.5401\n",
            "Avg 10 batchs takes  1.61  min\n",
            "EPOCH  41 batch  150 : Loss: 2.2196 Perplexity: 9.2035\n",
            "Avg 10 batchs takes  1.72  min\n",
            "EPOCH  41 batch  160 : Loss: 2.1785 Perplexity: 8.8327\n",
            "Avg 10 batchs takes  1.82  min\n",
            "EPOCH  41 batch  170 : Loss: 2.2747 Perplexity: 9.7248\n",
            "Avg 10 batchs takes  1.93  min\n",
            "EPOCH  41 batch  180 : Loss: 2.3652 Perplexity: 10.6458\n",
            "Avg 10 batchs takes  2.04  min\n",
            "EPOCH  41 batch  190 : Loss: 2.3596 Perplexity: 10.5869\n",
            "Avg 10 batchs takes  2.14  min\n",
            "EPOCH  41 batch  200 : Loss: 2.3045 Perplexity: 10.0193\n",
            "Avg 10 batchs takes  2.25  min\n",
            "EPOCH  41 batch  210 : Loss: 2.275 Perplexity: 9.7283\n",
            "Avg 10 batchs takes  2.35  min\n",
            "EPOCH  41 batch  220 : Loss: 2.2574 Perplexity: 9.5586\n",
            "Avg 10 batchs takes  2.45  min\n",
            "EPOCH  41 batch  230 : Loss: 2.2847 Perplexity: 9.8229\n",
            "Avg 10 batchs takes  2.56  min\n",
            "EPOCH  41 batch  240 : Loss: 2.3245 Perplexity: 10.2213\n",
            "Avg 10 batchs takes  2.66  min\n",
            "EPOCH  41 batch  250 : Loss: 2.2869 Perplexity: 9.844\n",
            "Avg 10 batchs takes  2.78  min\n",
            "EPOCH  41 batch  260 : Loss: 2.2123 Perplexity: 9.1366\n",
            "Avg 10 batchs takes  2.88  min\n",
            "EPOCH  41 batch  270 : Loss: 2.1497 Perplexity: 8.5819\n",
            "Avg 10 batchs takes  2.99  min\n",
            "EPOCH  41 batch  280 : Loss: 2.2981 Perplexity: 9.9549\n",
            "Avg 10 batchs takes  3.09  min\n",
            "EPOCH  41 batch  290 : Loss: 2.3317 Perplexity: 10.2958\n",
            "Avg 10 batchs takes  3.19  min\n",
            "EPOCH  41 batch  300 : Loss: 2.2964 Perplexity: 9.9381\n",
            " ********* Epoch  41  Average Levenshtein Distance is ****:  204.96875\n",
            "Avg 10 batchs takes  3.3  min\n",
            "EPOCH  41 batch  310 : Loss: 2.2526 Perplexity: 9.512\n",
            "Avg 10 batchs takes  3.41  min\n",
            "EPOCH  41 batch  320 : Loss: 2.3352 Perplexity: 10.3315\n",
            "Avg 10 batchs takes  3.52  min\n",
            "EPOCH  41 batch  330 : Loss: 2.2868 Perplexity: 9.8435\n",
            "Avg 10 batchs takes  3.62  min\n",
            "EPOCH  41 batch  340 : Loss: 2.3094 Perplexity: 10.068\n",
            "Avg 10 batchs takes  3.73  min\n",
            "EPOCH  41 batch  350 : Loss: 2.2529 Perplexity: 9.5148\n",
            "Avg 10 batchs takes  3.83  min\n",
            "EPOCH  41 batch  360 : Loss: 2.2068 Perplexity: 9.087\n",
            "Avg 10 batchs takes  3.94  min\n",
            "EPOCH  41 batch  370 : Loss: 2.2363 Perplexity: 9.3582\n",
            "Avg 10 batchs takes  4.04  min\n",
            "EPOCH  41 batch  380 : Loss: 2.2946 Perplexity: 9.9203\n",
            "Avg 10 batchs takes  4.15  min\n",
            "EPOCH  41 batch  390 : Loss: 2.2959 Perplexity: 9.9338\n",
            "Avg 10 batchs takes  4.26  min\n",
            "EPOCH  41 batch  400 : Loss: 2.2589 Perplexity: 9.5729\n",
            "Avg 10 batchs takes  4.36  min\n",
            "EPOCH  41 batch  410 : Loss: 2.2512 Perplexity: 9.4991\n",
            "Avg 10 batchs takes  4.47  min\n",
            "EPOCH  41 batch  420 : Loss: 2.188 Perplexity: 8.9177\n",
            "Avg 10 batchs takes  4.57  min\n",
            "EPOCH  41 batch  430 : Loss: 2.3095 Perplexity: 10.0697\n",
            "Avg 10 batchs takes  4.68  min\n",
            "EPOCH  41 batch  440 : Loss: 2.224 Perplexity: 9.2443\n",
            "Avg 10 batchs takes  4.79  min\n",
            "EPOCH  41 batch  450 : Loss: 2.1553 Perplexity: 8.6305\n",
            "Avg 10 batchs takes  4.9  min\n",
            "EPOCH  41 batch  460 : Loss: 2.3414 Perplexity: 10.3953\n",
            "Avg 10 batchs takes  5.0  min\n",
            "EPOCH  41 batch  470 : Loss: 2.229 Perplexity: 9.291\n",
            "Avg 10 batchs takes  5.11  min\n",
            "EPOCH  41 batch  480 : Loss: 2.2683 Perplexity: 9.6626\n",
            "Avg 10 batchs takes  5.22  min\n",
            "EPOCH  41 batch  490 : Loss: 2.2324 Perplexity: 9.3227\n",
            "Avg 10 batchs takes  5.32  min\n",
            "EPOCH  41 batch  500 : Loss: 2.308 Perplexity: 10.0546\n",
            "Avg 10 batchs takes  5.42  min\n",
            "EPOCH  41 batch  510 : Loss: 2.2412 Perplexity: 9.4043\n",
            "Avg 10 batchs takes  5.53  min\n",
            "EPOCH  41 batch  520 : Loss: 2.1143 Perplexity: 8.2838\n",
            "Avg 10 batchs takes  5.64  min\n",
            "EPOCH  41 batch  530 : Loss: 2.2775 Perplexity: 9.7519\n",
            "Avg 10 batchs takes  5.75  min\n",
            "EPOCH  41 batch  540 : Loss: 2.261 Perplexity: 9.5926\n",
            "Avg 10 batchs takes  5.86  min\n",
            "EPOCH  41 batch  550 : Loss: 2.2835 Perplexity: 9.8112\n",
            "Avg 10 batchs takes  5.96  min\n",
            "EPOCH  41 batch  560 : Loss: 2.3045 Perplexity: 10.0189\n",
            "Avg 10 batchs takes  6.07  min\n",
            "EPOCH  41 batch  570 : Loss: 2.2438 Perplexity: 9.429\n",
            "Avg 10 batchs takes  6.18  min\n",
            "EPOCH  41 batch  580 : Loss: 2.3162 Perplexity: 10.1366\n",
            "Avg 10 batchs takes  6.28  min\n",
            "EPOCH  41 batch  590 : Loss: 2.2733 Perplexity: 9.7113\n",
            "Avg 10 batchs takes  6.39  min\n",
            "EPOCH  41 batch  600 : Loss: 2.299 Perplexity: 9.9644\n",
            " ********* Epoch  41  Average Levenshtein Distance is ****:  283.09375\n",
            "Avg 10 batchs takes  6.49  min\n",
            "EPOCH  41 batch  610 : Loss: 2.3051 Perplexity: 10.0254\n",
            "Avg 10 batchs takes  6.6  min\n",
            "EPOCH  41 batch  620 : Loss: 2.2448 Perplexity: 9.4384\n",
            "Avg 10 batchs takes  6.7  min\n",
            "EPOCH  41 batch  630 : Loss: 2.2804 Perplexity: 9.7809\n",
            "Avg 10 batchs takes  6.81  min\n",
            "EPOCH  41 batch  640 : Loss: 2.2932 Perplexity: 9.907\n",
            "Avg 10 batchs takes  6.91  min\n",
            "EPOCH  41 batch  650 : Loss: 2.3028 Perplexity: 10.0019\n",
            "Avg 10 batchs takes  7.01  min\n",
            "EPOCH  41 batch  660 : Loss: 2.3197 Perplexity: 10.1727\n",
            "Avg 10 batchs takes  7.12  min\n",
            "EPOCH  41 batch  670 : Loss: 2.3018 Perplexity: 9.9923\n",
            "Avg 10 batchs takes  7.22  min\n",
            "EPOCH  41 batch  680 : Loss: 2.2934 Perplexity: 9.9082\n",
            "Avg 10 batchs takes  7.33  min\n",
            "EPOCH  41 batch  690 : Loss: 2.2657 Perplexity: 9.6382\n",
            "Avg 10 batchs takes  7.44  min\n",
            "EPOCH  41 batch  700 : Loss: 2.2725 Perplexity: 9.7035\n",
            "Avg 10 batchs takes  7.54  min\n",
            "EPOCH  41 batch  710 : Loss: 2.2548 Perplexity: 9.5329\n",
            "Avg 10 batchs takes  7.64  min\n",
            "EPOCH  41 batch  720 : Loss: 2.2336 Perplexity: 9.3333\n",
            "Avg 10 batchs takes  7.75  min\n",
            "EPOCH  41 batch  730 : Loss: 2.3032 Perplexity: 10.0058\n",
            "Avg 10 batchs takes  7.86  min\n",
            "EPOCH  41 batch  740 : Loss: 2.331 Perplexity: 10.2884\n",
            "Avg 10 batchs takes  7.96  min\n",
            "EPOCH  41 batch  750 : Loss: 2.253 Perplexity: 9.5161\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  41 batch  760 : Loss: 2.4001 Perplexity: 11.0238\n",
            "Avg 10 batchs takes  8.17  min\n",
            "EPOCH  41 batch  770 : Loss: 2.2811 Perplexity: 9.787\n",
            "Training loss after one epoch is: 2.2722678184509277\n",
            "Time take for an epoch is: 8.19  min\n",
            " ********************* Epoch  41  ******************\n",
            "Test loss   41  is  4.434545346668789\n",
            "Perplexity is:  84.31378263059351\n",
            "Avg distance is:  203.57420634920635\n",
            "Learning rate for epoch  42  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  42 batch  0 : Loss: 2.2215 Perplexity: 9.2213\n",
            " ********* Epoch  42  Average Levenshtein Distance is ****:  277.625\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  42 batch  10 : Loss: 2.2995 Perplexity: 9.9691\n",
            "Avg 10 batchs takes  0.22  min\n",
            "EPOCH  42 batch  20 : Loss: 2.356 Perplexity: 10.5491\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  42 batch  30 : Loss: 2.2919 Perplexity: 9.8938\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  42 batch  40 : Loss: 2.2593 Perplexity: 9.5767\n",
            "Avg 10 batchs takes  0.54  min\n",
            "EPOCH  42 batch  50 : Loss: 2.2255 Perplexity: 9.2584\n",
            "Avg 10 batchs takes  0.65  min\n",
            "EPOCH  42 batch  60 : Loss: 2.2346 Perplexity: 9.3426\n",
            "Avg 10 batchs takes  0.76  min\n",
            "EPOCH  42 batch  70 : Loss: 2.2496 Perplexity: 9.4843\n",
            "Avg 10 batchs takes  0.87  min\n",
            "EPOCH  42 batch  80 : Loss: 2.2848 Perplexity: 9.8235\n",
            "Avg 10 batchs takes  0.97  min\n",
            "EPOCH  42 batch  90 : Loss: 2.2284 Perplexity: 9.2853\n",
            "Avg 10 batchs takes  1.08  min\n",
            "EPOCH  42 batch  100 : Loss: 2.3076 Perplexity: 10.0499\n",
            "Avg 10 batchs takes  1.18  min\n",
            "EPOCH  42 batch  110 : Loss: 2.3552 Perplexity: 10.5401\n",
            "Avg 10 batchs takes  1.29  min\n",
            "EPOCH  42 batch  120 : Loss: 2.2668 Perplexity: 9.6489\n",
            "Avg 10 batchs takes  1.39  min\n",
            "EPOCH  42 batch  130 : Loss: 2.2444 Perplexity: 9.4348\n",
            "Avg 10 batchs takes  1.5  min\n",
            "EPOCH  42 batch  140 : Loss: 2.2312 Perplexity: 9.3112\n",
            "Avg 10 batchs takes  1.61  min\n",
            "EPOCH  42 batch  150 : Loss: 2.3121 Perplexity: 10.0959\n",
            "Avg 10 batchs takes  1.71  min\n",
            "EPOCH  42 batch  160 : Loss: 2.3144 Perplexity: 10.1184\n",
            "Avg 10 batchs takes  1.81  min\n",
            "EPOCH  42 batch  170 : Loss: 2.2455 Perplexity: 9.4448\n",
            "Avg 10 batchs takes  1.92  min\n",
            "EPOCH  42 batch  180 : Loss: 2.3346 Perplexity: 10.3251\n",
            "Avg 10 batchs takes  2.02  min\n",
            "EPOCH  42 batch  190 : Loss: 2.2646 Perplexity: 9.6268\n",
            "Avg 10 batchs takes  2.13  min\n",
            "EPOCH  42 batch  200 : Loss: 2.3082 Perplexity: 10.0563\n",
            "Avg 10 batchs takes  2.24  min\n",
            "EPOCH  42 batch  210 : Loss: 2.2371 Perplexity: 9.366\n",
            "Avg 10 batchs takes  2.35  min\n",
            "EPOCH  42 batch  220 : Loss: 2.3264 Perplexity: 10.2413\n",
            "Avg 10 batchs takes  2.45  min\n",
            "EPOCH  42 batch  230 : Loss: 2.2588 Perplexity: 9.5716\n",
            "Avg 10 batchs takes  2.56  min\n",
            "EPOCH  42 batch  240 : Loss: 2.3025 Perplexity: 9.9994\n",
            "Avg 10 batchs takes  2.67  min\n",
            "EPOCH  42 batch  250 : Loss: 2.2129 Perplexity: 9.1426\n",
            "Avg 10 batchs takes  2.77  min\n",
            "EPOCH  42 batch  260 : Loss: 2.2968 Perplexity: 9.9426\n",
            "Avg 10 batchs takes  2.88  min\n",
            "EPOCH  42 batch  270 : Loss: 2.3262 Perplexity: 10.2393\n",
            "Avg 10 batchs takes  2.98  min\n",
            "EPOCH  42 batch  280 : Loss: 2.2346 Perplexity: 9.3428\n",
            "Avg 10 batchs takes  3.09  min\n",
            "EPOCH  42 batch  290 : Loss: 2.228 Perplexity: 9.2817\n",
            "Avg 10 batchs takes  3.2  min\n",
            "EPOCH  42 batch  300 : Loss: 2.2865 Perplexity: 9.8407\n",
            " ********* Epoch  42  Average Levenshtein Distance is ****:  197.46875\n",
            "Avg 10 batchs takes  3.31  min\n",
            "EPOCH  42 batch  310 : Loss: 2.2953 Perplexity: 9.9277\n",
            "Avg 10 batchs takes  3.41  min\n",
            "EPOCH  42 batch  320 : Loss: 2.3428 Perplexity: 10.4103\n",
            "Avg 10 batchs takes  3.51  min\n",
            "EPOCH  42 batch  330 : Loss: 2.2404 Perplexity: 9.3975\n",
            "Avg 10 batchs takes  3.61  min\n",
            "EPOCH  42 batch  340 : Loss: 2.1568 Perplexity: 8.643\n",
            "Avg 10 batchs takes  3.72  min\n",
            "EPOCH  42 batch  350 : Loss: 2.2775 Perplexity: 9.7518\n",
            "Avg 10 batchs takes  3.82  min\n",
            "EPOCH  42 batch  360 : Loss: 2.1972 Perplexity: 9.0\n",
            "Avg 10 batchs takes  3.93  min\n",
            "EPOCH  42 batch  370 : Loss: 2.2471 Perplexity: 9.4605\n",
            "Avg 10 batchs takes  4.03  min\n",
            "EPOCH  42 batch  380 : Loss: 2.2623 Perplexity: 9.6055\n",
            "Avg 10 batchs takes  4.13  min\n",
            "EPOCH  42 batch  390 : Loss: 2.2378 Perplexity: 9.373\n",
            "Avg 10 batchs takes  4.23  min\n",
            "EPOCH  42 batch  400 : Loss: 2.2762 Perplexity: 9.7395\n",
            "Avg 10 batchs takes  4.34  min\n",
            "EPOCH  42 batch  410 : Loss: 2.2791 Perplexity: 9.7678\n",
            "Avg 10 batchs takes  4.45  min\n",
            "EPOCH  42 batch  420 : Loss: 2.2293 Perplexity: 9.2936\n",
            "Avg 10 batchs takes  4.55  min\n",
            "EPOCH  42 batch  430 : Loss: 2.212 Perplexity: 9.1339\n",
            "Avg 10 batchs takes  4.66  min\n",
            "EPOCH  42 batch  440 : Loss: 2.1961 Perplexity: 8.9902\n",
            "Avg 10 batchs takes  4.77  min\n",
            "EPOCH  42 batch  450 : Loss: 2.15 Perplexity: 8.5848\n",
            "Avg 10 batchs takes  4.88  min\n",
            "EPOCH  42 batch  460 : Loss: 2.1925 Perplexity: 8.9579\n",
            "Avg 10 batchs takes  4.99  min\n",
            "EPOCH  42 batch  470 : Loss: 2.2422 Perplexity: 9.4145\n",
            "Avg 10 batchs takes  5.1  min\n",
            "EPOCH  42 batch  480 : Loss: 2.1736 Perplexity: 8.7895\n",
            "Avg 10 batchs takes  5.2  min\n",
            "EPOCH  42 batch  490 : Loss: 2.2663 Perplexity: 9.644\n",
            "Avg 10 batchs takes  5.31  min\n",
            "EPOCH  42 batch  500 : Loss: 2.1563 Perplexity: 8.639\n",
            "Avg 10 batchs takes  5.41  min\n",
            "EPOCH  42 batch  510 : Loss: 2.2862 Perplexity: 9.8374\n",
            "Avg 10 batchs takes  5.52  min\n",
            "EPOCH  42 batch  520 : Loss: 2.1741 Perplexity: 8.7946\n",
            "Avg 10 batchs takes  5.63  min\n",
            "EPOCH  42 batch  530 : Loss: 2.2056 Perplexity: 9.0756\n",
            "Avg 10 batchs takes  5.73  min\n",
            "EPOCH  42 batch  540 : Loss: 2.2068 Perplexity: 9.0865\n",
            "Avg 10 batchs takes  5.84  min\n",
            "EPOCH  42 batch  550 : Loss: 2.2827 Perplexity: 9.8028\n",
            "Avg 10 batchs takes  5.94  min\n",
            "EPOCH  42 batch  560 : Loss: 2.2742 Perplexity: 9.7199\n",
            "Avg 10 batchs takes  6.05  min\n",
            "EPOCH  42 batch  570 : Loss: 2.3103 Perplexity: 10.0778\n",
            "Avg 10 batchs takes  6.15  min\n",
            "EPOCH  42 batch  580 : Loss: 2.2512 Perplexity: 9.4996\n",
            "Avg 10 batchs takes  6.26  min\n",
            "EPOCH  42 batch  590 : Loss: 2.2281 Perplexity: 9.2825\n",
            "Avg 10 batchs takes  6.36  min\n",
            "EPOCH  42 batch  600 : Loss: 2.3054 Perplexity: 10.0284\n",
            " ********* Epoch  42  Average Levenshtein Distance is ****:  211.34375\n",
            "Avg 10 batchs takes  6.47  min\n",
            "EPOCH  42 batch  610 : Loss: 2.2758 Perplexity: 9.7355\n",
            "Avg 10 batchs takes  6.58  min\n",
            "EPOCH  42 batch  620 : Loss: 2.246 Perplexity: 9.4503\n",
            "Avg 10 batchs takes  6.68  min\n",
            "EPOCH  42 batch  630 : Loss: 2.2737 Perplexity: 9.7158\n",
            "Avg 10 batchs takes  6.79  min\n",
            "EPOCH  42 batch  640 : Loss: 2.1943 Perplexity: 8.974\n",
            "Avg 10 batchs takes  6.9  min\n",
            "EPOCH  42 batch  650 : Loss: 2.1849 Perplexity: 8.8895\n",
            "Avg 10 batchs takes  7.01  min\n",
            "EPOCH  42 batch  660 : Loss: 2.2539 Perplexity: 9.5249\n",
            "Avg 10 batchs takes  7.11  min\n",
            "EPOCH  42 batch  670 : Loss: 2.2387 Perplexity: 9.381\n",
            "Avg 10 batchs takes  7.21  min\n",
            "EPOCH  42 batch  680 : Loss: 2.339 Perplexity: 10.3713\n",
            "Avg 10 batchs takes  7.32  min\n",
            "EPOCH  42 batch  690 : Loss: 2.2928 Perplexity: 9.9023\n",
            "Avg 10 batchs takes  7.43  min\n",
            "EPOCH  42 batch  700 : Loss: 2.2326 Perplexity: 9.3245\n",
            "Avg 10 batchs takes  7.54  min\n",
            "EPOCH  42 batch  710 : Loss: 2.1775 Perplexity: 8.8243\n",
            "Avg 10 batchs takes  7.64  min\n",
            "EPOCH  42 batch  720 : Loss: 2.2738 Perplexity: 9.7164\n",
            "Avg 10 batchs takes  7.75  min\n",
            "EPOCH  42 batch  730 : Loss: 2.3815 Perplexity: 10.8211\n",
            "Avg 10 batchs takes  7.85  min\n",
            "EPOCH  42 batch  740 : Loss: 2.2393 Perplexity: 9.387\n",
            "Avg 10 batchs takes  7.96  min\n",
            "EPOCH  42 batch  750 : Loss: 2.3465 Perplexity: 10.449\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  42 batch  760 : Loss: 2.1668 Perplexity: 8.7305\n",
            "Avg 10 batchs takes  8.17  min\n",
            "EPOCH  42 batch  770 : Loss: 2.2052 Perplexity: 9.0719\n",
            "Training loss after one epoch is: 2.3562703132629395\n",
            "Time take for an epoch is: 8.19  min\n",
            " ********************* Epoch  42  ******************\n",
            "Test loss   42  is  3.9527220249176027\n",
            "Perplexity is:  52.07692877762087\n",
            "Avg distance is:  201.19841269841268\n",
            "Learning rate for epoch  43  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  43 batch  0 : Loss: 2.2105 Perplexity: 9.12\n",
            " ********* Epoch  43  Average Levenshtein Distance is ****:  133.25\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  43 batch  10 : Loss: 2.1938 Perplexity: 8.9689\n",
            "Avg 10 batchs takes  0.22  min\n",
            "EPOCH  43 batch  20 : Loss: 2.2477 Perplexity: 9.4655\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  43 batch  30 : Loss: 2.2267 Perplexity: 9.2697\n",
            "Avg 10 batchs takes  0.43  min\n",
            "EPOCH  43 batch  40 : Loss: 2.239 Perplexity: 9.3835\n",
            "Avg 10 batchs takes  0.53  min\n",
            "EPOCH  43 batch  50 : Loss: 2.2573 Perplexity: 9.5569\n",
            "Avg 10 batchs takes  0.64  min\n",
            "EPOCH  43 batch  60 : Loss: 2.3113 Perplexity: 10.0876\n",
            "Avg 10 batchs takes  0.75  min\n",
            "EPOCH  43 batch  70 : Loss: 2.3296 Perplexity: 10.2734\n",
            "Avg 10 batchs takes  0.86  min\n",
            "EPOCH  43 batch  80 : Loss: 2.2508 Perplexity: 9.4957\n",
            "Avg 10 batchs takes  0.96  min\n",
            "EPOCH  43 batch  90 : Loss: 2.3035 Perplexity: 10.0089\n",
            "Avg 10 batchs takes  1.07  min\n",
            "EPOCH  43 batch  100 : Loss: 2.2333 Perplexity: 9.3307\n",
            "Avg 10 batchs takes  1.17  min\n",
            "EPOCH  43 batch  110 : Loss: 2.2085 Perplexity: 9.1017\n",
            "Avg 10 batchs takes  1.28  min\n",
            "EPOCH  43 batch  120 : Loss: 2.2667 Perplexity: 9.6476\n",
            "Avg 10 batchs takes  1.38  min\n",
            "EPOCH  43 batch  130 : Loss: 2.2736 Perplexity: 9.7141\n",
            "Avg 10 batchs takes  1.49  min\n",
            "EPOCH  43 batch  140 : Loss: 2.2288 Perplexity: 9.289\n",
            "Avg 10 batchs takes  1.59  min\n",
            "EPOCH  43 batch  150 : Loss: 2.2639 Perplexity: 9.6209\n",
            "Avg 10 batchs takes  1.7  min\n",
            "EPOCH  43 batch  160 : Loss: 2.2096 Perplexity: 9.1118\n",
            "Avg 10 batchs takes  1.81  min\n",
            "EPOCH  43 batch  170 : Loss: 2.2448 Perplexity: 9.4388\n",
            "Avg 10 batchs takes  1.91  min\n",
            "EPOCH  43 batch  180 : Loss: 2.2309 Perplexity: 9.3084\n",
            "Avg 10 batchs takes  2.02  min\n",
            "EPOCH  43 batch  190 : Loss: 2.3134 Perplexity: 10.1088\n",
            "Avg 10 batchs takes  2.13  min\n",
            "EPOCH  43 batch  200 : Loss: 2.3149 Perplexity: 10.1241\n",
            "Avg 10 batchs takes  2.23  min\n",
            "EPOCH  43 batch  210 : Loss: 2.2171 Perplexity: 9.1806\n",
            "Avg 10 batchs takes  2.34  min\n",
            "EPOCH  43 batch  220 : Loss: 2.3286 Perplexity: 10.2632\n",
            "Avg 10 batchs takes  2.45  min\n",
            "EPOCH  43 batch  230 : Loss: 2.3068 Perplexity: 10.0418\n",
            "Avg 10 batchs takes  2.55  min\n",
            "EPOCH  43 batch  240 : Loss: 2.161 Perplexity: 8.6794\n",
            "Avg 10 batchs takes  2.66  min\n",
            "EPOCH  43 batch  250 : Loss: 2.3222 Perplexity: 10.1984\n",
            "Avg 10 batchs takes  2.77  min\n",
            "EPOCH  43 batch  260 : Loss: 2.2377 Perplexity: 9.3718\n",
            "Avg 10 batchs takes  2.87  min\n",
            "EPOCH  43 batch  270 : Loss: 2.2057 Perplexity: 9.077\n",
            "Avg 10 batchs takes  2.97  min\n",
            "EPOCH  43 batch  280 : Loss: 2.1501 Perplexity: 8.5856\n",
            "Avg 10 batchs takes  3.08  min\n",
            "EPOCH  43 batch  290 : Loss: 2.2408 Perplexity: 9.4004\n",
            "Avg 10 batchs takes  3.19  min\n",
            "EPOCH  43 batch  300 : Loss: 2.2734 Perplexity: 9.7125\n",
            " ********* Epoch  43  Average Levenshtein Distance is ****:  307.9375\n",
            "Avg 10 batchs takes  3.3  min\n",
            "EPOCH  43 batch  310 : Loss: 2.2025 Perplexity: 9.0476\n",
            "Avg 10 batchs takes  3.4  min\n",
            "EPOCH  43 batch  320 : Loss: 2.2567 Perplexity: 9.5519\n",
            "Avg 10 batchs takes  3.52  min\n",
            "EPOCH  43 batch  330 : Loss: 2.2098 Perplexity: 9.1138\n",
            "Avg 10 batchs takes  3.63  min\n",
            "EPOCH  43 batch  340 : Loss: 2.2107 Perplexity: 9.1217\n",
            "Avg 10 batchs takes  3.73  min\n",
            "EPOCH  43 batch  350 : Loss: 2.2429 Perplexity: 9.4203\n",
            "Avg 10 batchs takes  3.84  min\n",
            "EPOCH  43 batch  360 : Loss: 2.2932 Perplexity: 9.9067\n",
            "Avg 10 batchs takes  3.94  min\n",
            "EPOCH  43 batch  370 : Loss: 2.1572 Perplexity: 8.6469\n",
            "Avg 10 batchs takes  4.05  min\n",
            "EPOCH  43 batch  380 : Loss: 2.257 Perplexity: 9.5544\n",
            "Avg 10 batchs takes  4.15  min\n",
            "EPOCH  43 batch  390 : Loss: 2.1895 Perplexity: 8.9307\n",
            "Avg 10 batchs takes  4.26  min\n",
            "EPOCH  43 batch  400 : Loss: 2.2837 Perplexity: 9.8133\n",
            "Avg 10 batchs takes  4.37  min\n",
            "EPOCH  43 batch  410 : Loss: 2.2767 Perplexity: 9.7444\n",
            "Avg 10 batchs takes  4.47  min\n",
            "EPOCH  43 batch  420 : Loss: 2.1114 Perplexity: 8.2599\n",
            "Avg 10 batchs takes  4.58  min\n",
            "EPOCH  43 batch  430 : Loss: 2.311 Perplexity: 10.0841\n",
            "Avg 10 batchs takes  4.68  min\n",
            "EPOCH  43 batch  440 : Loss: 2.3088 Perplexity: 10.0626\n",
            "Avg 10 batchs takes  4.79  min\n",
            "EPOCH  43 batch  450 : Loss: 2.2045 Perplexity: 9.0653\n",
            "Avg 10 batchs takes  4.9  min\n",
            "EPOCH  43 batch  460 : Loss: 2.2104 Perplexity: 9.1194\n",
            "Avg 10 batchs takes  5.01  min\n",
            "EPOCH  43 batch  470 : Loss: 2.2554 Perplexity: 9.5393\n",
            "Avg 10 batchs takes  5.11  min\n",
            "EPOCH  43 batch  480 : Loss: 2.2728 Perplexity: 9.7068\n",
            "Avg 10 batchs takes  5.22  min\n",
            "EPOCH  43 batch  490 : Loss: 2.1919 Perplexity: 8.952\n",
            "Avg 10 batchs takes  5.33  min\n",
            "EPOCH  43 batch  500 : Loss: 2.3078 Perplexity: 10.0518\n",
            "Avg 10 batchs takes  5.43  min\n",
            "EPOCH  43 batch  510 : Loss: 2.1583 Perplexity: 8.6568\n",
            "Avg 10 batchs takes  5.54  min\n",
            "EPOCH  43 batch  520 : Loss: 2.1628 Perplexity: 8.6957\n",
            "Avg 10 batchs takes  5.64  min\n",
            "EPOCH  43 batch  530 : Loss: 2.2577 Perplexity: 9.5607\n",
            "Avg 10 batchs takes  5.75  min\n",
            "EPOCH  43 batch  540 : Loss: 2.2048 Perplexity: 9.0687\n",
            "Avg 10 batchs takes  5.85  min\n",
            "EPOCH  43 batch  550 : Loss: 2.231 Perplexity: 9.3091\n",
            "Avg 10 batchs takes  5.96  min\n",
            "EPOCH  43 batch  560 : Loss: 2.2499 Perplexity: 9.4864\n",
            "Avg 10 batchs takes  6.06  min\n",
            "EPOCH  43 batch  570 : Loss: 2.2828 Perplexity: 9.8036\n",
            "Avg 10 batchs takes  6.16  min\n",
            "EPOCH  43 batch  580 : Loss: 2.2708 Perplexity: 9.6871\n",
            "Avg 10 batchs takes  6.26  min\n",
            "EPOCH  43 batch  590 : Loss: 2.2289 Perplexity: 9.2894\n",
            "Avg 10 batchs takes  6.37  min\n",
            "EPOCH  43 batch  600 : Loss: 2.1929 Perplexity: 8.961\n",
            " ********* Epoch  43  Average Levenshtein Distance is ****:  180.1875\n",
            "Avg 10 batchs takes  6.48  min\n",
            "EPOCH  43 batch  610 : Loss: 2.228 Perplexity: 9.2817\n",
            "Avg 10 batchs takes  6.58  min\n",
            "EPOCH  43 batch  620 : Loss: 2.2814 Perplexity: 9.7902\n",
            "Avg 10 batchs takes  6.69  min\n",
            "EPOCH  43 batch  630 : Loss: 2.3219 Perplexity: 10.1954\n",
            "Avg 10 batchs takes  6.8  min\n",
            "EPOCH  43 batch  640 : Loss: 2.2709 Perplexity: 9.6884\n",
            "Avg 10 batchs takes  6.9  min\n",
            "EPOCH  43 batch  650 : Loss: 2.2945 Perplexity: 9.9191\n",
            "Avg 10 batchs takes  7.01  min\n",
            "EPOCH  43 batch  660 : Loss: 2.2761 Perplexity: 9.739\n",
            "Avg 10 batchs takes  7.11  min\n",
            "EPOCH  43 batch  670 : Loss: 2.2194 Perplexity: 9.2018\n",
            "Avg 10 batchs takes  7.21  min\n",
            "EPOCH  43 batch  680 : Loss: 2.2664 Perplexity: 9.6444\n",
            "Avg 10 batchs takes  7.32  min\n",
            "EPOCH  43 batch  690 : Loss: 2.2096 Perplexity: 9.1125\n",
            "Avg 10 batchs takes  7.43  min\n",
            "EPOCH  43 batch  700 : Loss: 2.3674 Perplexity: 10.6694\n",
            "Avg 10 batchs takes  7.54  min\n",
            "EPOCH  43 batch  710 : Loss: 2.171 Perplexity: 8.7668\n",
            "Avg 10 batchs takes  7.64  min\n",
            "EPOCH  43 batch  720 : Loss: 2.2209 Perplexity: 9.2157\n",
            "Avg 10 batchs takes  7.75  min\n",
            "EPOCH  43 batch  730 : Loss: 2.3026 Perplexity: 10.0\n",
            "Avg 10 batchs takes  7.86  min\n",
            "EPOCH  43 batch  740 : Loss: 2.2367 Perplexity: 9.3627\n",
            "Avg 10 batchs takes  7.96  min\n",
            "EPOCH  43 batch  750 : Loss: 2.2418 Perplexity: 9.4107\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  43 batch  760 : Loss: 2.2365 Perplexity: 9.3601\n",
            "Avg 10 batchs takes  8.18  min\n",
            "EPOCH  43 batch  770 : Loss: 2.1913 Perplexity: 8.9471\n",
            "Training loss after one epoch is: 2.235529899597168\n",
            "Time take for an epoch is: 8.2  min\n",
            " ********************* Epoch  43  ******************\n",
            "Test loss   43  is  3.7694953645978657\n",
            "Perplexity is:  43.35817924197574\n",
            "Avg distance is:  196.58333333333334\n",
            "Learning rate for epoch  44  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  44 batch  0 : Loss: 2.3027 Perplexity: 10.0015\n",
            " ********* Epoch  44  Average Levenshtein Distance is ****:  282.40625\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  44 batch  10 : Loss: 2.1818 Perplexity: 8.8621\n",
            "Avg 10 batchs takes  0.22  min\n",
            "EPOCH  44 batch  20 : Loss: 2.2738 Perplexity: 9.7165\n",
            "Avg 10 batchs takes  0.32  min\n",
            "EPOCH  44 batch  30 : Loss: 2.2736 Perplexity: 9.7142\n",
            "Avg 10 batchs takes  0.43  min\n",
            "EPOCH  44 batch  40 : Loss: 2.2089 Perplexity: 9.1057\n",
            "Avg 10 batchs takes  0.53  min\n",
            "EPOCH  44 batch  50 : Loss: 2.1929 Perplexity: 8.9609\n",
            "Avg 10 batchs takes  0.64  min\n",
            "EPOCH  44 batch  60 : Loss: 2.2255 Perplexity: 9.258\n",
            "Avg 10 batchs takes  0.75  min\n",
            "EPOCH  44 batch  70 : Loss: 2.2365 Perplexity: 9.3607\n",
            "Avg 10 batchs takes  0.85  min\n",
            "EPOCH  44 batch  80 : Loss: 2.2002 Perplexity: 9.027\n",
            "Avg 10 batchs takes  0.96  min\n",
            "EPOCH  44 batch  90 : Loss: 2.2715 Perplexity: 9.6936\n",
            "Avg 10 batchs takes  1.07  min\n",
            "EPOCH  44 batch  100 : Loss: 2.275 Perplexity: 9.7284\n",
            "Avg 10 batchs takes  1.18  min\n",
            "EPOCH  44 batch  110 : Loss: 2.2437 Perplexity: 9.4282\n",
            "Avg 10 batchs takes  1.29  min\n",
            "EPOCH  44 batch  120 : Loss: 2.1219 Perplexity: 8.3469\n",
            "Avg 10 batchs takes  1.39  min\n",
            "EPOCH  44 batch  130 : Loss: 2.2526 Perplexity: 9.5122\n",
            "Avg 10 batchs takes  1.5  min\n",
            "EPOCH  44 batch  140 : Loss: 2.2539 Perplexity: 9.5249\n",
            "Avg 10 batchs takes  1.6  min\n",
            "EPOCH  44 batch  150 : Loss: 2.2939 Perplexity: 9.9131\n",
            "Avg 10 batchs takes  1.7  min\n",
            "EPOCH  44 batch  160 : Loss: 2.3288 Perplexity: 10.2661\n",
            "Avg 10 batchs takes  1.81  min\n",
            "EPOCH  44 batch  170 : Loss: 2.24 Perplexity: 9.3938\n",
            "Avg 10 batchs takes  1.92  min\n",
            "EPOCH  44 batch  180 : Loss: 2.3002 Perplexity: 9.9759\n",
            "Avg 10 batchs takes  2.03  min\n",
            "EPOCH  44 batch  190 : Loss: 2.2787 Perplexity: 9.7641\n",
            "Avg 10 batchs takes  2.14  min\n",
            "EPOCH  44 batch  200 : Loss: 2.2079 Perplexity: 9.0962\n",
            "Avg 10 batchs takes  2.24  min\n",
            "EPOCH  44 batch  210 : Loss: 2.2796 Perplexity: 9.7723\n",
            "Avg 10 batchs takes  2.35  min\n",
            "EPOCH  44 batch  220 : Loss: 2.2326 Perplexity: 9.3239\n",
            "Avg 10 batchs takes  2.46  min\n",
            "EPOCH  44 batch  230 : Loss: 2.2329 Perplexity: 9.3265\n",
            "Avg 10 batchs takes  2.57  min\n",
            "EPOCH  44 batch  240 : Loss: 2.1962 Perplexity: 8.9904\n",
            "Avg 10 batchs takes  2.67  min\n",
            "EPOCH  44 batch  250 : Loss: 2.3152 Perplexity: 10.1268\n",
            "Avg 10 batchs takes  2.77  min\n",
            "EPOCH  44 batch  260 : Loss: 2.2873 Perplexity: 9.8482\n",
            "Avg 10 batchs takes  2.88  min\n",
            "EPOCH  44 batch  270 : Loss: 2.2667 Perplexity: 9.648\n",
            "Avg 10 batchs takes  2.99  min\n",
            "EPOCH  44 batch  280 : Loss: 2.1975 Perplexity: 9.0022\n",
            "Avg 10 batchs takes  3.09  min\n",
            "EPOCH  44 batch  290 : Loss: 2.2492 Perplexity: 9.48\n",
            "Avg 10 batchs takes  3.2  min\n",
            "EPOCH  44 batch  300 : Loss: 2.2789 Perplexity: 9.766\n",
            " ********* Epoch  44  Average Levenshtein Distance is ****:  286.5\n",
            "Avg 10 batchs takes  3.3  min\n",
            "EPOCH  44 batch  310 : Loss: 2.1904 Perplexity: 8.9386\n",
            "Avg 10 batchs takes  3.41  min\n",
            "EPOCH  44 batch  320 : Loss: 2.2279 Perplexity: 9.2803\n",
            "Avg 10 batchs takes  3.51  min\n",
            "EPOCH  44 batch  330 : Loss: 2.2467 Perplexity: 9.4563\n",
            "Avg 10 batchs takes  3.62  min\n",
            "EPOCH  44 batch  340 : Loss: 2.2496 Perplexity: 9.4837\n",
            "Avg 10 batchs takes  3.73  min\n",
            "EPOCH  44 batch  350 : Loss: 2.3227 Perplexity: 10.2027\n",
            "Avg 10 batchs takes  3.84  min\n",
            "EPOCH  44 batch  360 : Loss: 2.2699 Perplexity: 9.6789\n",
            "Avg 10 batchs takes  3.95  min\n",
            "EPOCH  44 batch  370 : Loss: 2.2701 Perplexity: 9.6808\n",
            "Avg 10 batchs takes  4.05  min\n",
            "EPOCH  44 batch  380 : Loss: 2.3536 Perplexity: 10.5238\n",
            "Avg 10 batchs takes  4.16  min\n",
            "EPOCH  44 batch  390 : Loss: 2.2282 Perplexity: 9.2831\n",
            "Avg 10 batchs takes  4.26  min\n",
            "EPOCH  44 batch  400 : Loss: 2.2656 Perplexity: 9.637\n",
            "Avg 10 batchs takes  4.37  min\n",
            "EPOCH  44 batch  410 : Loss: 2.1788 Perplexity: 8.8358\n",
            "Avg 10 batchs takes  4.48  min\n",
            "EPOCH  44 batch  420 : Loss: 2.2333 Perplexity: 9.3308\n",
            "Avg 10 batchs takes  4.58  min\n",
            "EPOCH  44 batch  430 : Loss: 2.3115 Perplexity: 10.0898\n",
            "Avg 10 batchs takes  4.69  min\n",
            "EPOCH  44 batch  440 : Loss: 2.2706 Perplexity: 9.6856\n",
            "Avg 10 batchs takes  4.79  min\n",
            "EPOCH  44 batch  450 : Loss: 2.3169 Perplexity: 10.1443\n",
            "Avg 10 batchs takes  4.89  min\n",
            "EPOCH  44 batch  460 : Loss: 2.2348 Perplexity: 9.3447\n",
            "Avg 10 batchs takes  5.0  min\n",
            "EPOCH  44 batch  470 : Loss: 2.2653 Perplexity: 9.6338\n",
            "Avg 10 batchs takes  5.1  min\n",
            "EPOCH  44 batch  480 : Loss: 2.3145 Perplexity: 10.1194\n",
            "Avg 10 batchs takes  5.21  min\n",
            "EPOCH  44 batch  490 : Loss: 2.2427 Perplexity: 9.4186\n",
            "Avg 10 batchs takes  5.32  min\n",
            "EPOCH  44 batch  500 : Loss: 2.1678 Perplexity: 8.7387\n",
            "Avg 10 batchs takes  5.42  min\n",
            "EPOCH  44 batch  510 : Loss: 2.2648 Perplexity: 9.6289\n",
            "Avg 10 batchs takes  5.53  min\n",
            "EPOCH  44 batch  520 : Loss: 2.1831 Perplexity: 8.8735\n",
            "Avg 10 batchs takes  5.64  min\n",
            "EPOCH  44 batch  530 : Loss: 2.223 Perplexity: 9.2353\n",
            "Avg 10 batchs takes  5.75  min\n",
            "EPOCH  44 batch  540 : Loss: 2.2715 Perplexity: 9.6935\n",
            "Avg 10 batchs takes  5.86  min\n",
            "EPOCH  44 batch  550 : Loss: 2.3076 Perplexity: 10.0498\n",
            "Avg 10 batchs takes  5.96  min\n",
            "EPOCH  44 batch  560 : Loss: 2.3242 Perplexity: 10.2189\n",
            "Avg 10 batchs takes  6.07  min\n",
            "EPOCH  44 batch  570 : Loss: 2.3453 Perplexity: 10.4364\n",
            "Avg 10 batchs takes  6.18  min\n",
            "EPOCH  44 batch  580 : Loss: 2.2668 Perplexity: 9.6483\n",
            "Avg 10 batchs takes  6.28  min\n",
            "EPOCH  44 batch  590 : Loss: 2.1568 Perplexity: 8.6437\n",
            "Avg 10 batchs takes  6.39  min\n",
            "EPOCH  44 batch  600 : Loss: 2.2751 Perplexity: 9.7287\n",
            " ********* Epoch  44  Average Levenshtein Distance is ****:  138.96875\n",
            "Avg 10 batchs takes  6.5  min\n",
            "EPOCH  44 batch  610 : Loss: 2.1848 Perplexity: 8.8885\n",
            "Avg 10 batchs takes  6.6  min\n",
            "EPOCH  44 batch  620 : Loss: 2.2816 Perplexity: 9.7928\n",
            "Avg 10 batchs takes  6.71  min\n",
            "EPOCH  44 batch  630 : Loss: 2.1629 Perplexity: 8.6964\n",
            "Avg 10 batchs takes  6.81  min\n",
            "EPOCH  44 batch  640 : Loss: 2.2113 Perplexity: 9.1276\n",
            "Avg 10 batchs takes  6.91  min\n",
            "EPOCH  44 batch  650 : Loss: 2.2523 Perplexity: 9.5091\n",
            "Avg 10 batchs takes  7.02  min\n",
            "EPOCH  44 batch  660 : Loss: 2.1629 Perplexity: 8.6963\n",
            "Avg 10 batchs takes  7.12  min\n",
            "EPOCH  44 batch  670 : Loss: 2.249 Perplexity: 9.4784\n",
            "Avg 10 batchs takes  7.22  min\n",
            "EPOCH  44 batch  680 : Loss: 2.2163 Perplexity: 9.1732\n",
            "Avg 10 batchs takes  7.33  min\n",
            "EPOCH  44 batch  690 : Loss: 2.1861 Perplexity: 8.9008\n",
            "Avg 10 batchs takes  7.44  min\n",
            "EPOCH  44 batch  700 : Loss: 2.1957 Perplexity: 8.9864\n",
            "Avg 10 batchs takes  7.54  min\n",
            "EPOCH  44 batch  710 : Loss: 2.183 Perplexity: 8.8725\n",
            "Avg 10 batchs takes  7.65  min\n",
            "EPOCH  44 batch  720 : Loss: 2.1827 Perplexity: 8.8705\n",
            "Avg 10 batchs takes  7.75  min\n",
            "EPOCH  44 batch  730 : Loss: 2.2126 Perplexity: 9.1397\n",
            "Avg 10 batchs takes  7.85  min\n",
            "EPOCH  44 batch  740 : Loss: 2.2135 Perplexity: 9.1478\n",
            "Avg 10 batchs takes  7.96  min\n",
            "EPOCH  44 batch  750 : Loss: 2.1868 Perplexity: 8.9069\n",
            "Avg 10 batchs takes  8.06  min\n",
            "EPOCH  44 batch  760 : Loss: 2.1344 Perplexity: 8.452\n",
            "Avg 10 batchs takes  8.17  min\n",
            "EPOCH  44 batch  770 : Loss: 2.2054 Perplexity: 9.0738\n",
            "Training loss after one epoch is: 2.050028085708618\n",
            "Time take for an epoch is: 8.19  min\n",
            " ********************* Epoch  44  ******************\n",
            "Test loss   44  is  3.68776045526777\n",
            "Perplexity is:  39.95526507998323\n",
            "Avg distance is:  163.79494047619048\n",
            "Learning rate for epoch  45  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  45 batch  0 : Loss: 2.1876 Perplexity: 8.9135\n",
            " ********* Epoch  45  Average Levenshtein Distance is ****:  295.09375\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  45 batch  10 : Loss: 2.238 Perplexity: 9.3742\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  45 batch  20 : Loss: 2.2623 Perplexity: 9.6048\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  45 batch  30 : Loss: 2.2666 Perplexity: 9.6469\n",
            "Avg 10 batchs takes  0.43  min\n",
            "EPOCH  45 batch  40 : Loss: 2.1978 Perplexity: 9.0056\n",
            "Avg 10 batchs takes  0.53  min\n",
            "EPOCH  45 batch  50 : Loss: 2.2614 Perplexity: 9.5963\n",
            "Avg 10 batchs takes  0.64  min\n",
            "EPOCH  45 batch  60 : Loss: 2.3196 Perplexity: 10.1716\n",
            "Avg 10 batchs takes  0.75  min\n",
            "EPOCH  45 batch  70 : Loss: 2.2452 Perplexity: 9.4424\n",
            "Avg 10 batchs takes  0.85  min\n",
            "EPOCH  45 batch  80 : Loss: 2.2241 Perplexity: 9.2451\n",
            "Avg 10 batchs takes  0.96  min\n",
            "EPOCH  45 batch  90 : Loss: 2.2089 Perplexity: 9.1054\n",
            "Avg 10 batchs takes  1.07  min\n",
            "EPOCH  45 batch  100 : Loss: 2.2927 Perplexity: 9.902\n",
            "Avg 10 batchs takes  1.18  min\n",
            "EPOCH  45 batch  110 : Loss: 2.1956 Perplexity: 8.9858\n",
            "Avg 10 batchs takes  1.29  min\n",
            "EPOCH  45 batch  120 : Loss: 2.2553 Perplexity: 9.5383\n",
            "Avg 10 batchs takes  1.4  min\n",
            "EPOCH  45 batch  130 : Loss: 2.2594 Perplexity: 9.5771\n",
            "Avg 10 batchs takes  1.5  min\n",
            "EPOCH  45 batch  140 : Loss: 2.2914 Perplexity: 9.8885\n",
            "Avg 10 batchs takes  1.61  min\n",
            "EPOCH  45 batch  150 : Loss: 2.3513 Perplexity: 10.4994\n",
            "Avg 10 batchs takes  1.71  min\n",
            "EPOCH  45 batch  160 : Loss: 2.2521 Perplexity: 9.5077\n",
            "Avg 10 batchs takes  1.82  min\n",
            "EPOCH  45 batch  170 : Loss: 2.1804 Perplexity: 8.8498\n",
            "Avg 10 batchs takes  1.93  min\n",
            "EPOCH  45 batch  180 : Loss: 2.1535 Perplexity: 8.6146\n",
            "Avg 10 batchs takes  2.04  min\n",
            "EPOCH  45 batch  190 : Loss: 2.2405 Perplexity: 9.3976\n",
            "Avg 10 batchs takes  2.14  min\n",
            "EPOCH  45 batch  200 : Loss: 2.2529 Perplexity: 9.5151\n",
            "Avg 10 batchs takes  2.25  min\n",
            "EPOCH  45 batch  210 : Loss: 2.1968 Perplexity: 8.9961\n",
            "Avg 10 batchs takes  2.36  min\n",
            "EPOCH  45 batch  220 : Loss: 2.2452 Perplexity: 9.4423\n",
            "Avg 10 batchs takes  2.47  min\n",
            "EPOCH  45 batch  230 : Loss: 2.1918 Perplexity: 8.9514\n",
            "Avg 10 batchs takes  2.57  min\n",
            "EPOCH  45 batch  240 : Loss: 2.2264 Perplexity: 9.2661\n",
            "Avg 10 batchs takes  2.68  min\n",
            "EPOCH  45 batch  250 : Loss: 2.2112 Perplexity: 9.1265\n",
            "Avg 10 batchs takes  2.78  min\n",
            "EPOCH  45 batch  260 : Loss: 2.2141 Perplexity: 9.153\n",
            "Avg 10 batchs takes  2.89  min\n",
            "EPOCH  45 batch  270 : Loss: 2.2006 Perplexity: 9.0308\n",
            "Avg 10 batchs takes  2.99  min\n",
            "EPOCH  45 batch  280 : Loss: 2.2857 Perplexity: 9.8326\n",
            "Avg 10 batchs takes  3.1  min\n",
            "EPOCH  45 batch  290 : Loss: 2.2087 Perplexity: 9.1035\n",
            "Avg 10 batchs takes  3.2  min\n",
            "EPOCH  45 batch  300 : Loss: 2.256 Perplexity: 9.5448\n",
            " ********* Epoch  45  Average Levenshtein Distance is ****:  255.125\n",
            "Avg 10 batchs takes  3.31  min\n",
            "EPOCH  45 batch  310 : Loss: 2.283 Perplexity: 9.8059\n",
            "Avg 10 batchs takes  3.41  min\n",
            "EPOCH  45 batch  320 : Loss: 2.2642 Perplexity: 9.6231\n",
            "Avg 10 batchs takes  3.52  min\n",
            "EPOCH  45 batch  330 : Loss: 2.172 Perplexity: 8.776\n",
            "Avg 10 batchs takes  3.63  min\n",
            "EPOCH  45 batch  340 : Loss: 2.202 Perplexity: 9.0428\n",
            "Avg 10 batchs takes  3.73  min\n",
            "EPOCH  45 batch  350 : Loss: 2.1502 Perplexity: 8.5867\n",
            "Avg 10 batchs takes  3.84  min\n",
            "EPOCH  45 batch  360 : Loss: 2.2097 Perplexity: 9.113\n",
            "Avg 10 batchs takes  3.94  min\n",
            "EPOCH  45 batch  370 : Loss: 2.3058 Perplexity: 10.0321\n",
            "Avg 10 batchs takes  4.05  min\n",
            "EPOCH  45 batch  380 : Loss: 2.2154 Perplexity: 9.1647\n",
            "Avg 10 batchs takes  4.15  min\n",
            "EPOCH  45 batch  390 : Loss: 2.2424 Perplexity: 9.4155\n",
            "Avg 10 batchs takes  4.26  min\n",
            "EPOCH  45 batch  400 : Loss: 2.2358 Perplexity: 9.3543\n",
            "Avg 10 batchs takes  4.37  min\n",
            "EPOCH  45 batch  410 : Loss: 2.211 Perplexity: 9.1252\n",
            "Avg 10 batchs takes  4.47  min\n",
            "EPOCH  45 batch  420 : Loss: 2.3092 Perplexity: 10.0665\n",
            "Avg 10 batchs takes  4.58  min\n",
            "EPOCH  45 batch  430 : Loss: 2.3323 Perplexity: 10.3014\n",
            "Avg 10 batchs takes  4.69  min\n",
            "EPOCH  45 batch  440 : Loss: 2.1625 Perplexity: 8.693\n",
            "Avg 10 batchs takes  4.79  min\n",
            "EPOCH  45 batch  450 : Loss: 2.2483 Perplexity: 9.4713\n",
            "Avg 10 batchs takes  4.9  min\n",
            "EPOCH  45 batch  460 : Loss: 2.2546 Perplexity: 9.5311\n",
            "Avg 10 batchs takes  5.0  min\n",
            "EPOCH  45 batch  470 : Loss: 2.355 Perplexity: 10.5382\n",
            "Avg 10 batchs takes  5.1  min\n",
            "EPOCH  45 batch  480 : Loss: 2.2149 Perplexity: 9.1606\n",
            "Avg 10 batchs takes  5.21  min\n",
            "EPOCH  45 batch  490 : Loss: 2.2427 Perplexity: 9.4192\n",
            "Avg 10 batchs takes  5.32  min\n",
            "EPOCH  45 batch  500 : Loss: 2.1517 Perplexity: 8.5993\n",
            "Avg 10 batchs takes  5.43  min\n",
            "EPOCH  45 batch  510 : Loss: 2.2921 Perplexity: 9.8957\n",
            "Avg 10 batchs takes  5.54  min\n",
            "EPOCH  45 batch  520 : Loss: 2.2671 Perplexity: 9.651\n",
            "Avg 10 batchs takes  5.65  min\n",
            "EPOCH  45 batch  530 : Loss: 2.1818 Perplexity: 8.862\n",
            "Avg 10 batchs takes  5.76  min\n",
            "EPOCH  45 batch  540 : Loss: 2.2762 Perplexity: 9.7393\n",
            "Avg 10 batchs takes  5.87  min\n",
            "EPOCH  45 batch  550 : Loss: 2.2913 Perplexity: 9.8878\n",
            "Avg 10 batchs takes  5.98  min\n",
            "EPOCH  45 batch  560 : Loss: 2.1782 Perplexity: 8.8307\n",
            "Avg 10 batchs takes  6.08  min\n",
            "EPOCH  45 batch  570 : Loss: 2.2515 Perplexity: 9.5016\n",
            "Avg 10 batchs takes  6.19  min\n",
            "EPOCH  45 batch  580 : Loss: 2.2652 Perplexity: 9.6332\n",
            "Avg 10 batchs takes  6.3  min\n",
            "EPOCH  45 batch  590 : Loss: 2.2505 Perplexity: 9.4921\n",
            "Avg 10 batchs takes  6.4  min\n",
            "EPOCH  45 batch  600 : Loss: 2.3188 Perplexity: 10.1638\n",
            " ********* Epoch  45  Average Levenshtein Distance is ****:  287.21875\n",
            "Avg 10 batchs takes  6.52  min\n",
            "EPOCH  45 batch  610 : Loss: 2.1717 Perplexity: 8.7736\n",
            "Avg 10 batchs takes  6.62  min\n",
            "EPOCH  45 batch  620 : Loss: 2.1888 Perplexity: 8.9241\n",
            "Avg 10 batchs takes  6.73  min\n",
            "EPOCH  45 batch  630 : Loss: 2.2425 Perplexity: 9.4169\n",
            "Avg 10 batchs takes  6.83  min\n",
            "EPOCH  45 batch  640 : Loss: 2.2842 Perplexity: 9.8175\n",
            "Avg 10 batchs takes  6.93  min\n",
            "EPOCH  45 batch  650 : Loss: 2.2883 Perplexity: 9.8579\n",
            "Avg 10 batchs takes  7.04  min\n",
            "EPOCH  45 batch  660 : Loss: 2.2177 Perplexity: 9.1859\n",
            "Avg 10 batchs takes  7.15  min\n",
            "EPOCH  45 batch  670 : Loss: 2.2711 Perplexity: 9.69\n",
            "Avg 10 batchs takes  7.25  min\n",
            "EPOCH  45 batch  680 : Loss: 2.2251 Perplexity: 9.2545\n",
            "Avg 10 batchs takes  7.36  min\n",
            "EPOCH  45 batch  690 : Loss: 2.2948 Perplexity: 9.9221\n",
            "Avg 10 batchs takes  7.46  min\n",
            "EPOCH  45 batch  700 : Loss: 2.2368 Perplexity: 9.3636\n",
            "Avg 10 batchs takes  7.57  min\n",
            "EPOCH  45 batch  710 : Loss: 2.25 Perplexity: 9.488\n",
            "Avg 10 batchs takes  7.67  min\n",
            "EPOCH  45 batch  720 : Loss: 2.259 Perplexity: 9.5739\n",
            "Avg 10 batchs takes  7.78  min\n",
            "EPOCH  45 batch  730 : Loss: 2.2937 Perplexity: 9.912\n",
            "Avg 10 batchs takes  7.88  min\n",
            "EPOCH  45 batch  740 : Loss: 2.1774 Perplexity: 8.8231\n",
            "Avg 10 batchs takes  7.99  min\n",
            "EPOCH  45 batch  750 : Loss: 2.2525 Perplexity: 9.5111\n",
            "Avg 10 batchs takes  8.09  min\n",
            "EPOCH  45 batch  760 : Loss: 2.2724 Perplexity: 9.7022\n",
            "Avg 10 batchs takes  8.2  min\n",
            "EPOCH  45 batch  770 : Loss: 2.1251 Perplexity: 8.3741\n",
            "Training loss after one epoch is: 2.2338411808013916\n",
            "Time take for an epoch is: 8.22  min\n",
            " ********************* Epoch  45  ******************\n",
            "Test loss   45  is  3.9956908566611156\n",
            "Perplexity is:  54.36338496006266\n",
            "Avg distance is:  166.05982142857144\n",
            "Learning rate for epoch  46  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  46 batch  0 : Loss: 2.2445 Perplexity: 9.4353\n",
            " ********* Epoch  46  Average Levenshtein Distance is ****:  150.09375\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  46 batch  10 : Loss: 2.1985 Perplexity: 9.0112\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  46 batch  20 : Loss: 2.2906 Perplexity: 9.8807\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  46 batch  30 : Loss: 2.2024 Perplexity: 9.0471\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  46 batch  40 : Loss: 2.2417 Perplexity: 9.409\n",
            "Avg 10 batchs takes  0.55  min\n",
            "EPOCH  46 batch  50 : Loss: 2.237 Perplexity: 9.3652\n",
            "Avg 10 batchs takes  0.65  min\n",
            "EPOCH  46 batch  60 : Loss: 2.1749 Perplexity: 8.8011\n",
            "Avg 10 batchs takes  0.76  min\n",
            "EPOCH  46 batch  70 : Loss: 2.2303 Perplexity: 9.3023\n",
            "Avg 10 batchs takes  0.86  min\n",
            "EPOCH  46 batch  80 : Loss: 2.1504 Perplexity: 8.5885\n",
            "Avg 10 batchs takes  0.97  min\n",
            "EPOCH  46 batch  90 : Loss: 2.2383 Perplexity: 9.3775\n",
            "Avg 10 batchs takes  1.08  min\n",
            "EPOCH  46 batch  100 : Loss: 2.1916 Perplexity: 8.9499\n",
            "Avg 10 batchs takes  1.19  min\n",
            "EPOCH  46 batch  110 : Loss: 2.2873 Perplexity: 9.8485\n",
            "Avg 10 batchs takes  1.3  min\n",
            "EPOCH  46 batch  120 : Loss: 2.068 Perplexity: 7.9094\n",
            "Avg 10 batchs takes  1.4  min\n",
            "EPOCH  46 batch  130 : Loss: 2.241 Perplexity: 9.4027\n",
            "Avg 10 batchs takes  1.5  min\n",
            "EPOCH  46 batch  140 : Loss: 2.2192 Perplexity: 9.2001\n",
            "Avg 10 batchs takes  1.61  min\n",
            "EPOCH  46 batch  150 : Loss: 2.2306 Perplexity: 9.3057\n",
            "Avg 10 batchs takes  1.72  min\n",
            "EPOCH  46 batch  160 : Loss: 2.4702 Perplexity: 11.825\n",
            "Avg 10 batchs takes  1.82  min\n",
            "EPOCH  46 batch  170 : Loss: 2.186 Perplexity: 8.8994\n",
            "Avg 10 batchs takes  1.93  min\n",
            "EPOCH  46 batch  180 : Loss: 2.1179 Perplexity: 8.3137\n",
            "Avg 10 batchs takes  2.04  min\n",
            "EPOCH  46 batch  190 : Loss: 2.3549 Perplexity: 10.5369\n",
            "Avg 10 batchs takes  2.15  min\n",
            "EPOCH  46 batch  200 : Loss: 2.1261 Perplexity: 8.3824\n",
            "Avg 10 batchs takes  2.25  min\n",
            "EPOCH  46 batch  210 : Loss: 2.2848 Perplexity: 9.824\n",
            "Avg 10 batchs takes  2.36  min\n",
            "EPOCH  46 batch  220 : Loss: 2.269 Perplexity: 9.6695\n",
            "Avg 10 batchs takes  2.46  min\n",
            "EPOCH  46 batch  230 : Loss: 2.2016 Perplexity: 9.0391\n",
            "Avg 10 batchs takes  2.57  min\n",
            "EPOCH  46 batch  240 : Loss: 2.2676 Perplexity: 9.6567\n",
            "Avg 10 batchs takes  2.67  min\n",
            "EPOCH  46 batch  250 : Loss: 2.1836 Perplexity: 8.8784\n",
            "Avg 10 batchs takes  2.78  min\n",
            "EPOCH  46 batch  260 : Loss: 2.1631 Perplexity: 8.6983\n",
            "Avg 10 batchs takes  2.89  min\n",
            "EPOCH  46 batch  270 : Loss: 2.1958 Perplexity: 8.9873\n",
            "Avg 10 batchs takes  2.99  min\n",
            "EPOCH  46 batch  280 : Loss: 2.2051 Perplexity: 9.0708\n",
            "Avg 10 batchs takes  3.1  min\n",
            "EPOCH  46 batch  290 : Loss: 2.2789 Perplexity: 9.7655\n",
            "Avg 10 batchs takes  3.21  min\n",
            "EPOCH  46 batch  300 : Loss: 2.1834 Perplexity: 8.8768\n",
            " ********* Epoch  46  Average Levenshtein Distance is ****:  237.1875\n",
            "Avg 10 batchs takes  3.31  min\n",
            "EPOCH  46 batch  310 : Loss: 2.1027 Perplexity: 8.188\n",
            "Avg 10 batchs takes  3.42  min\n",
            "EPOCH  46 batch  320 : Loss: 2.249 Perplexity: 9.4778\n",
            "Avg 10 batchs takes  3.53  min\n",
            "EPOCH  46 batch  330 : Loss: 2.2474 Perplexity: 9.4626\n",
            "Avg 10 batchs takes  3.64  min\n",
            "EPOCH  46 batch  340 : Loss: 2.2405 Perplexity: 9.3982\n",
            "Avg 10 batchs takes  3.75  min\n",
            "EPOCH  46 batch  350 : Loss: 2.2145 Perplexity: 9.1564\n",
            "Avg 10 batchs takes  3.85  min\n",
            "EPOCH  46 batch  360 : Loss: 2.2206 Perplexity: 9.2131\n",
            "Avg 10 batchs takes  3.95  min\n",
            "EPOCH  46 batch  370 : Loss: 2.3252 Perplexity: 10.2283\n",
            "Avg 10 batchs takes  4.06  min\n",
            "EPOCH  46 batch  380 : Loss: 2.2519 Perplexity: 9.5055\n",
            "Avg 10 batchs takes  4.16  min\n",
            "EPOCH  46 batch  390 : Loss: 2.1982 Perplexity: 9.009\n",
            "Avg 10 batchs takes  4.27  min\n",
            "EPOCH  46 batch  400 : Loss: 2.2876 Perplexity: 9.851\n",
            "Avg 10 batchs takes  4.37  min\n",
            "EPOCH  46 batch  410 : Loss: 2.2824 Perplexity: 9.7998\n",
            "Avg 10 batchs takes  4.48  min\n",
            "EPOCH  46 batch  420 : Loss: 2.2674 Perplexity: 9.6546\n",
            "Avg 10 batchs takes  4.58  min\n",
            "EPOCH  46 batch  430 : Loss: 2.1752 Perplexity: 8.8035\n",
            "Avg 10 batchs takes  4.68  min\n",
            "EPOCH  46 batch  440 : Loss: 2.1964 Perplexity: 8.9922\n",
            "Avg 10 batchs takes  4.79  min\n",
            "EPOCH  46 batch  450 : Loss: 2.2742 Perplexity: 9.7201\n",
            "Avg 10 batchs takes  4.9  min\n",
            "EPOCH  46 batch  460 : Loss: 2.2656 Perplexity: 9.6372\n",
            "Avg 10 batchs takes  5.0  min\n",
            "EPOCH  46 batch  470 : Loss: 2.1416 Perplexity: 8.5127\n",
            "Avg 10 batchs takes  5.11  min\n",
            "EPOCH  46 batch  480 : Loss: 2.2123 Perplexity: 9.1369\n",
            "Avg 10 batchs takes  5.22  min\n",
            "EPOCH  46 batch  490 : Loss: 2.2349 Perplexity: 9.3451\n",
            "Avg 10 batchs takes  5.32  min\n",
            "EPOCH  46 batch  500 : Loss: 2.2739 Perplexity: 9.7172\n",
            "Avg 10 batchs takes  5.43  min\n",
            "EPOCH  46 batch  510 : Loss: 2.2335 Perplexity: 9.3328\n",
            "Avg 10 batchs takes  5.53  min\n",
            "EPOCH  46 batch  520 : Loss: 2.2211 Perplexity: 9.2177\n",
            "Avg 10 batchs takes  5.63  min\n",
            "EPOCH  46 batch  530 : Loss: 2.1658 Perplexity: 8.7213\n",
            "Avg 10 batchs takes  5.73  min\n",
            "EPOCH  46 batch  540 : Loss: 2.244 Perplexity: 9.4305\n",
            "Avg 10 batchs takes  5.84  min\n",
            "EPOCH  46 batch  550 : Loss: 2.2606 Perplexity: 9.5889\n",
            "Avg 10 batchs takes  5.95  min\n",
            "EPOCH  46 batch  560 : Loss: 2.2165 Perplexity: 9.1749\n",
            "Avg 10 batchs takes  6.06  min\n",
            "EPOCH  46 batch  570 : Loss: 2.3204 Perplexity: 10.1796\n",
            "Avg 10 batchs takes  6.17  min\n",
            "EPOCH  46 batch  580 : Loss: 2.209 Perplexity: 9.1065\n",
            "Avg 10 batchs takes  6.27  min\n",
            "EPOCH  46 batch  590 : Loss: 2.2232 Perplexity: 9.2373\n",
            "Avg 10 batchs takes  6.38  min\n",
            "EPOCH  46 batch  600 : Loss: 2.2021 Perplexity: 9.0443\n",
            " ********* Epoch  46  Average Levenshtein Distance is ****:  348.0\n",
            "Avg 10 batchs takes  6.49  min\n",
            "EPOCH  46 batch  610 : Loss: 2.1709 Perplexity: 8.7661\n",
            "Avg 10 batchs takes  6.59  min\n",
            "EPOCH  46 batch  620 : Loss: 2.2607 Perplexity: 9.5902\n",
            "Avg 10 batchs takes  6.7  min\n",
            "EPOCH  46 batch  630 : Loss: 2.1733 Perplexity: 8.7868\n",
            "Avg 10 batchs takes  6.81  min\n",
            "EPOCH  46 batch  640 : Loss: 2.2041 Perplexity: 9.0623\n",
            "Avg 10 batchs takes  6.91  min\n",
            "EPOCH  46 batch  650 : Loss: 2.2415 Perplexity: 9.4075\n",
            "Avg 10 batchs takes  7.02  min\n",
            "EPOCH  46 batch  660 : Loss: 2.2147 Perplexity: 9.1591\n",
            "Avg 10 batchs takes  7.12  min\n",
            "EPOCH  46 batch  670 : Loss: 2.2279 Perplexity: 9.2801\n",
            "Avg 10 batchs takes  7.23  min\n",
            "EPOCH  46 batch  680 : Loss: 2.2091 Perplexity: 9.1072\n",
            "Avg 10 batchs takes  7.33  min\n",
            "EPOCH  46 batch  690 : Loss: 2.2087 Perplexity: 9.104\n",
            "Avg 10 batchs takes  7.43  min\n",
            "EPOCH  46 batch  700 : Loss: 2.217 Perplexity: 9.1801\n",
            "Avg 10 batchs takes  7.54  min\n",
            "EPOCH  46 batch  710 : Loss: 2.2412 Perplexity: 9.4047\n",
            "Avg 10 batchs takes  7.64  min\n",
            "EPOCH  46 batch  720 : Loss: 2.2501 Perplexity: 9.4885\n",
            "Avg 10 batchs takes  7.75  min\n",
            "EPOCH  46 batch  730 : Loss: 2.1895 Perplexity: 8.9309\n",
            "Avg 10 batchs takes  7.86  min\n",
            "EPOCH  46 batch  740 : Loss: 2.1813 Perplexity: 8.8582\n",
            "Avg 10 batchs takes  7.97  min\n",
            "EPOCH  46 batch  750 : Loss: 2.2466 Perplexity: 9.4556\n",
            "Avg 10 batchs takes  8.08  min\n",
            "EPOCH  46 batch  760 : Loss: 2.2839 Perplexity: 9.8152\n",
            "Avg 10 batchs takes  8.18  min\n",
            "EPOCH  46 batch  770 : Loss: 2.2464 Perplexity: 9.4539\n",
            "Training loss after one epoch is: 2.2603161334991455\n",
            "Time take for an epoch is: 8.2  min\n",
            " ********************* Epoch  46  ******************\n",
            "Test loss   46  is  4.513857623508998\n",
            "Perplexity is:  91.27323804142158\n",
            "Avg distance is:  197.36378968253968\n",
            "Learning rate for epoch  47  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  47 batch  0 : Loss: 2.1377 Perplexity: 8.4798\n",
            " ********* Epoch  47  Average Levenshtein Distance is ****:  149.9375\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  47 batch  10 : Loss: 2.3446 Perplexity: 10.429\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  47 batch  20 : Loss: 2.1355 Perplexity: 8.461\n",
            "Avg 10 batchs takes  0.33  min\n",
            "EPOCH  47 batch  30 : Loss: 2.2852 Perplexity: 9.828\n",
            "Avg 10 batchs takes  0.44  min\n",
            "EPOCH  47 batch  40 : Loss: 2.2389 Perplexity: 9.3829\n",
            "Avg 10 batchs takes  0.55  min\n",
            "EPOCH  47 batch  50 : Loss: 2.2486 Perplexity: 9.4749\n",
            "Avg 10 batchs takes  0.65  min\n",
            "EPOCH  47 batch  60 : Loss: 2.0855 Perplexity: 8.0483\n",
            "Avg 10 batchs takes  0.76  min\n",
            "EPOCH  47 batch  70 : Loss: 2.1502 Perplexity: 8.5866\n",
            "Avg 10 batchs takes  0.87  min\n",
            "EPOCH  47 batch  80 : Loss: 2.1396 Perplexity: 8.4962\n",
            "Avg 10 batchs takes  0.98  min\n",
            "EPOCH  47 batch  90 : Loss: 2.4589 Perplexity: 11.6914\n",
            "Avg 10 batchs takes  1.08  min\n",
            "EPOCH  47 batch  100 : Loss: 2.212 Perplexity: 9.1342\n",
            "Avg 10 batchs takes  1.19  min\n",
            "EPOCH  47 batch  110 : Loss: 2.0671 Perplexity: 7.9023\n",
            "Avg 10 batchs takes  1.3  min\n",
            "EPOCH  47 batch  120 : Loss: 2.2593 Perplexity: 9.5763\n",
            "Avg 10 batchs takes  1.4  min\n",
            "EPOCH  47 batch  130 : Loss: 2.2552 Perplexity: 9.5374\n",
            "Avg 10 batchs takes  1.51  min\n",
            "EPOCH  47 batch  140 : Loss: 2.1475 Perplexity: 8.5634\n",
            "Avg 10 batchs takes  1.62  min\n",
            "EPOCH  47 batch  150 : Loss: 2.1536 Perplexity: 8.6154\n",
            "Avg 10 batchs takes  1.73  min\n",
            "EPOCH  47 batch  160 : Loss: 2.2153 Perplexity: 9.164\n",
            "Avg 10 batchs takes  1.83  min\n",
            "EPOCH  47 batch  170 : Loss: 2.3027 Perplexity: 10.0013\n",
            "Avg 10 batchs takes  1.93  min\n",
            "EPOCH  47 batch  180 : Loss: 2.1498 Perplexity: 8.583\n",
            "Avg 10 batchs takes  2.03  min\n",
            "EPOCH  47 batch  190 : Loss: 2.2574 Perplexity: 9.558\n",
            "Avg 10 batchs takes  2.14  min\n",
            "EPOCH  47 batch  200 : Loss: 2.2331 Perplexity: 9.329\n",
            "Avg 10 batchs takes  2.24  min\n",
            "EPOCH  47 batch  210 : Loss: 2.2658 Perplexity: 9.6384\n",
            "Avg 10 batchs takes  2.35  min\n",
            "EPOCH  47 batch  220 : Loss: 2.1851 Perplexity: 8.8913\n",
            "Avg 10 batchs takes  2.45  min\n",
            "EPOCH  47 batch  230 : Loss: 2.3371 Perplexity: 10.3508\n",
            "Avg 10 batchs takes  2.56  min\n",
            "EPOCH  47 batch  240 : Loss: 2.1784 Perplexity: 8.832\n",
            "Avg 10 batchs takes  2.66  min\n",
            "EPOCH  47 batch  250 : Loss: 2.2986 Perplexity: 9.9604\n",
            "Avg 10 batchs takes  2.77  min\n",
            "EPOCH  47 batch  260 : Loss: 2.211 Perplexity: 9.1249\n",
            "Avg 10 batchs takes  2.87  min\n",
            "EPOCH  47 batch  270 : Loss: 2.2627 Perplexity: 9.6094\n",
            "Avg 10 batchs takes  2.97  min\n",
            "EPOCH  47 batch  280 : Loss: 2.1376 Perplexity: 8.4792\n",
            "Avg 10 batchs takes  3.08  min\n",
            "EPOCH  47 batch  290 : Loss: 2.2434 Perplexity: 9.4252\n",
            "Avg 10 batchs takes  3.18  min\n",
            "EPOCH  47 batch  300 : Loss: 2.165 Perplexity: 8.7144\n",
            " ********* Epoch  47  Average Levenshtein Distance is ****:  178.59375\n",
            "Avg 10 batchs takes  3.29  min\n",
            "EPOCH  47 batch  310 : Loss: 2.1734 Perplexity: 8.7881\n",
            "Avg 10 batchs takes  3.39  min\n",
            "EPOCH  47 batch  320 : Loss: 2.3058 Perplexity: 10.0321\n",
            "Avg 10 batchs takes  3.5  min\n",
            "EPOCH  47 batch  330 : Loss: 2.1931 Perplexity: 8.9632\n",
            "Avg 10 batchs takes  3.6  min\n",
            "EPOCH  47 batch  340 : Loss: 2.2779 Perplexity: 9.7559\n",
            "Avg 10 batchs takes  3.71  min\n",
            "EPOCH  47 batch  350 : Loss: 2.2683 Perplexity: 9.6634\n",
            "Avg 10 batchs takes  3.81  min\n",
            "EPOCH  47 batch  360 : Loss: 2.281 Perplexity: 9.7861\n",
            "Avg 10 batchs takes  3.92  min\n",
            "EPOCH  47 batch  370 : Loss: 2.2763 Perplexity: 9.7405\n",
            "Avg 10 batchs takes  4.03  min\n",
            "EPOCH  47 batch  380 : Loss: 2.1496 Perplexity: 8.5812\n",
            "Avg 10 batchs takes  4.13  min\n",
            "EPOCH  47 batch  390 : Loss: 2.2657 Perplexity: 9.6374\n",
            "Avg 10 batchs takes  4.24  min\n",
            "EPOCH  47 batch  400 : Loss: 2.1774 Perplexity: 8.823\n",
            "Avg 10 batchs takes  4.34  min\n",
            "EPOCH  47 batch  410 : Loss: 2.2302 Perplexity: 9.3019\n",
            "Avg 10 batchs takes  4.46  min\n",
            "EPOCH  47 batch  420 : Loss: 2.2141 Perplexity: 9.1533\n",
            "Avg 10 batchs takes  4.56  min\n",
            "EPOCH  47 batch  430 : Loss: 2.2136 Perplexity: 9.1488\n",
            "Avg 10 batchs takes  4.66  min\n",
            "EPOCH  47 batch  440 : Loss: 2.2668 Perplexity: 9.6481\n",
            "Avg 10 batchs takes  4.77  min\n",
            "EPOCH  47 batch  450 : Loss: 2.202 Perplexity: 9.0429\n",
            "Avg 10 batchs takes  4.87  min\n",
            "EPOCH  47 batch  460 : Loss: 2.2069 Perplexity: 9.088\n",
            "Avg 10 batchs takes  4.98  min\n",
            "EPOCH  47 batch  470 : Loss: 2.281 Perplexity: 9.7866\n",
            "Avg 10 batchs takes  5.09  min\n",
            "EPOCH  47 batch  480 : Loss: 2.1434 Perplexity: 8.5282\n",
            "Avg 10 batchs takes  5.19  min\n",
            "EPOCH  47 batch  490 : Loss: 2.3 Perplexity: 9.9745\n",
            "Avg 10 batchs takes  5.3  min\n",
            "EPOCH  47 batch  500 : Loss: 2.1719 Perplexity: 8.7752\n",
            "Avg 10 batchs takes  5.4  min\n",
            "EPOCH  47 batch  510 : Loss: 2.3252 Perplexity: 10.2285\n",
            "Avg 10 batchs takes  5.51  min\n",
            "EPOCH  47 batch  520 : Loss: 2.2751 Perplexity: 9.7293\n",
            "Avg 10 batchs takes  5.61  min\n",
            "EPOCH  47 batch  530 : Loss: 2.2169 Perplexity: 9.1787\n",
            "Avg 10 batchs takes  5.72  min\n",
            "EPOCH  47 batch  540 : Loss: 2.2271 Perplexity: 9.273\n",
            "Avg 10 batchs takes  5.83  min\n",
            "EPOCH  47 batch  550 : Loss: 2.2219 Perplexity: 9.2244\n",
            "Avg 10 batchs takes  5.93  min\n",
            "EPOCH  47 batch  560 : Loss: 2.2159 Perplexity: 9.1695\n",
            "Avg 10 batchs takes  6.04  min\n",
            "EPOCH  47 batch  570 : Loss: 2.3248 Perplexity: 10.2242\n",
            "Avg 10 batchs takes  6.14  min\n",
            "EPOCH  47 batch  580 : Loss: 2.3564 Perplexity: 10.5534\n",
            "Avg 10 batchs takes  6.24  min\n",
            "EPOCH  47 batch  590 : Loss: 2.2679 Perplexity: 9.659\n",
            "Avg 10 batchs takes  6.35  min\n",
            "EPOCH  47 batch  600 : Loss: 2.24 Perplexity: 9.3938\n",
            " ********* Epoch  47  Average Levenshtein Distance is ****:  300.75\n",
            "Avg 10 batchs takes  6.46  min\n",
            "EPOCH  47 batch  610 : Loss: 2.2415 Perplexity: 9.4072\n",
            "Avg 10 batchs takes  6.57  min\n",
            "EPOCH  47 batch  620 : Loss: 2.3175 Perplexity: 10.1505\n",
            "Avg 10 batchs takes  6.67  min\n",
            "EPOCH  47 batch  630 : Loss: 2.2916 Perplexity: 9.8906\n",
            "Avg 10 batchs takes  6.78  min\n",
            "EPOCH  47 batch  640 : Loss: 2.0979 Perplexity: 8.1488\n",
            "Avg 10 batchs takes  6.88  min\n",
            "EPOCH  47 batch  650 : Loss: 2.2262 Perplexity: 9.2643\n",
            "Avg 10 batchs takes  6.98  min\n",
            "EPOCH  47 batch  660 : Loss: 2.2217 Perplexity: 9.2227\n",
            "Avg 10 batchs takes  7.08  min\n",
            "EPOCH  47 batch  670 : Loss: 2.2915 Perplexity: 9.8897\n",
            "Avg 10 batchs takes  7.19  min\n",
            "EPOCH  47 batch  680 : Loss: 2.1955 Perplexity: 8.9843\n",
            "Avg 10 batchs takes  7.29  min\n",
            "EPOCH  47 batch  690 : Loss: 2.1697 Perplexity: 8.7559\n",
            "Avg 10 batchs takes  7.4  min\n",
            "EPOCH  47 batch  700 : Loss: 2.191 Perplexity: 8.9438\n",
            "Avg 10 batchs takes  7.51  min\n",
            "EPOCH  47 batch  710 : Loss: 2.1716 Perplexity: 8.7722\n",
            "Avg 10 batchs takes  7.61  min\n",
            "EPOCH  47 batch  720 : Loss: 2.2426 Perplexity: 9.418\n",
            "Avg 10 batchs takes  7.72  min\n",
            "EPOCH  47 batch  730 : Loss: 2.1495 Perplexity: 8.5805\n",
            "Avg 10 batchs takes  7.83  min\n",
            "EPOCH  47 batch  740 : Loss: 2.1061 Perplexity: 8.2161\n",
            "Avg 10 batchs takes  7.93  min\n",
            "EPOCH  47 batch  750 : Loss: 2.2806 Perplexity: 9.783\n",
            "Avg 10 batchs takes  8.03  min\n",
            "EPOCH  47 batch  760 : Loss: 2.1626 Perplexity: 8.6934\n",
            "Avg 10 batchs takes  8.15  min\n",
            "EPOCH  47 batch  770 : Loss: 2.2274 Perplexity: 9.2756\n",
            "Training loss after one epoch is: 2.295117139816284\n",
            "Time take for an epoch is: 8.17  min\n",
            " ********************* Epoch  47  ******************\n",
            "Test loss   47  is  3.8342467308044434\n",
            "Perplexity is:  46.25856938541492\n",
            "Avg distance is:  173.51478174603176\n",
            "Learning rate for epoch  48  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  48 batch  0 : Loss: 2.2727 Perplexity: 9.7052\n",
            " ********* Epoch  48  Average Levenshtein Distance is ****:  206.03125\n",
            "Avg 10 batchs takes  0.12  min\n",
            "EPOCH  48 batch  10 : Loss: 2.204 Perplexity: 9.061\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  48 batch  20 : Loss: 2.1866 Perplexity: 8.9048\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  48 batch  30 : Loss: 2.2332 Perplexity: 9.3293\n",
            "Avg 10 batchs takes  0.45  min\n",
            "EPOCH  48 batch  40 : Loss: 2.2451 Perplexity: 9.4411\n",
            "Avg 10 batchs takes  0.55  min\n",
            "EPOCH  48 batch  50 : Loss: 2.3635 Perplexity: 10.6276\n",
            "Avg 10 batchs takes  0.66  min\n",
            "EPOCH  48 batch  60 : Loss: 2.22 Perplexity: 9.2076\n",
            "Avg 10 batchs takes  0.77  min\n",
            "EPOCH  48 batch  70 : Loss: 2.2016 Perplexity: 9.0393\n",
            "Avg 10 batchs takes  0.87  min\n",
            "EPOCH  48 batch  80 : Loss: 2.1768 Perplexity: 8.8177\n",
            "Avg 10 batchs takes  0.98  min\n",
            "EPOCH  48 batch  90 : Loss: 2.2153 Perplexity: 9.1646\n",
            "Avg 10 batchs takes  1.09  min\n",
            "EPOCH  48 batch  100 : Loss: 2.0972 Perplexity: 8.1435\n",
            "Avg 10 batchs takes  1.2  min\n",
            "EPOCH  48 batch  110 : Loss: 2.2125 Perplexity: 9.1388\n",
            "Avg 10 batchs takes  1.31  min\n",
            "EPOCH  48 batch  120 : Loss: 2.2403 Perplexity: 9.3965\n",
            "Avg 10 batchs takes  1.42  min\n",
            "EPOCH  48 batch  130 : Loss: 2.156 Perplexity: 8.6362\n",
            "Avg 10 batchs takes  1.52  min\n",
            "EPOCH  48 batch  140 : Loss: 2.2203 Perplexity: 9.2101\n",
            "Avg 10 batchs takes  1.63  min\n",
            "EPOCH  48 batch  150 : Loss: 2.2029 Perplexity: 9.0515\n",
            "Avg 10 batchs takes  1.74  min\n",
            "EPOCH  48 batch  160 : Loss: 2.3275 Perplexity: 10.2518\n",
            "Avg 10 batchs takes  1.85  min\n",
            "EPOCH  48 batch  170 : Loss: 2.2288 Perplexity: 9.2886\n",
            "Avg 10 batchs takes  1.95  min\n",
            "EPOCH  48 batch  180 : Loss: 2.2317 Perplexity: 9.3156\n",
            "Avg 10 batchs takes  2.06  min\n",
            "EPOCH  48 batch  190 : Loss: 2.2009 Perplexity: 9.033\n",
            "Avg 10 batchs takes  2.16  min\n",
            "EPOCH  48 batch  200 : Loss: 2.205 Perplexity: 9.0706\n",
            "Avg 10 batchs takes  2.27  min\n",
            "EPOCH  48 batch  210 : Loss: 2.2224 Perplexity: 9.2295\n",
            "Avg 10 batchs takes  2.38  min\n",
            "EPOCH  48 batch  220 : Loss: 2.2082 Perplexity: 9.0993\n",
            "Avg 10 batchs takes  2.49  min\n",
            "EPOCH  48 batch  230 : Loss: 2.3319 Perplexity: 10.2973\n",
            "Avg 10 batchs takes  2.6  min\n",
            "EPOCH  48 batch  240 : Loss: 2.1961 Perplexity: 8.9898\n",
            "Avg 10 batchs takes  2.7  min\n",
            "EPOCH  48 batch  250 : Loss: 2.2228 Perplexity: 9.2329\n",
            "Avg 10 batchs takes  2.81  min\n",
            "EPOCH  48 batch  260 : Loss: 2.19 Perplexity: 8.9348\n",
            "Avg 10 batchs takes  2.92  min\n",
            "EPOCH  48 batch  270 : Loss: 2.2147 Perplexity: 9.1588\n",
            "Avg 10 batchs takes  3.03  min\n",
            "EPOCH  48 batch  280 : Loss: 2.1811 Perplexity: 8.856\n",
            "Avg 10 batchs takes  3.13  min\n",
            "EPOCH  48 batch  290 : Loss: 2.1403 Perplexity: 8.5019\n",
            "Avg 10 batchs takes  3.24  min\n",
            "EPOCH  48 batch  300 : Loss: 2.2052 Perplexity: 9.072\n",
            " ********* Epoch  48  Average Levenshtein Distance is ****:  338.5625\n",
            "Avg 10 batchs takes  3.35  min\n",
            "EPOCH  48 batch  310 : Loss: 2.2323 Perplexity: 9.3209\n",
            "Avg 10 batchs takes  3.46  min\n",
            "EPOCH  48 batch  320 : Loss: 2.1098 Perplexity: 8.247\n",
            "Avg 10 batchs takes  3.56  min\n",
            "EPOCH  48 batch  330 : Loss: 2.2072 Perplexity: 9.0903\n",
            "Avg 10 batchs takes  3.67  min\n",
            "EPOCH  48 batch  340 : Loss: 2.249 Perplexity: 9.4781\n",
            "Avg 10 batchs takes  3.77  min\n",
            "EPOCH  48 batch  350 : Loss: 2.2167 Perplexity: 9.1767\n",
            "Avg 10 batchs takes  3.88  min\n",
            "EPOCH  48 batch  360 : Loss: 2.249 Perplexity: 9.4778\n",
            "Avg 10 batchs takes  3.99  min\n",
            "EPOCH  48 batch  370 : Loss: 2.224 Perplexity: 9.2443\n",
            "Avg 10 batchs takes  4.1  min\n",
            "EPOCH  48 batch  380 : Loss: 2.1749 Perplexity: 8.8014\n",
            "Avg 10 batchs takes  4.21  min\n",
            "EPOCH  48 batch  390 : Loss: 2.2515 Perplexity: 9.5018\n",
            "Avg 10 batchs takes  4.31  min\n",
            "EPOCH  48 batch  400 : Loss: 2.2365 Perplexity: 9.3606\n",
            "Avg 10 batchs takes  4.42  min\n",
            "EPOCH  48 batch  410 : Loss: 2.2443 Perplexity: 9.4342\n",
            "Avg 10 batchs takes  4.53  min\n",
            "EPOCH  48 batch  420 : Loss: 2.2766 Perplexity: 9.7436\n",
            "Avg 10 batchs takes  4.64  min\n",
            "EPOCH  48 batch  430 : Loss: 2.2541 Perplexity: 9.5265\n",
            "Avg 10 batchs takes  4.75  min\n",
            "EPOCH  48 batch  440 : Loss: 2.161 Perplexity: 8.6798\n",
            "Avg 10 batchs takes  4.86  min\n",
            "EPOCH  48 batch  450 : Loss: 2.1714 Perplexity: 8.7701\n",
            "Avg 10 batchs takes  4.96  min\n",
            "EPOCH  48 batch  460 : Loss: 2.1625 Perplexity: 8.6932\n",
            "Avg 10 batchs takes  5.07  min\n",
            "EPOCH  48 batch  470 : Loss: 2.292 Perplexity: 9.8944\n",
            "Avg 10 batchs takes  5.18  min\n",
            "EPOCH  48 batch  480 : Loss: 2.2461 Perplexity: 9.4505\n",
            "Avg 10 batchs takes  5.28  min\n",
            "EPOCH  48 batch  490 : Loss: 2.2626 Perplexity: 9.6083\n",
            "Avg 10 batchs takes  5.39  min\n",
            "EPOCH  48 batch  500 : Loss: 2.191 Perplexity: 8.9439\n",
            "Avg 10 batchs takes  5.5  min\n",
            "EPOCH  48 batch  510 : Loss: 2.2308 Perplexity: 9.3072\n",
            "Avg 10 batchs takes  5.6  min\n",
            "EPOCH  48 batch  520 : Loss: 2.2112 Perplexity: 9.127\n",
            "Avg 10 batchs takes  5.71  min\n",
            "EPOCH  48 batch  530 : Loss: 2.2104 Perplexity: 9.1192\n",
            "Avg 10 batchs takes  5.82  min\n",
            "EPOCH  48 batch  540 : Loss: 2.1323 Perplexity: 8.4342\n",
            "Avg 10 batchs takes  5.92  min\n",
            "EPOCH  48 batch  550 : Loss: 2.1738 Perplexity: 8.7914\n",
            "Avg 10 batchs takes  6.04  min\n",
            "EPOCH  48 batch  560 : Loss: 2.1496 Perplexity: 8.5815\n",
            "Avg 10 batchs takes  6.14  min\n",
            "EPOCH  48 batch  570 : Loss: 2.1613 Perplexity: 8.6823\n",
            "Avg 10 batchs takes  6.25  min\n",
            "EPOCH  48 batch  580 : Loss: 2.1968 Perplexity: 8.9964\n",
            "Avg 10 batchs takes  6.36  min\n",
            "EPOCH  48 batch  590 : Loss: 2.2194 Perplexity: 9.2017\n",
            "Avg 10 batchs takes  6.48  min\n",
            "EPOCH  48 batch  600 : Loss: 2.1772 Perplexity: 8.8218\n",
            " ********* Epoch  48  Average Levenshtein Distance is ****:  241.6875\n",
            "Avg 10 batchs takes  6.59  min\n",
            "EPOCH  48 batch  610 : Loss: 2.3621 Perplexity: 10.613\n",
            "Avg 10 batchs takes  6.71  min\n",
            "EPOCH  48 batch  620 : Loss: 2.2135 Perplexity: 9.1475\n",
            "Avg 10 batchs takes  6.82  min\n",
            "EPOCH  48 batch  630 : Loss: 2.2722 Perplexity: 9.7009\n",
            "Avg 10 batchs takes  6.93  min\n",
            "EPOCH  48 batch  640 : Loss: 2.184 Perplexity: 8.8814\n",
            "Avg 10 batchs takes  7.04  min\n",
            "EPOCH  48 batch  650 : Loss: 2.3342 Perplexity: 10.321\n",
            "Avg 10 batchs takes  7.15  min\n",
            "EPOCH  48 batch  660 : Loss: 2.2759 Perplexity: 9.7364\n",
            "Avg 10 batchs takes  7.26  min\n",
            "EPOCH  48 batch  670 : Loss: 2.1986 Perplexity: 9.0122\n",
            "Avg 10 batchs takes  7.37  min\n",
            "EPOCH  48 batch  680 : Loss: 2.2272 Perplexity: 9.2737\n",
            "Avg 10 batchs takes  7.48  min\n",
            "EPOCH  48 batch  690 : Loss: 2.221 Perplexity: 9.2163\n",
            "Avg 10 batchs takes  7.58  min\n",
            "EPOCH  48 batch  700 : Loss: 2.1931 Perplexity: 8.9629\n",
            "Avg 10 batchs takes  7.68  min\n",
            "EPOCH  48 batch  710 : Loss: 2.1463 Perplexity: 8.5533\n",
            "Avg 10 batchs takes  7.79  min\n",
            "EPOCH  48 batch  720 : Loss: 2.1598 Perplexity: 8.6693\n",
            "Avg 10 batchs takes  7.9  min\n",
            "EPOCH  48 batch  730 : Loss: 2.1948 Perplexity: 8.9778\n",
            "Avg 10 batchs takes  8.01  min\n",
            "EPOCH  48 batch  740 : Loss: 2.2868 Perplexity: 9.8437\n",
            "Avg 10 batchs takes  8.12  min\n",
            "EPOCH  48 batch  750 : Loss: 2.2346 Perplexity: 9.3425\n",
            "Avg 10 batchs takes  8.22  min\n",
            "EPOCH  48 batch  760 : Loss: 2.2505 Perplexity: 9.4926\n",
            "Avg 10 batchs takes  8.34  min\n",
            "EPOCH  48 batch  770 : Loss: 2.2381 Perplexity: 9.3754\n",
            "Training loss after one epoch is: 2.1244912147521973\n",
            "Time take for an epoch is: 8.36  min\n",
            " ********************* Epoch  48  ******************\n",
            "Test loss   48  is  3.9333404404776435\n",
            "Perplexity is:  51.07731373836041\n",
            "Avg distance is:  148.4308531746032\n",
            "Learning rate for epoch  49  is 0.0003\n",
            "Avg 10 batchs takes  0.01  min\n",
            "EPOCH  49 batch  0 : Loss: 2.1754 Perplexity: 8.8055\n",
            " ********* Epoch  49  Average Levenshtein Distance is ****:  290.65625\n",
            "Avg 10 batchs takes  0.13  min\n",
            "EPOCH  49 batch  10 : Loss: 2.0767 Perplexity: 7.9777\n",
            "Avg 10 batchs takes  0.23  min\n",
            "EPOCH  49 batch  20 : Loss: 2.1448 Perplexity: 8.5403\n",
            "Avg 10 batchs takes  0.34  min\n",
            "EPOCH  49 batch  30 : Loss: 2.1618 Perplexity: 8.6872\n",
            "Avg 10 batchs takes  0.45  min\n",
            "EPOCH  49 batch  40 : Loss: 2.1654 Perplexity: 8.7182\n",
            "Avg 10 batchs takes  0.56  min\n",
            "EPOCH  49 batch  50 : Loss: 2.209 Perplexity: 9.1064\n",
            "Avg 10 batchs takes  0.68  min\n",
            "EPOCH  49 batch  60 : Loss: 2.2066 Perplexity: 9.0848\n",
            "Avg 10 batchs takes  0.79  min\n",
            "EPOCH  49 batch  70 : Loss: 2.2476 Perplexity: 9.4651\n",
            "Avg 10 batchs takes  0.89  min\n",
            "EPOCH  49 batch  80 : Loss: 2.2062 Perplexity: 9.0813\n",
            "Avg 10 batchs takes  1.0  min\n",
            "EPOCH  49 batch  90 : Loss: 2.1587 Perplexity: 8.6602\n",
            "Avg 10 batchs takes  1.11  min\n",
            "EPOCH  49 batch  100 : Loss: 2.111 Perplexity: 8.2562\n",
            "Avg 10 batchs takes  1.22  min\n",
            "EPOCH  49 batch  110 : Loss: 2.17 Perplexity: 8.7587\n",
            "Avg 10 batchs takes  1.33  min\n",
            "EPOCH  49 batch  120 : Loss: 2.1806 Perplexity: 8.8512\n",
            "Avg 10 batchs takes  1.43  min\n",
            "EPOCH  49 batch  130 : Loss: 2.1248 Perplexity: 8.3715\n",
            "Avg 10 batchs takes  1.54  min\n",
            "EPOCH  49 batch  140 : Loss: 2.2591 Perplexity: 9.574\n",
            "Avg 10 batchs takes  1.65  min\n",
            "EPOCH  49 batch  150 : Loss: 2.1803 Perplexity: 8.8487\n",
            "Avg 10 batchs takes  1.76  min\n",
            "EPOCH  49 batch  160 : Loss: 2.2396 Perplexity: 9.3898\n",
            "Avg 10 batchs takes  1.87  min\n",
            "EPOCH  49 batch  170 : Loss: 2.117 Perplexity: 8.3059\n",
            "Avg 10 batchs takes  1.98  min\n",
            "EPOCH  49 batch  180 : Loss: 2.3572 Perplexity: 10.561\n",
            "Avg 10 batchs takes  2.09  min\n",
            "EPOCH  49 batch  190 : Loss: 2.1286 Perplexity: 8.4029\n",
            "Avg 10 batchs takes  2.2  min\n",
            "EPOCH  49 batch  200 : Loss: 2.2949 Perplexity: 9.9233\n",
            "Avg 10 batchs takes  2.3  min\n",
            "EPOCH  49 batch  210 : Loss: 2.1709 Perplexity: 8.7658\n",
            "Avg 10 batchs takes  2.41  min\n",
            "EPOCH  49 batch  220 : Loss: 2.2387 Perplexity: 9.3814\n",
            "Avg 10 batchs takes  2.52  min\n",
            "EPOCH  49 batch  230 : Loss: 2.222 Perplexity: 9.2255\n",
            "Avg 10 batchs takes  2.63  min\n",
            "EPOCH  49 batch  240 : Loss: 2.3429 Perplexity: 10.4114\n",
            "Avg 10 batchs takes  2.74  min\n",
            "EPOCH  49 batch  250 : Loss: 2.2224 Perplexity: 9.2298\n",
            "Avg 10 batchs takes  2.85  min\n",
            "EPOCH  49 batch  260 : Loss: 2.1783 Perplexity: 8.831\n",
            "Avg 10 batchs takes  2.96  min\n",
            "EPOCH  49 batch  270 : Loss: 2.18 Perplexity: 8.8466\n",
            "Avg 10 batchs takes  3.07  min\n",
            "EPOCH  49 batch  280 : Loss: 2.2765 Perplexity: 9.7423\n",
            "Avg 10 batchs takes  3.18  min\n",
            "EPOCH  49 batch  290 : Loss: 2.2844 Perplexity: 9.8196\n",
            "Avg 10 batchs takes  3.29  min\n",
            "EPOCH  49 batch  300 : Loss: 2.2361 Perplexity: 9.3567\n",
            " ********* Epoch  49  Average Levenshtein Distance is ****:  320.84375\n",
            "Avg 10 batchs takes  3.4  min\n",
            "EPOCH  49 batch  310 : Loss: 2.2238 Perplexity: 9.242\n",
            "Avg 10 batchs takes  3.51  min\n",
            "EPOCH  49 batch  320 : Loss: 2.1959 Perplexity: 8.9884\n",
            "Avg 10 batchs takes  3.61  min\n",
            "EPOCH  49 batch  330 : Loss: 2.2586 Perplexity: 9.5694\n",
            "Avg 10 batchs takes  3.72  min\n",
            "EPOCH  49 batch  340 : Loss: 2.2383 Perplexity: 9.3778\n",
            "Avg 10 batchs takes  3.83  min\n",
            "EPOCH  49 batch  350 : Loss: 2.1649 Perplexity: 8.7135\n",
            "Avg 10 batchs takes  3.94  min\n",
            "EPOCH  49 batch  360 : Loss: 2.2388 Perplexity: 9.3819\n",
            "Avg 10 batchs takes  4.05  min\n",
            "EPOCH  49 batch  370 : Loss: 2.1325 Perplexity: 8.436\n",
            "Avg 10 batchs takes  4.16  min\n",
            "EPOCH  49 batch  380 : Loss: 2.3352 Perplexity: 10.3312\n",
            "Avg 10 batchs takes  4.27  min\n",
            "EPOCH  49 batch  390 : Loss: 2.2576 Perplexity: 9.56\n",
            "Avg 10 batchs takes  4.38  min\n",
            "EPOCH  49 batch  400 : Loss: 2.2579 Perplexity: 9.5626\n",
            "Avg 10 batchs takes  4.48  min\n",
            "EPOCH  49 batch  410 : Loss: 2.323 Perplexity: 10.2063\n",
            "Avg 10 batchs takes  4.6  min\n",
            "EPOCH  49 batch  420 : Loss: 2.1734 Perplexity: 8.7877\n",
            "Avg 10 batchs takes  4.7  min\n",
            "EPOCH  49 batch  430 : Loss: 2.2846 Perplexity: 9.8222\n",
            "Avg 10 batchs takes  4.81  min\n",
            "EPOCH  49 batch  440 : Loss: 2.2696 Perplexity: 9.6756\n",
            "Avg 10 batchs takes  4.92  min\n",
            "EPOCH  49 batch  450 : Loss: 2.2071 Perplexity: 9.0896\n",
            "Avg 10 batchs takes  5.02  min\n",
            "EPOCH  49 batch  460 : Loss: 2.2985 Perplexity: 9.9593\n",
            "Avg 10 batchs takes  5.13  min\n",
            "EPOCH  49 batch  470 : Loss: 2.1775 Perplexity: 8.8246\n",
            "Avg 10 batchs takes  5.24  min\n",
            "EPOCH  49 batch  480 : Loss: 2.1884 Perplexity: 8.9208\n",
            "Avg 10 batchs takes  5.35  min\n",
            "EPOCH  49 batch  490 : Loss: 2.1432 Perplexity: 8.5265\n",
            "Avg 10 batchs takes  5.46  min\n",
            "EPOCH  49 batch  500 : Loss: 2.2209 Perplexity: 9.2158\n",
            "Avg 10 batchs takes  5.58  min\n",
            "EPOCH  49 batch  510 : Loss: 2.23 Perplexity: 9.2999\n",
            "Avg 10 batchs takes  5.68  min\n",
            "EPOCH  49 batch  520 : Loss: 2.1057 Perplexity: 8.2128\n",
            "Avg 10 batchs takes  5.79  min\n",
            "EPOCH  49 batch  530 : Loss: 2.1822 Perplexity: 8.8655\n",
            "Avg 10 batchs takes  5.9  min\n",
            "EPOCH  49 batch  540 : Loss: 2.1507 Perplexity: 8.5908\n",
            "Avg 10 batchs takes  6.0  min\n",
            "EPOCH  49 batch  550 : Loss: 2.1881 Perplexity: 8.9185\n",
            "Avg 10 batchs takes  6.11  min\n",
            "EPOCH  49 batch  560 : Loss: 2.2457 Perplexity: 9.4474\n",
            "Avg 10 batchs takes  6.22  min\n",
            "EPOCH  49 batch  570 : Loss: 2.1608 Perplexity: 8.678\n",
            "Avg 10 batchs takes  6.34  min\n",
            "EPOCH  49 batch  580 : Loss: 2.1288 Perplexity: 8.4049\n",
            "Avg 10 batchs takes  6.44  min\n",
            "EPOCH  49 batch  590 : Loss: 2.3063 Perplexity: 10.0374\n",
            "Avg 10 batchs takes  6.56  min\n",
            "EPOCH  49 batch  600 : Loss: 2.1922 Perplexity: 8.9553\n",
            " ********* Epoch  49  Average Levenshtein Distance is ****:  292.5625\n",
            "Avg 10 batchs takes  6.67  min\n",
            "EPOCH  49 batch  610 : Loss: 2.2845 Perplexity: 9.8203\n",
            "Avg 10 batchs takes  6.79  min\n",
            "EPOCH  49 batch  620 : Loss: 2.2858 Perplexity: 9.8336\n",
            "Avg 10 batchs takes  6.89  min\n",
            "EPOCH  49 batch  630 : Loss: 2.166 Perplexity: 8.723\n",
            "Avg 10 batchs takes  7.0  min\n",
            "EPOCH  49 batch  640 : Loss: 2.1255 Perplexity: 8.3768\n",
            "Avg 10 batchs takes  7.11  min\n",
            "EPOCH  49 batch  650 : Loss: 2.2035 Perplexity: 9.0571\n",
            "Avg 10 batchs takes  7.21  min\n",
            "EPOCH  49 batch  660 : Loss: 2.2474 Perplexity: 9.4635\n",
            "Avg 10 batchs takes  7.32  min\n",
            "EPOCH  49 batch  670 : Loss: 2.1177 Perplexity: 8.3122\n",
            "Avg 10 batchs takes  7.43  min\n",
            "EPOCH  49 batch  680 : Loss: 2.0885 Perplexity: 8.0728\n",
            "Avg 10 batchs takes  7.54  min\n",
            "EPOCH  49 batch  690 : Loss: 2.259 Perplexity: 9.574\n",
            "Avg 10 batchs takes  7.64  min\n",
            "EPOCH  49 batch  700 : Loss: 2.1706 Perplexity: 8.7636\n",
            "Avg 10 batchs takes  7.75  min\n",
            "EPOCH  49 batch  710 : Loss: 2.2305 Perplexity: 9.3043\n",
            "Avg 10 batchs takes  7.86  min\n",
            "EPOCH  49 batch  720 : Loss: 2.2065 Perplexity: 9.084\n",
            "Avg 10 batchs takes  7.96  min\n",
            "EPOCH  49 batch  730 : Loss: 2.1461 Perplexity: 8.5513\n",
            "Avg 10 batchs takes  8.07  min\n",
            "EPOCH  49 batch  740 : Loss: 2.1757 Perplexity: 8.8081\n",
            "Avg 10 batchs takes  8.18  min\n",
            "EPOCH  49 batch  750 : Loss: 2.0981 Perplexity: 8.1507\n",
            "Avg 10 batchs takes  8.28  min\n",
            "EPOCH  49 batch  760 : Loss: 2.2817 Perplexity: 9.7929\n",
            "Avg 10 batchs takes  8.39  min\n",
            "EPOCH  49 batch  770 : Loss: 2.2856 Perplexity: 9.8317\n",
            "Training loss after one epoch is: 2.21761417388916\n",
            "Time take for an epoch is: 8.41  min\n",
            " ********************* Epoch  49  ******************\n",
            "Test loss   49  is  4.097262511934553\n",
            "Perplexity is:  60.17533266429103\n",
            "Avg distance is:  126.25436507936507\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4GcQzg2U-8Af"
      },
      "source": [
        "## Inference\n",
        "• Pass only the utterance and the [start] character to your model  \n",
        "• Generate text from your model by sampling from the predicted distribution for some number of steps.  \n",
        "• Generate many samples in this manner for each test   utterance (100s or 1000s). You only do this on the test set to generate the Kaggle submission so the run time shouldn’t matter.  \n",
        "• Calculate the sequence lengths for each generated   sequence by finding the first [end] character  \n",
        "• Now run each of these generated samples back through your model to give each a loss value  \n",
        "• Take the randomly generated sample with the best loss value, optionally re-weighted or modified in some way like the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "avNnvD--XaMT",
        "colab": {}
      },
      "source": [
        "# generate input and decoding output\n",
        "def inference(model, test_loader):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "    # inference\n",
        "    predictions = []\n",
        "    for X, X_lens in test_loader:\n",
        "        X, X_lens = X.to(DEVICE), X_lens.to(DEVICE)\n",
        "\n",
        "        prediction = model(X, X_lens, isTrain=False)\n",
        "\n",
        "        predictions.append(prediction)\n",
        "\n",
        "        del X\n",
        "        del X_lens\n",
        "        # print(\"Current shape is\",predictions.shape)\n",
        "    model.train()\n",
        "    return torch.cat(predictions,dim=0)\n",
        "    # decoding\n",
        "    \n",
        "# get the best model\n",
        "\n",
        "###############\n",
        "model_id = 41\n",
        "###############\n",
        "model_save_name = 'classifier' + str(model_id+1)+ '.pt'\n",
        "path = F\"/content/drive/My Drive/Kaggle/{model_save_name}\"\n",
        "model = torch.load(path)\n",
        "model.to(DEVICE)\n",
        "# test\n",
        "criterion = nn.CrossEntropyLoss(reduce=False)\n",
        "test(model, dev_loader,criterion, 0)\n",
        "# predictions = inference(model, test_loader, isRandom= True)\n",
        "# print(predictions.shape)\n",
        "# decoded_list = decode_sentence(predictions,LETTER_LIST)\n",
        "\n",
        "# # save results\n",
        "# import pandas as pd\n",
        "# submission = pd.read_csv(\"test_sample_submission.csv\")\n",
        "# submission['Predicted'] = decoded_list\n",
        "# print(submission.head())\n",
        "# submission.to_csv('submission_no_noise.csv',index=False)\n",
        "\n",
        "predictions = inference(model, test_loader)\n",
        "print(predictions.shape)\n",
        "decoded_list = decode_sentence(predictions,LETTER_LIST)\n",
        "# save results\n",
        "import pandas as pd\n",
        "submission = pd.read_csv(\"test_sample_submission.csv\")\n",
        "submission['Predicted'] = decoded_list\n",
        "print(submission.head())\n",
        "submission.to_csv('/content/drive/My Drive/Kaggle/submission_no_noise.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GdEPe3UK-WJa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "52e7babd-4b8a-4fa7-8069-8632f1b8da4b"
      },
      "source": [
        "def RandomSearch(model, X,X_lens, search_width):\n",
        "    predictions = []\n",
        "    scores = []\n",
        "    # search_width = 3\n",
        "    print(X.shape)\n",
        "    batch_size = X.shape[1]\n",
        "    max_score = torch.zeros((batch_size,1)).to(DEVICE)\n",
        "\n",
        "    # hard code\n",
        "    max_prediction = torch.zeros((batch_size, 249, 35)).to(DEVICE)\n",
        "    for i in range(search_width):\n",
        "        # prediction: (batch, max_len, vocab_size)\n",
        "        # score: (batch, max_len)\n",
        "        prediction = model(X, X_lens, isTrain=False)\n",
        "\n",
        "        # update score \n",
        "        score = calScore(prediction)\n",
        "\n",
        "        k = (score > max_score).int()\n",
        "        max_score = k * score + (1-k) * max_score\n",
        "        mask = torch.ones(batch_size,249).to(DEVICE)\n",
        "        k = (mask * k).unsqueeze(dim=2)\n",
        "        max_prediction = k * prediction + (1 - k) * max_prediction\n",
        "\n",
        "        del prediction\n",
        "        del score\n",
        "        del k\n",
        "        del mask\n",
        "        \n",
        "    del max_score\n",
        "    return max_prediction\n",
        "\n",
        "\n",
        "\n",
        "def calScore(prediction):\n",
        "    '''\n",
        "        input prediction: (batch_size, max_len, vocab_size)\n",
        "        return score: (batch_size,)\n",
        "    '''\n",
        "    batch_size = prediction.shape[0]\n",
        "    max_len = prediciton.shape[1]\n",
        "    score = torch.zeros((batch_size,1)).to(DEVICE)\n",
        "    for i in range(batch_size):\n",
        "        # prediction[i,:,:] (max_len, vocab_size)\n",
        "        val, indice = torch.max(prediction[i,:,:], dim=1)\n",
        "        # should be max_len\n",
        "        try:\n",
        "            idx = (indice == 34).nonzero()[0]\n",
        "            # get the first index\n",
        "            idx = idx[0].item()\n",
        "            # print(idx)\n",
        "            score[i] = val[:idx].sum().item() / (idx + 1)\n",
        "        except:\n",
        "            score[i] = val.sum().item() / max_len\n",
        "        del val\n",
        "        del indice\n",
        "        \n",
        "    return score\n",
        "\n",
        "# def calScore(prediction, score):\n",
        "#     # prediction: (max_len,)\n",
        "\n",
        "\n",
        "# generate input and decoding output\n",
        "def inference(model, test_loader, isRandom=False):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        count = 0\n",
        "        # inference\n",
        "        predictions = []\n",
        "        for X, X_lens in test_loader:\n",
        "            # def handle_batch():\n",
        "            X, X_lens = X.to(DEVICE), X_lens.to(DEVICE)\n",
        "            if isRandom:\n",
        "                prediction = RandomSearch(model, X,X_lens, 3)\n",
        "\n",
        "            else:\n",
        "                prediction = model(X, X_lens, isTrain=False)\n",
        "\n",
        "            predictions.append(prediction)\n",
        "            del X\n",
        "            del X_lens\n",
        "            del prediction\n",
        "            # handle_batch()\n",
        "        # Make sure deallocation has taken place\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        model.train()\n",
        "    return torch.cat(predictions,dim=0)\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss(reduce=False)\n",
        "# model = torch.load(\"model/classifier39.pt\")\n",
        "# model.to(DEVICE)\n",
        "\n",
        "# test(model, dev_loader,criterion, 0)\n",
        "# # print(test(model, dev_loader, criterion, 0))\n",
        "# predictions = inference(model, test_loader, isRandom=True)\n",
        "# print(predictions.shape)\n",
        "# decoded_list = decode_sentence(predictions,LETTER_LIST)\n",
        "\n",
        "# # # save results\n",
        "# import pandas as pd\n",
        "# submission = pd.read_csv(\"test_sample_submission.csv\")\n",
        "# submission['Predicted'] = decoded_list\n",
        "# print(submission.head())\n",
        "# submission.to_csv('submission_no_noise.csv',index=False)\n",
        "\n",
        "# # compared result\n",
        "# predictions = inference(model, test_loader)\n",
        "# print(predictions.shape)\n",
        "# decoded_list = decode_sentence(predictions,LETTER_LIST)\n",
        "# # save results\n",
        "# import pandas as pd\n",
        "# submission = pd.read_csv(\"test_sample_submission.csv\")\n",
        "# submission['Predicted'] = decoded_list\n",
        "# print(submission.head())\n",
        "# submission.to_csv('submission_no_noise.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:559: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
            "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
            "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:559: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
            "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:559: UserWarning: Couldn't retrieve source code for container of type pBLSTM. It won't be checked for correctness upon loading.\n",
            "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
            "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:559: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
            "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:559: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (lstm1): pBLSTM(\n",
              "      (lstm): LSTM(40, 128, bidirectional=True)\n",
              "    )\n",
              "    (lstm2): pBLSTM(\n",
              "      (lstm): LSTM(512, 256, bidirectional=True)\n",
              "    )\n",
              "    (lstm3): pBLSTM(\n",
              "      (lstm): LSTM(1024, 256, bidirectional=True)\n",
              "    )\n",
              "    (KeyLinear): Linear(in_features=1024, out_features=256, bias=False)\n",
              "    (ValueLinear): Linear(in_features=1024, out_features=256, bias=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(35, 256, padding_idx=0)\n",
              "    (lstm1): LSTMCell(512, 256)\n",
              "    (lstm2): LSTMCell(256, 256)\n",
              "    (attention): Attention()\n",
              "    (character_prob): Linear(in_features=512, out_features=35, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39hvYWmeEUdP",
        "colab_type": "code",
        "colab": {},
        "outputId": "9f816098-5beb-403d-8404-621f9d4672e9"
      },
      "source": [
        "predictions = inference(model, test_loader)\n",
        "print(predictions.shape)\n",
        "decoded_list = decode_sentence(predictions,LETTER_LIST)\n",
        "# save results\n",
        "import pandas as pd\n",
        "submission = pd.read_csv(\"test_sample_submission.csv\")\n",
        "submission['Predicted'] = decoded_list\n",
        "print(submission.head())\n",
        "submission.to_csv('submission_no_noise.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([523, 249, 35])\n",
            "   Id                                          Predicted\n",
            "0   0  THE COMPANY ALSO DONATED FIFTY THOUSAND DOLLAR...\n",
            "1   1  MR. BENATZ'S CALL FOR MANDATORY TESTING HASN'T...\n",
            "2   2  A NEWS RERELIES ISSUED YESTERDAY SAID THAT THE...\n",
            "3   3  THE BUY OUTPLAN IS CONTINUED ON ALLEGHENY RECE...\n",
            "4   4  HOWEVER INCREASING THE COST OF RESEARCHERS NOT...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}