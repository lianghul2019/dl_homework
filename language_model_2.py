# -*- coding: utf-8 -*-
"""language_model_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Ec557VnsufudCGhcv10NpclX3vBvQzA

# CTC
"""

import torch
import torch.nn as nn
import torch.nn.utils.rnn as rnn
from torch.utils.data import Dataset, DataLoader, TensorDataset

from torch.nn.utils.rnn import *
import zipfile

import torchvision.transforms as transforms

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DEVICE

import numpy as np
import time

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

"""## Import Library"""

import sys
sys.path.append('/content/drive/My Drive/HW3p2')

from phoneme_list import PHONEME_MAP, PHONEME_LIST

len(PHONEME_LIST)

"""## Read Data files
read data from files
"""

dev_x = np.load('/content/drive/My Drive/HW3p2/hw3p2/wsj0_dev.npy',allow_pickle=True,encoding='latin1')
dev_label = np.load('/content/drive/My Drive/HW3p2/hw3p2/wsj0_dev_merged_labels.npy',allow_pickle=True,encoding='latin1')

train_x = np.load('/content/drive/My Drive/HW3p2/hw3p2/wsj0_train',allow_pickle=True,encoding='latin1')
train_label = np.load('/content/drive/My Drive/HW3p2/hw3p2/wsj0_train_merged_labels.npy',allow_pickle=True,encoding='latin1')

# see train data set size
print("The size of train set size is : ",len(train_x))
print("The size of dev set size is : ",len(dev_x))
print(dev_x.shape)

"""## Dataset Processing: Packed sequences
Since the length of training data is changing, we will use pad & pack strategy:  
1. pad_sequence  
2. pack_padded_sequence

### 1. Data Set
"""

def normalize(x, m, s): return (x-m)/s
def normalize_to(train):
    m,s = train.mean(axis = 0),train.std(axis = 0)
    return normalize(train, m, s)


class FrameDataset(Dataset):
    def __init__(self,data, labels):
        # apply normalization on features

        self.X = [torch.Tensor(i) for i in data]
        self.Y = [torch.Tensor(j)+1 for j in labels]

        self.X_lens = torch.LongTensor([len(seq) for seq in self.X])
        self.Y_lens = torch.LongTensor([len(seq) for seq in self.Y])
        
    def __getitem__(self,index):
        X = self.X[index]

        
        return X, self.X_lens[index], self.Y[index], self.Y_lens[index]

    def __len__(self):
        return len(self.X)

#### when use pad_sequence or pack_padded_sequence, remember the batch_first=True ####
#### this is mandantary when using CTCLoss

def my_collate(batch):
    data = [item[0] for item in batch]
    for i in range(len(data)):
        data[i] = normalize_to(data[i])
    target = [item[2] for item in batch]
    data_lens = torch.LongTensor([seq[1] for seq in batch])
    target_lens = torch.LongTensor([seq[3] for seq in batch])
    data = pad_sequence(data)

    target = pad_sequence(target, batch_first=True)
    
    return data, data_lens, target, target_lens


# use data set to read data, need to align with the train labels & dev labels
train_dataset = FrameDataset(train_x,train_label)
train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn = my_collate)

dev_dataset = FrameDataset(dev_x,dev_label)
dev_loader = DataLoader(dev_dataset, shuffle=False, batch_size=64, collate_fn = my_collate)

from torch.nn.utils.rnn import pad_sequence
from torch.nn.utils.rnn import pack_padded_sequence
from torch.nn.utils.rnn import pad_packed_sequence
# pad_sequence(sequences, batch_first=True)

"""### 2. Data Loader
loading data

## FUTURE STEPS
1. Remember to add cnn layer on the model
2. Remember to add regularizer
3. Remember to add decoder(generate function)
4. Remember to use L distance for evaluation (add eval function)

## Model

Embedding output size (seq_len, N,embed_size)  
Conv1d input size (N,embed_size,seq_len)  
LSTM input size (seq_len, N, embed_size)
"""

class Model(nn.Module):
    def __init__(self, in_channels, out_channels, hidden_size, out_size, dim_feature):
        super(Model, self).__init__()
        # a CNN layer
        self.cnn = nn.Conv1d(in_channels, out_channels,kernel_size = 3, padding=1)
        # LSTM layers
        self.lstm = nn.LSTM(128, 128, bidirectional=True)
        self.lstm1 = nn.LSTM(256, 256, bidirectional=True)
        self.lstm2 = nn.LSTM(512, 512, bidirectional=True)
        self.lstm3 = nn.LSTM(1024,512, bidirectional=True)


        # a linear layer
        self.flatten = nn.Linear(1024, 512)
        self.activation = nn.Tanh()
        self.flatten1 = nn.Linear(512, dim_feature)

    def forward(self,x, lengths):
        # transpose to input x
        
        # out = x

        # CNN layer
        x = torch.Tensor(np.transpose(x.cpu().numpy(),axes=[1,2,0]))
        x = x.to(DEVICE)
        out = self.cnn(x)
        out = torch.Tensor(np.transpose(out.detach().cpu().numpy(),axes=[2,0,1]))

        # LSTM layer
        out = out.to(DEVICE)

        out = pack_padded_sequence(out, lengths, enforce_sorted=False)
        out = self.lstm(out)[0]
        # out = self.bn(out)
        out = self.lstm1(out)[0]
        # out = self.bn(out)
        out = self.lstm2(out)[0]
        # out = self.bn(out)
        out = self.lstm3(out)[0]
        # out = self.bn(out)
        out = self.lstm3(out)[0]
        # out = self.bn(out)

        # out = self.lstm2(out)[0]

        out, out_len = pad_packed_sequence(out)

        ########## why log_softmax(2) ?? ######
        out = self.flatten(out)
        out = self.activation(out)
        out = self.flatten1(out).log_softmax(2)

        #### need revision here ####
        return out, out_len

# torch.manual_seed(11785)
batch_size = 64
model = Model(40, 128, 256, 1024, len(PHONEME_LIST) + 1)
model.to(DEVICE)

print(model)

# function for one epoch
def train_epoch_packed(model, optimizer, train_loader):
    model.train()
    criterion = nn.CTCLoss() 
    criterion = criterion.to(DEVICE)
    batch_id=0
    avg_loss = 0
    before = time.time()
    print("Training", len(train_loader), "number of batches")
    print("Current running lr is: ",optimizer.param_groups[0]['lr'])
    for inputs, input_lens,targets, target_lens in train_loader: # lists, presorted, preloaded on GPU
        batch_id+=1
        inputs, input_lens, targets, target_lens = inputs.to(DEVICE), input_lens.to(DEVICE),targets.to(DEVICE), target_lens.to(DEVICE)
        out, out_lens = model(inputs, input_lens)
        # print(target_lens)
        loss = criterion(out,  targets, out_lens, target_lens) # criterion of the concatenated output
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        avg_loss += loss.item()
        if batch_id % 100 == 0:
            after = time.time()
            # nwords = np.sum(np.array([len(l) for l in inputs]))
            # lpw = loss.item() / nwords
            print("Time elapsed: ", after - before)
            print("At batch",batch_id,'Loss', avg_loss/(100))
            avg_loss = 0
            before = after
        del inputs
        del input_lens
        del targets
        del target_lens
        del out
        del out_lens
    return model
    
    

def validate(model, val_loader):
    print("START validation process ...... ")
    model.eval()
    val_loss = 0
    batch_id = 0
    criterion = nn.CTCLoss() 
    for inputs, input_lens, targets, target_lens in val_loader:
        inputs, input_lens, targets, target_lens = inputs.to(DEVICE), input_lens.to(DEVICE),targets.to(DEVICE), target_lens.to(DEVICE)

        batch_id+=1
        out, out_lens = model(inputs, input_lens)
        loss = criterion(out,  targets, out_lens, target_lens)
        val_loss+=loss.item()
        del inputs
        del input_lens
        del targets
        del target_lens
    
    print("Loss in validation is : ", val_loss / (len(val_loader)))
    
    model.train()
    return val_loss

##### 5 LSTM + with CNN + with activation (tanh) #######
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.1,patience=1)

for i in range(40):
    # 
    print("Epoch ",i,"..........")
    model = train_epoch_packed(model, optimizer, train_loader)
    val_loss = validate(model, dev_loader)
    scheduler.step(val_loss)
    if i >= 8:
        model_saved_name = 'language_model2_' + str(i + 1)+ '.pt'
        torch.save(model, F"/content/drive/My Drive/Models/{model_saved_name}")

"""## Decoding"""

!git clone --recursive https://github.com/parlance/ctcdecode.git
!pip install wget

rm -rf /ctcdecode

# Commented out IPython magic to ensure Python compatibility.
# %cd ctcdecode
!pip install .
# %cd ..

## read test dataset
class ReadTestDataset(Dataset):
    def __init__(self,data):

        self.X = [torch.Tensor(i) for i in data]
        
        self.X_lens = torch.LongTensor([len(seq) for seq in self.X])
        
    def __getitem__(self,index):

        return self.X[index], self.X_lens[index]

    def __len__(self):
        return len(self.X)

#### when use pad_sequence or pack_padded_sequence, remember the batch_first=True ####
#### this is mandantary when using CTCLoss


def my_collate1(batch):
    data = [item[0] for item in batch]
    data_lens = torch.LongTensor([seq[1] for seq in batch])
    data = pad_sequence(data)
    return data, data_lens

# read test dataset, batch_size = 1

test = np.load("/content/drive/My Drive/HW3p2/hw3p2/wsj0_test",allow_pickle=True,encoding="latin1")
test_dataset = ReadTestDataset(test)
test_loader = DataLoader(test_dataset,batch_size=64, collate_fn=my_collate1)

"""## Optional: Resume Model"""

# model = Model(40, 40, 256, len(PHONEME_LIST) + 1)
# model.to(DEVICE)
# model = torch.load("/content/drive/My Drive/trained_model3.pt")
# model.eval()
# print(model)

torch.save(model, "/content/drive/My Drive/trained_model6.pt")

"""## Generate Result and Save Files"""

# read sample submission file
import pandas as pd
submission = pd.read_csv("/content/drive/My Drive/HW3p2/hw3p2/sample_submission.csv")

from ctcdecode import CTCBeamDecoder
PHONEME_MAP = [" "] + PHONEME_MAP
decoder = CTCBeamDecoder(PHONEME_MAP, beam_width=15, log_probs_input=True)

def Decoder(model, val_loader, phoneme_map, decoder):
    print("START decoding process ...... ")
    model.eval()

    # criterion = nn.CTCLoss() 
    # phoneme_map = [' '] + phoneme_map
    
    return_list = []

    for inputs, input_lens in val_loader:
        # loading data
        inputs, input_lens = inputs.to(DEVICE), input_lens.to(DEVICE)

        with torch.no_grad():
            out, out_lens = model(inputs, input_lens)

        # decoder
        test_Y, _, _, test_Y_lens = decoder.decode(out.transpose(0, 1), out_lens)
        # print(test_Y)

        # mapping
        for i in range(len(test_Y)):
            best_seq = test_Y[i, 0, :test_Y_lens[i, 0]]
            best_pron = ''.join(phoneme_map[i] for i in best_seq)
            # print("Decoded phoneme: ",best_pron)
            return_list.append(best_pron)

        del inputs
        del input_lens
        del out
        del out_lens
        del test_Y
        del test_Y_lens
    
    
    model.train()
    return return_list

return_list = Decoder(model, test_loader,PHONEME_MAP,decoder)

submission['Predicted'] = return_list

submission.head()
submission.to_csv("submission3.csv",index=False)

### log #### 
# start with 0.0001 is not a good choice, maybe we should start with a higher init, and reduce the result later

"""## Future work
1. Write validation function. (already add a linear layer at the end)
2. Apply data preprocessing. Use mean normalization
3. Can we add a CNN before (done)
4. change the implementation of CNN, through channels.

### Reminders

By default, for all rnn modules (rnn, GRU, LSTM) batch_first = False
To use packed sequences, your inputs first need to be sorted in descending order of length (longest to shortest)
Batches need to have inputs of the same length
"""