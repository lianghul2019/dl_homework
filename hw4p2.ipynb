{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4p2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZkJVvxjLyTe",
        "colab_type": "code",
        "outputId": "d06d610c-80c9-4250-e60c-dcf9b1fc629c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwpEanuMLzgz",
        "colab_type": "code",
        "outputId": "b4943f5b-05ca-4773-ea1e-5e03d25ae60e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import zipfile\n",
        "import tarfile\n",
        "from PIL import Image\n",
        "from torch.nn.utils.rnn import *\n",
        "\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "print(cuda)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS1jv8nUTY5k",
        "colab_type": "code",
        "outputId": "a4dedfe7-54e3-485e-ab12-17d976270197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "with zipfile.ZipFile('/content/drive/My Drive/11-785-s20-hw4p2.zip','r') as zp:\n",
        "    print('Starting to extract......')\n",
        "    zp.extractall()\n",
        "    print('Done!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting to extract......\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QukUcetNT6Zh",
        "colab_type": "text"
      },
      "source": [
        "## Loading data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnvwfwkPTgyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    speech_train = np.load('train_new.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_valid = np.load('dev_new.npy', allow_pickle=True, encoding='bytes')\n",
        "    speech_test = np.load('test_new.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "    transcript_train = np.load('train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "    transcript_valid = np.load('dev_transcripts.npy', allow_pickle=True,encoding='bytes')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRVhBfzF0PHw",
        "colab_type": "text"
      },
      "source": [
        "## Adding <sos> and <eos> label to each sentence in label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7orJLHXa0PaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(transcript_train)):\n",
        "  transcript_train[i] = np.asarray(['<sos>'] + list(transcript_train[i]) + ['<eos>'])\n",
        "\n",
        "for i in range(len(transcript_valid)):\n",
        "  transcript_valid[i] = np.asarray(['<sos>'] + list(transcript_valid[i]) + ['<eos>'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGs5M8lzf8fz",
        "colab_type": "text"
      },
      "source": [
        "## appending sos and eos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "697RyMcJfSGB",
        "colab_type": "code",
        "outputId": "cf435fbb-4c0d-4cc5-8073-4f63c39c956f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "\n",
        "print('Check train.........')\n",
        "for i in range(10):\n",
        "  print(transcript_train[i])\n",
        "\n",
        "print()\n",
        "print('Check valid.........')\n",
        "for i in range(10):\n",
        "  print(transcript_valid[i])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Check train.........\n",
            "['<sos>' 'THE' 'FEMALE' 'PRODUCES' 'A' 'LITTER' 'OF' 'TWO' 'TO' 'FOUR'\n",
            " 'YOUNG' 'IN' 'NOVEMBER' 'AND' 'DECEMBER' '<eos>']\n",
            "['<sos>' 'NUMEROUS' 'WORKS' 'OF' 'ART' 'ARE' 'BASED' 'ON' 'THE' 'STORY'\n",
            " 'OF' 'THE' 'SACRIFICE' 'OF' 'ISAAC' '<eos>']\n",
            "['<sos>' 'THEIR' 'SOLUTION' 'REQUIRES' 'DEVELOPMENT' 'OF' 'THE' 'HUMAN'\n",
            " 'CAPACITY' 'FOR' 'SOCIAL' 'INTEREST' '<eos>']\n",
            "['<sos>' 'HIS' 'MOST' 'SIGNIFICANT' 'SCIENTIFIC' 'PUBLICATIONS' 'WERE'\n",
            " 'STUDIES' 'OF' 'BIRDS' 'AND' 'ANIMALS' '<eos>']\n",
            "['<sos>' 'IN' 'RECENT' 'YEARS' 'SHE' 'HAS' 'PRIMARILY' 'APPEARED' 'IN'\n",
            " 'TELEVISION' 'FILMS' 'SUCH' 'AS' 'LITTLE' 'GLORIA' '<eos>']\n",
            "['<sos>' 'THE' 'PROCESS' 'BY' 'WHICH' 'THE' 'LENS' 'FOCUSES' 'ON'\n",
            " 'EXTERNAL' 'OBJECTS' 'IS' 'CALLED' 'ACCOMMODATION' '<eos>']\n",
            "['<sos>' 'TWO' 'NARROW' 'GAUGE' 'RAILROADS' 'FROM' 'CHINA' 'ENTER' 'THE'\n",
            " 'CITY' 'FROM' 'THE' 'NORTHEAST' 'AND' 'NORTHWEST' '<eos>']\n",
            "['<sos>' 'SOME' 'MAPS' 'USE' 'BANDS' 'OF' 'COLOR' 'TO' 'INDICATE'\n",
            " 'DIFFERENT' 'INTERVALS' 'OF' 'VALUE' '<eos>']\n",
            "['<sos>' 'ORIGINS' 'OR' 'CAUSES' 'OF' 'SPONTANEOUS' 'MUTATION' 'ARE' 'NOT'\n",
            " 'YET' 'COMPLETELY' 'CLEAR' '<eos>']\n",
            "['<sos>' 'UNUSUALLY' 'HIGH' 'LEVELS' 'OF' 'RADIATION' 'WERE' 'DETECTED'\n",
            " 'IN' 'MANY' 'EUROPEAN' 'COUNTRIES' '<eos>']\n",
            "\n",
            "Check valid.........\n",
            "['<sos>' 'THE' 'FEMALE' 'PRODUCES' 'A' 'LITTER' 'OF' 'TWO' 'TO' 'FOUR'\n",
            " 'YOUNG' 'IN' 'NOVEMBER' 'AND' 'DECEMBER' '<eos>']\n",
            "['<sos>' 'NUMEROUS' 'WORKS' 'OF' 'ART' 'ARE' 'BASED' 'ON' 'THE' 'STORY'\n",
            " 'OF' 'THE' 'SACRIFICE' 'OF' 'ISAAC' '<eos>']\n",
            "['<sos>' 'THEIR' 'SOLUTION' 'REQUIRES' 'DEVELOPMENT' 'OF' 'THE' 'HUMAN'\n",
            " 'CAPACITY' 'FOR' 'SOCIAL' 'INTEREST' '<eos>']\n",
            "['<sos>' 'HIS' 'MOST' 'SIGNIFICANT' 'SCIENTIFIC' 'PUBLICATIONS' 'WERE'\n",
            " 'STUDIES' 'OF' 'BIRDS' 'AND' 'ANIMALS' '<eos>']\n",
            "['<sos>' 'IN' 'RECENT' 'YEARS' 'SHE' 'HAS' 'PRIMARILY' 'APPEARED' 'IN'\n",
            " 'TELEVISION' 'FILMS' 'SUCH' 'AS' 'LITTLE' 'GLORIA' '<eos>']\n",
            "['<sos>' 'THE' 'PROCESS' 'BY' 'WHICH' 'THE' 'LENS' 'FOCUSES' 'ON'\n",
            " 'EXTERNAL' 'OBJECTS' 'IS' 'CALLED' 'ACCOMMODATION' '<eos>']\n",
            "['<sos>' 'TWO' 'NARROW' 'GAUGE' 'RAILROADS' 'FROM' 'CHINA' 'ENTER' 'THE'\n",
            " 'CITY' 'FROM' 'THE' 'NORTHEAST' 'AND' 'NORTHWEST' '<eos>']\n",
            "['<sos>' 'SOME' 'MAPS' 'USE' 'BANDS' 'OF' 'COLOR' 'TO' 'INDICATE'\n",
            " 'DIFFERENT' 'INTERVALS' 'OF' 'VALUE' '<eos>']\n",
            "['<sos>' 'ORIGINS' 'OR' 'CAUSES' 'OF' 'SPONTANEOUS' 'MUTATION' 'ARE' 'NOT'\n",
            " 'YET' 'COMPLETELY' 'CLEAR' '<eos>']\n",
            "['<sos>' 'UNUSUALLY' 'HIGH' 'LEVELS' 'OF' 'RADIATION' 'WERE' 'DETECTED'\n",
            " 'IN' 'MANY' 'EUROPEAN' 'COUNTRIES' '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WMrU4qfxYoI",
        "colab_type": "code",
        "outputId": "eee16555-40de-4b37-9168-c4e01da65dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(transcript_valid))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzFDUm1P0Fdf",
        "colab_type": "text"
      },
      "source": [
        "## Change labels to character based level, May expend the whole label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9nC4A1lvjWo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LETTER_LIST = ['<pad>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \\\n",
        "               'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5RmYyZ4x75h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label_num(word):\n",
        "  result = []\n",
        "  if word == '<sos>' or word == '<eos>' or word == '<pad>':\n",
        "    result.append(LETTER_LIST.index(word))\n",
        "    return result\n",
        "\n",
        "  for i in word:\n",
        "    result.append(LETTER_LIST.index(i))\n",
        "  result.append(0)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuHHfVSf02Xl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label(label):\n",
        "  result = []\n",
        "  for i in range(len(label)):\n",
        "    temp = []\n",
        "    for j in label[i]:\n",
        "      temp += get_label_num(j)\n",
        "    result.append(temp)\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG4u2_-glRES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_train = np.asarray(get_label(transcript_train))\n",
        "label_valid = np.asarray(get_label(transcript_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq2cX4h63vbK",
        "colab_type": "code",
        "outputId": "f2d45bb3-f01a-44d1-f147-f97fccca77a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(label_train)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoJg2SCoJaYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x, m, s): return (x-m)/s\n",
        "def normalize_to(train):\n",
        "    m,s = train.mean(),train.std()\n",
        "    return normalize(train, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLuW5PZ6lL1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(object):\n",
        "\n",
        "    def __init__(self, X, Y):\n",
        "        self.data_dict = []\n",
        "        self.Y = [torch.Tensor(j) for j in Y]\n",
        "        self.X = [torch.Tensor(j) for j in X]\n",
        "\n",
        "        self.X_lens = torch.LongTensor([len(seq) for seq in X])\n",
        "        self.Y_lens = torch.LongTensor([len(seq) for seq in Y])\n",
        "        self.size = len(self.X)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.X_lens[index], self.Y[index], self.Y_lens[index]\n",
        "      \n",
        "def my_collate(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    target = [item[2] for item in batch]\n",
        "    data_lens = torch.LongTensor([seq[1] for seq in batch])\n",
        "    target_lens = torch.LongTensor([seq[3] for seq in batch])\n",
        "    for i in range(len(data)):\n",
        "      data[i] = normalize_to(data[i])\n",
        "    data = pad_sequence(data, batch_first=True)\n",
        "    target = pad_sequence(target, batch_first=True)\n",
        "    pad = torch.zeros((data.shape[0], 1, data.shape[2]))\n",
        "    if data.shape[1] % 2 != 0:\n",
        "      data = torch.cat((data, pad), dim = 1)\n",
        "    return data, data_lens, target, target_lens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8MGFCWDJnFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train dataset\n",
        "train_dataset = MyDataset(speech_train, label_train)\n",
        "train_loader = data.DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn = my_collate)\n",
        "\n",
        "# validation dataset\n",
        "valid_dataset = MyDataset(speech_valid, label_valid)\n",
        "valid_loader = data.DataLoader(valid_dataset, shuffle=False, batch_size=64, collate_fn = my_collate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txLEJWxlJ3Bo",
        "colab_type": "code",
        "outputId": "7c35b078-252c-4ae8-dffc-118a024b5e70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "count = 0\n",
        "for features, features_lens, targets, target_lens in train_loader: \n",
        "  print('features: ', features.shape)\n",
        "  print('features_lens: ', features_lens)\n",
        "  print('targets: ', targets.shape)\n",
        "  print('target_lens: ', target_lens)\n",
        "  print()\n",
        "  if count == 10:\n",
        "    break\n",
        "  count+=1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features:  torch.Size([64, 1368, 40])\n",
            "features_lens:  tensor([ 553,  584, 1094,  712,  410,  155,  904, 1094,  489,  638,  421,  583,\n",
            "         585,  483,  495,  268, 1212, 1368,  720,  477,  942,  284,  517,  587,\n",
            "         534,  349,  957, 1354,  506,  378,  473,  764,  721,  297,  426,  588,\n",
            "         702,  477, 1101,  832,  415,  964,  752,  169,  578,  573,  957,  430,\n",
            "         560, 1183,  561,  390,  174,  412,  592,  481, 1022, 1187,  795, 1058,\n",
            "         937,  583,  517,  371])\n",
            "targets:  torch.Size([64, 218])\n",
            "target_lens:  tensor([ 65, 121, 185, 132,  57,  22, 165, 157,  82, 120,  85,  89,  82,  84,\n",
            "         73,  43, 181, 180,  96,  91, 155,  39,  73,  94,  82,  64, 147, 218,\n",
            "         99,  60,  75, 132, 136,  42,  73, 131, 101,  56, 216, 130,  83, 160,\n",
            "        115,  24, 107,  99, 133,  82,  79, 152,  94,  64,  34,  92,  82,  76,\n",
            "        168, 163, 102, 166, 183, 110,  82,  52])\n",
            "\n",
            "features:  torch.Size([64, 1352, 40])\n",
            "features_lens:  tensor([ 685,  530,  822,  782,  617,  454,  807,  462, 1063,  534,  693,  642,\n",
            "         491,  253,  444,  954,  615,  844,  821,  701,  576,  392,  458,  897,\n",
            "         697,  760,  663,  487,  738, 1352,  485,  487,  360,  560,  591, 1218,\n",
            "         518,  421,  457,  578, 1019,  779,  608,  648,  481,  568,  804, 1042,\n",
            "         582,  966,  896,  991,  834,  794,  533, 1011,  395,  449, 1088,  428,\n",
            "         717,  639,  488, 1043])\n",
            "targets:  torch.Size([64, 199])\n",
            "target_lens:  tensor([104,  78, 128, 131, 104,  90, 134,  63, 164, 110,  88,  82,  75,  39,\n",
            "         74, 181, 130, 143, 154, 115,  88,  72,  71, 130, 113, 138, 130,  89,\n",
            "        129, 152,  92,  83,  49,  85, 106, 199,  95,  79,  87,  88, 175, 144,\n",
            "        113,  87,  69,  82, 155, 157, 112, 154, 138, 135, 130, 128,  90, 147,\n",
            "         72,  77, 178,  78, 129, 116, 105, 167])\n",
            "\n",
            "features:  torch.Size([64, 1348, 40])\n",
            "features_lens:  tensor([ 816,  672,  277, 1347,  767,  502,  839,  917,  775, 1144,  526, 1060,\n",
            "         987,  335,  487,  444,  526,  818,  820,  555,  449,  600,  897,  555,\n",
            "         543,  459,  538,  456,  982,  359,  736,  396,  506,  513, 1036,  374,\n",
            "        1205,  947,  865,  167,  483,  698,  256,  938,  680,  464,  720,  487,\n",
            "         214,  787,  490, 1080,  817,  752,  374,  427,  624,  568,  132, 1020,\n",
            "         539,  480,  916,  624])\n",
            "targets:  torch.Size([64, 228])\n",
            "target_lens:  tensor([132, 132,  30, 228, 152, 102, 173, 102, 107, 166,  94, 137, 182,  54,\n",
            "         61,  87,  78, 138, 148,  96,  88,  92, 147, 115,  66,  72,  84,  83,\n",
            "        141,  47, 112,  73,  77,  88, 194,  73, 181, 166, 147,  21,  84, 138,\n",
            "         33, 145, 120,  84, 102,  57,  15, 127,  73, 183, 150, 114,  83,  84,\n",
            "         77,  87,  25, 145,  78,  78, 126, 117])\n",
            "\n",
            "features:  torch.Size([64, 1218, 40])\n",
            "features_lens:  tensor([ 653,  633,  527,  546,  632,  664, 1083,  803,  419,  903,  582,  640,\n",
            "         844,  287,  591,  795,  700, 1218,  528,  535,  892,  576,  551,  804,\n",
            "         505,  841,  802,  678,  411,  562,  631,  774,  469,  908,  597,  578,\n",
            "         572,  612,  890,  759,  671,  526,  442,  623, 1114,  449,  610,  710,\n",
            "         380,  646,  459,  533,  832,  625,  900,  403,  428,  526,  487,  848,\n",
            "         371,  334,  377,  647])\n",
            "targets:  torch.Size([64, 194])\n",
            "target_lens:  tensor([120,  95,  92,  91, 107, 111, 151, 137,  71, 148, 106, 104, 140,  51,\n",
            "         98, 121, 138, 194,  89,  95, 138,  94,  83, 144,  91, 166, 126, 128,\n",
            "         77, 111,  96, 141,  75, 139,  94, 102, 100, 119, 139, 132, 116, 112,\n",
            "         89,  89, 179,  97,  90, 124,  82,  82,  78,  92, 160,  77, 158,  82,\n",
            "         78,  91,  93, 139,  59,  65,  69, 106])\n",
            "\n",
            "features:  torch.Size([64, 1312, 40])\n",
            "features_lens:  tensor([ 814,  837,  384,  401, 1094,  494,  431,  287,  214,  613,  823,  537,\n",
            "         443,  576,  296,  658,  977,  394,  488,  405,  661,  977,  575,  482,\n",
            "         832,  197,  447,  898,  823,  543,  858,  651,  210,  912,  810,  659,\n",
            "        1171,  237, 1311,  377,  484,  721,  376,  349,  947,  561, 1190,  658,\n",
            "         441, 1019,  696,  850,  996,  568,  786,  667,  836, 1020,  473,  833,\n",
            "         943,  652,  637,  214])\n",
            "targets:  torch.Size([64, 181])\n",
            "target_lens:  tensor([152, 148,  65,  84, 158,  72,  66,  32,  26, 118, 159,  91,  76, 103,\n",
            "         34,  94, 160,  70,  77,  69, 111, 169,  84, 104, 125,  31,  68, 147,\n",
            "         99,  82, 148,  86,  29, 162,  98, 124, 181,  43, 180,  63,  88, 140,\n",
            "         63,  62, 140,  90, 170, 109,  94, 144, 111, 170, 157,  86, 146, 107,\n",
            "        148, 178,  82, 165, 158, 129, 105,  45])\n",
            "\n",
            "features:  torch.Size([64, 1484, 40])\n",
            "features_lens:  tensor([ 714,  906,  455,  614, 1217,  438,  589,  691,  465,  926,  439,  461,\n",
            "         485,  490,  978,  303,  293,  567,  353, 1171,  402,  251,  651,  406,\n",
            "         881,  963,  534,  467,  478,  318,  868,  690,  426,  815,  863,  917,\n",
            "         303,  882,  508,  945,  606,  885, 1090,  630,  763,  242,  514,  626,\n",
            "         458,  944,  814,  735,  460,  689,  566, 1484,  754,  832,  573,  786,\n",
            "         482,  468,  566,  617])\n",
            "targets:  torch.Size([64, 193])\n",
            "target_lens:  tensor([104, 144,  67,  78, 178,  85,  80, 105,  84, 182,  64,  77,  78,  67,\n",
            "        170,  45,  43,  87,  79, 193,  67,  47, 127,  73, 174, 190,  59,  68,\n",
            "         85,  53, 162, 137,  61, 123, 124, 141,  57, 129,  95, 134, 117, 161,\n",
            "        168, 101, 164,  36,  83,  88,  78, 178, 138, 112,  79,  86,  93, 184,\n",
            "        117, 156,  97, 134,  69,  71, 104, 102])\n",
            "\n",
            "features:  torch.Size([64, 1284, 40])\n",
            "features_lens:  tensor([ 725,  195,  605,  769,  784,  897,  459,  534,  621,  618,  824,  442,\n",
            "         562,  359,  884,  672,  488,  655,  956, 1284,  859,  741,  565,  368,\n",
            "         761,  554,  513,  555,  647,  591,  626,  336,  908,  720,  944, 1049,\n",
            "         388,  349,  692, 1064,  645,  309,  714,  663,  782,  600,   91,  443,\n",
            "         825,  618, 1172,  716,  506,  821,  771,  439, 1175,  447,  222,  724,\n",
            "         441,  464, 1046,  574])\n",
            "targets:  torch.Size([64, 197])\n",
            "target_lens:  tensor([106,  25, 132, 132, 159, 143,  84,  97, 125, 109, 169,  73,  92,  59,\n",
            "        173, 107,  95, 148, 152, 178, 152, 120, 112,  47, 144,  80,  94,  97,\n",
            "        116,  93, 127,  50, 165, 118, 139, 163,  73,  50, 113, 171, 107,  54,\n",
            "        128, 139, 155,  78,   6,  84, 133, 117, 131, 124,  82, 136, 141,  77,\n",
            "        156,  80,  23, 119,  73,  83, 197,  97])\n",
            "\n",
            "features:  torch.Size([64, 1468, 40])\n",
            "features_lens:  tensor([ 811,  938,  507,  531,  335,  285,  856,  638,  294,  863,  539,  945,\n",
            "         778,  604,  977,  760,  921,  806,  402, 1300,  368,  725,  817,  264,\n",
            "         555,  813,  581,  879,  567, 1468,  984,  592,  738,  521,  951,  688,\n",
            "        1004,  797,  880,  627,  636, 1005,  612,  217, 1124,  835,  990,  744,\n",
            "         925,  615,  642,  354,  419,  278,  624,  364,  740,  807,  424,  456,\n",
            "         766,  143,  514,  354])\n",
            "targets:  torch.Size([64, 195])\n",
            "target_lens:  tensor([132, 156, 103, 105,  63,  54, 165, 120,  52, 131,  73, 159, 150,  98,\n",
            "        189,  79, 176, 129,  78, 158,  77, 125, 137,  51,  83, 114, 113, 129,\n",
            "         94, 195, 136,  88, 123,  88, 148,  92, 152, 122, 139, 100, 113, 180,\n",
            "         95,  44, 191, 166, 183,  92, 141, 108, 109,  64,  82,  39,  86,  67,\n",
            "        106,  96,  83,  72, 140,  15, 120,  67])\n",
            "\n",
            "features:  torch.Size([64, 1402, 40])\n",
            "features_lens:  tensor([ 505,  764, 1004,  506,  612,  418, 1201,  715,  525,  452,  941, 1087,\n",
            "         524,  571,  681,  425,  423,  594,  775,  485,  729,  479,  716, 1402,\n",
            "         919,  803,  548,  579,  891,  615,  596,  895,  675,  355,  502,  546,\n",
            "         513, 1138,  555,  843,  431,  472,  623,  493,  505,  489,  772,  547,\n",
            "         460,  553,  234,  670,  368,  438,  655,  401,  529,  397,  316,  517,\n",
            "         314,  493,  472,  461])\n",
            "targets:  torch.Size([64, 187])\n",
            "target_lens:  tensor([ 75, 143, 138,  92, 120,  78, 187,  82,  94,  57, 134, 175, 115, 114,\n",
            "        117,  63,  66,  83, 136,  71, 108,  87, 132, 178, 166, 123, 101,  84,\n",
            "        137,  94,  98, 164, 118,  63,  73,  97,  76, 169,  67, 163,  79,  79,\n",
            "        117,  72,  94,  87, 137,  81,  79, 100,  41, 100,  58,  75,  77,  73,\n",
            "         81,  66,  56,  84,  55,  94,  74,  78])\n",
            "\n",
            "features:  torch.Size([64, 1490, 40])\n",
            "features_lens:  tensor([ 632,  678,  907,  347,  691,  209,  359,  788, 1029,  765,  457,  457,\n",
            "         764,  872,  298,  240,  791,  516,  488,  441,  707,  393,  507,  555,\n",
            "         518,  412,  720, 1489,  972,  494, 1173,  180,  530,  487,  880, 1092,\n",
            "         855,  511,  455,  800,  547,  685,  525,  527,  561,  503, 1037,  753,\n",
            "         378,  511,  812,  406,  619,  393,  482,  761,  727,  631, 1157, 1350,\n",
            "         431,  977,  425,  568])\n",
            "targets:  torch.Size([64, 222])\n",
            "target_lens:  tensor([ 83, 115, 116,  59,  84,  24,  56, 144, 152, 109,  88,  70, 127, 166,\n",
            "         49,  42, 161,  78,  76,  77,  78,  54,  83,  76,  83,  72, 150, 222,\n",
            "        187,  98, 158,  26,  80,  77, 166, 136, 126,  84,  90, 144,  73,  90,\n",
            "         64,  87,  96,  91, 147, 130,  65,  78, 120,  66, 112,  68,  86, 140,\n",
            "        135, 118, 157, 200,  79, 132,  53,  78])\n",
            "\n",
            "features:  torch.Size([64, 1482, 40])\n",
            "features_lens:  tensor([1117,  710,  901,  401,  283,  414, 1481, 1090,  362,  615,  872,  487,\n",
            "         481,  508,  536,  579,  525,  590, 1151,  886,  850, 1110, 1076,  451,\n",
            "         798,  445,  427,  886, 1186,  889,  528,  481,  275,  626,  477,  731,\n",
            "         688, 1253, 1028,  534,  929,  478,  886,  192,  510,  520,  560,  564,\n",
            "        1248,  236,  791,  959,  469,  173,  909,  437,  625,  511, 1110,  405,\n",
            "         403,  534,  566,  419])\n",
            "targets:  torch.Size([64, 201])\n",
            "target_lens:  tensor([192, 104, 137,  60,  42,  69, 176, 168,  49,  84, 179,  53,  82,  63,\n",
            "         82,  96,  76, 115, 134, 148, 146, 184, 156,  79, 115,  78,  82, 134,\n",
            "        162, 140,  54,  84,  58, 117,  84, 113, 101, 175, 139,  94, 143,  87,\n",
            "        159,  14,  94,  88,  79,  84, 169,  38,  89, 157,  78,  33, 146,  71,\n",
            "         82,  85, 201,  81,  58,  69,  85,  70])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSGV0kd-qpQQ",
        "colab_type": "text"
      },
      "source": [
        "## pBLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2XSG7gyTSAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pBLSTM(nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
        "    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n",
        "    and inferior results even after extensive training.\n",
        "    The major reason is inability of AttendAndSpell operation to extract relevant information\n",
        "    from a large number of input steps.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = nn.LSTM(input_size=input_dim*2, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "    def forward(self, x, length):\n",
        "        '''\n",
        "        :param x :(T, B, N) input to the pBLSTM\n",
        "        :return output: (T/2, B, N*2) encoded sequence from pyramidal Bi-LSTM \n",
        "        '''\n",
        "        # change the data from B * T * H -> B * T/2 * H*2\n",
        "        DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        pad = torch.zeros((x.shape[0], 1, x.shape[2]))\n",
        "        pad = pad.to(DEVICE)\n",
        "        if x.shape[1] % 2 != 0:\n",
        "          x = torch.cat((x, pad), dim = 1).to(DEVICE)\n",
        "        batch_size = x.shape[0]\n",
        "        seq_len = x.shape[1]\n",
        "        dim = x.shape[2]\n",
        "        l1 = (seq_len // 2)\n",
        "        # print('l1: ', l1)\n",
        "        # print('temp[:l1]: ', x[:l1])\n",
        "        inputs = torch.cat((x[:, :l1, :], x[:, l1:, :]), dim = 2).to(DEVICE)\n",
        "\n",
        "        # change the each length of data from a to a / 2\n",
        "        for i in range(len(length)):\n",
        "          if length[i] % 2 != 0:\n",
        "            length[i] = (length[i] + 1) / 2\n",
        "          else:\n",
        "            length[i] = length[i] / 2\n",
        "        \n",
        "        input_data = pack_padded_sequence(inputs, length, batch_first=True,enforce_sorted=False)\n",
        "        output, _ = self.blstm(input_data)\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True)\n",
        "        return output, length\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxc948QgqpGH",
        "colab_type": "text"
      },
      "source": [
        "## Listener "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NpU6chP5zKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key and value.\n",
        "    Key and value are nothing but simple projections of the output from pBLSTM network.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, value_size=128,key_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.pBLSTM1 = pBLSTM(input_dim, hidden_dim)\n",
        "        self.pBLSTM2 = pBLSTM(2*hidden_dim, hidden_dim)\n",
        "        self.pBLSTM3 = pBLSTM(2*hidden_dim, hidden_dim)\n",
        "        \n",
        "        ### Add code to define the blocks of pBLSTMs! ###\n",
        "\n",
        "        self.key_network = nn.Linear(hidden_dim*2, value_size)\n",
        "        self.value_network = nn.Linear(hidden_dim*2, key_size)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        output, input_lengths = self.pBLSTM1(x, lens)\n",
        "        output, input_lengths = self.pBLSTM2(output, input_lengths)\n",
        "        output, input_lengths = self.pBLSTM3(output, input_lengths)\n",
        "        keys = self.key_network(output)\n",
        "        value = self.value_network(output)\n",
        "\n",
        "        return keys, value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaESEG6trxqS",
        "colab_type": "text"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMUtjP0CrhdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key, value and query from Encoder and decoder.\n",
        "    Below are the set of operations you need to perform for computing attention:\n",
        "        energy = bmm(key, query)\n",
        "        attention = softmax(energy)\n",
        "        context = bmm(attention, value)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, query, key, value, lens):\n",
        "        '''\n",
        "        :param query :(N, context_size) Query is the output of LSTMCell from Decoder\n",
        "        :param key: (N, key_size) Key Projection from Encoder per time step\n",
        "        :param value: (N, value_size) Value Projection from Encoder per time step\n",
        "        :return output: Attended Context\n",
        "        :return attention_mask: Attention mask that can be plotted  \n",
        "        '''\n",
        "        energy = torch.bmm(key.unsqueeze(2), query.unsqueeze(1))\n",
        "\n",
        "        # Create an (batch_size, max_len) boolean mask for all padding positions\n",
        "        # Make use of broadcasting: (1, max_len), (batch_size, 1) -> (batch_size, max_len)\n",
        "        mask = torch.arange(key.size(1)).unsqueeze(0) >= lens.unsqueeze(1)\n",
        "        \n",
        "        # Set attention logits at padding positions to negative infinity.\n",
        "        energy.masked_fill_(mask, -1e9)\n",
        "\n",
        "        attention = nn.functional.softmax(energy, dim = -1)\n",
        "        context = torch.bmm(attention, value.unsqueeze(2)).squeeze(2)\n",
        "        return context, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_srZ-0aXYLzq",
        "colab_type": "text"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEWRmuzInJBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, \n",
        "    thus we use LSTMCell instead of LSLTM here.\n",
        "    The output from the second LSTMCell can be used as query here for attention module.\n",
        "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=False):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
        "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
        "\n",
        "        self.isAttended = isAttended\n",
        "        if (isAttended == True):\n",
        "            self.attention = Attention()\n",
        "\n",
        "        self.character_prob = nn.Linear(key_size + value_size, vocab_size)\n",
        "\n",
        "    def forward(self, key, values, text, isTrain=True):\n",
        "        '''\n",
        "        :param key :(T, N, key_size) Output of the Encoder Key projection layer\n",
        "        :param values: (T, N, value_size) Output of the Encoder Value projection layer\n",
        "        :param text: (N, text_len) Batch input of text with text_length\n",
        "        :param isTrain: Train or eval mode\n",
        "        :return predictions: Returns the character perdiction probability \n",
        "        '''\n",
        "        batch_size = key.shape[0]\n",
        "        print('key: ', key.shape)\n",
        "        print('value: ', values.shape)\n",
        "        print('text: ', text.shape)\n",
        "        if (isTrain == True):\n",
        "            max_len =  text.shape[1]\n",
        "            embeddings = self.embedding(text.long())\n",
        "        else:\n",
        "            max_len = 250\n",
        "\n",
        "        predictions = []\n",
        "        hidden_states = [None, None]\n",
        "        prediction = torch.zeros(batch_size,1).to(DEVICE)\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # * Implement Gumble noise and teacher forcing techniques \n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
        "            # * If you haven't implemented attention yet, then you may want to check the index and break \n",
        "            #   out of the loop so you do you do not get index out of range errors. \n",
        "\n",
        "            if (isTrain):\n",
        "                char_embed = embeddings[:,i,:]\n",
        "                # shape : N, 1, 35\n",
        "            else:\n",
        "                char_embed = self.embedding(prediction.argmax(dim=-1))\n",
        "\n",
        "            # value[i,:,:] -> 1, N, value_size\n",
        "            # print('char_embed shape: ', char_embed.shape)\n",
        "            # print('values shape: ', values[:,i,:].shape)\n",
        "            if i < values.shape[1]:\n",
        "              inp = torch.cat([char_embed, values[:,i,:]], dim=1)\n",
        "            else:\n",
        "              pad = torch.zeros((char_embed.shape)).to(DEVICE)\n",
        "              inp = torch.cat([char_embed, pad], dim=1)\n",
        "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
        "\n",
        "            inp_2 = hidden_states[0][0]\n",
        "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
        "\n",
        "            ### Compute attention from the output of the second LSTM Cell ###\n",
        "            output = hidden_states[1][0]\n",
        "\n",
        "            if self.isAttended:\n",
        "              output, attention = self.attention(output, key[:,i,:], values[:,i,:], lens)\n",
        "\n",
        "            if i < values.shape[1]:\n",
        "              prediction = self.character_prob(torch.cat([output, values[:,i,:]], dim=1))\n",
        "            else:\n",
        "              pad = torch.zeros((output.shape)).to(DEVICE)\n",
        "              inp = torch.cat([output, pad], dim=1)\n",
        "            predictions.append(prediction.unsqueeze(1))\n",
        "\n",
        "        return torch.cat(predictions, dim=1)\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=False):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim)\n",
        "\n",
        "    def forward(self, speech_input, speech_len, text_input, isTrain=True):\n",
        "        key, value = self.encoder(speech_input, speech_len)\n",
        "        if (isTrain == True):\n",
        "            # print('text_input: ', text_input)\n",
        "            predictions = self.decoder(key, value, text_input)\n",
        "        else:\n",
        "            predictions = self.decoder(key, value, text=None, isTrain=False)\n",
        "        return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpGQ0Dd2aVP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWgt16KJ7HCh",
        "colab_type": "text"
      },
      "source": [
        "## Train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnfXhDTq7KD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    model.to(DEVICE)\n",
        "    st = time.time()\n",
        "\n",
        "    # 1) Iterate through your loader\n",
        "\n",
        "        # 2) Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion\n",
        "        \n",
        "            # 3) Set the inputs to the device.\n",
        "\n",
        "            # 4) Pass your inputs, and length of speech into the model.\n",
        "\n",
        "            # 5) Generate a mask based on the lengths of the text to create a masked loss. \n",
        "            # 5.1) Ensure the mask is on the device and is the correct shape.\n",
        "\n",
        "            # 6) If necessary, reshape your predictions and origianl text input \n",
        "            # 6.1) Use .contiguous() if you need to. \n",
        "\n",
        "            # 7) Use the criterion to get the loss.\n",
        "\n",
        "            # 8) Use the mask to calculate a masked loss. \n",
        "\n",
        "            # 9) Run the backward pass on the masked loss. \n",
        "\n",
        "            # 10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "            \n",
        "            # 11) Take a step with your optimizer\n",
        "\n",
        "            # 12) Normalize the masked loss\n",
        "\n",
        "            # 13) Optionally print the training loss after every N batches\n",
        "    batch_index = 1\n",
        "    for features, features_lens, targets, target_lens in train_loader:\n",
        "      st = time.time()\n",
        "      features, features_lens, targets, target_lens = features.to(DEVICE), features_lens.to(DEVICE), targets.to(DEVICE), target_lens.to(DEVICE)\n",
        "      \n",
        "      predictions = model(speech_input = features, speech_len = features_lens, text_input = targets, isTrain=True)\n",
        "\n",
        "      print('features shape: ', features.shape)\n",
        "      print('predictions shape: ', predictions.shape)\n",
        "      print('prediction: ', predictions)\n",
        "      print('targets: ', targets.shape)\n",
        "      print('targets_len: ', target_lens)\n",
        "\n",
        "      for i in range(len(target_lens)):\n",
        "        print('target_lens[i]: ', target_lens[i])\n",
        "        predictions[:, i, target_lens[i]:] = 0\n",
        "\n",
        "      _, predictions = torch.max(predictions, dim = 2)\n",
        "      predictions = predictions.float()\n",
        "      print('predicted answer: ', predictions.shape)\n",
        "      print('predicted answer: ', predictions)\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(predictions,  targets)\n",
        "      loss.backward() \n",
        "      optimizer.step()\n",
        "      avg_loss += loss.item()\n",
        "      if batch_index % 50 == 0:\n",
        "          print(\"Time: \", time.time() - st)\n",
        "          print(\"batch index: \",batch_index,' Loss: ', avg_loss / 50)\n",
        "          avg_loss = 0\n",
        "          st = time.time()\n",
        "      batch_index += 1\n",
        "\n",
        "      del features\n",
        "      del features_lens\n",
        "      del targets\n",
        "      del target_lens\n",
        "      del loss\n",
        "\n",
        "def test(model, test_loader, epoch):\n",
        "    ### Write your test code here! ###\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8xoXte07ewN",
        "colab_type": "text"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHUVyflb7L7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22e5d8e4-6010-4523-973a-8919a262f1c8"
      },
      "source": [
        "model = Seq2Seq(input_dim=40, vocab_size=len(LETTER_LIST), hidden_dim=128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(reduction=None)\n",
        "nepochs = 25\n",
        "\n",
        "for epoch in range(nepochs):\n",
        "  train(model, train_loader, criterion, optimizer, epoch)\n",
        "  # val()\n",
        "  # test(model, test_loader, epoch)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "key:  torch.Size([64, 128, 128])\n",
            "value:  torch.Size([64, 128, 128])\n",
            "text:  torch.Size([64, 191])\n",
            "features shape:  torch.Size([64, 1018, 40])\n",
            "predictions shape:  torch.Size([64, 191, 35])\n",
            "prediction:  tensor([[[ 0.0159,  0.0031,  0.0410,  ...,  0.0112,  0.0001,  0.0411],\n",
            "         [ 0.0283,  0.0083,  0.0462,  ...,  0.0218, -0.0032,  0.0445],\n",
            "         [ 0.0307,  0.0038,  0.0467,  ...,  0.0170,  0.0022,  0.0510],\n",
            "         ...,\n",
            "         [-0.0061,  0.0074,  0.0379,  ...,  0.0057,  0.0041,  0.0407],\n",
            "         [-0.0061,  0.0074,  0.0379,  ...,  0.0057,  0.0041,  0.0407],\n",
            "         [-0.0061,  0.0074,  0.0379,  ...,  0.0057,  0.0041,  0.0407]],\n",
            "\n",
            "        [[ 0.0191,  0.0085,  0.0404,  ..., -0.0004,  0.0185,  0.0435],\n",
            "         [ 0.0236,  0.0177,  0.0334,  ...,  0.0013,  0.0214,  0.0381],\n",
            "         [ 0.0178,  0.0191,  0.0207,  ..., -0.0094,  0.0173,  0.0306],\n",
            "         ...,\n",
            "         [-0.0062,  0.0071,  0.0378,  ...,  0.0058,  0.0037,  0.0403],\n",
            "         [-0.0062,  0.0071,  0.0378,  ...,  0.0058,  0.0037,  0.0403],\n",
            "         [-0.0062,  0.0071,  0.0378,  ...,  0.0058,  0.0037,  0.0403]],\n",
            "\n",
            "        [[ 0.0135,  0.0057,  0.0454,  ...,  0.0105, -0.0025,  0.0399],\n",
            "         [ 0.0060, -0.0031,  0.0565,  ...,  0.0176, -0.0058,  0.0414],\n",
            "         [ 0.0127,  0.0021,  0.0495,  ...,  0.0038, -0.0014,  0.0355],\n",
            "         ...,\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402],\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402],\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0076,  0.0129,  0.0495,  ...,  0.0056, -0.0013,  0.0397],\n",
            "         [ 0.0036,  0.0145,  0.0521,  ...,  0.0023,  0.0067,  0.0343],\n",
            "         [ 0.0026,  0.0133,  0.0639,  ...,  0.0020, -0.0037,  0.0299],\n",
            "         ...,\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402],\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402],\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402]],\n",
            "\n",
            "        [[ 0.0071,  0.0160,  0.0483,  ...,  0.0028, -0.0004,  0.0412],\n",
            "         [ 0.0025,  0.0250,  0.0451,  ..., -0.0020,  0.0031,  0.0447],\n",
            "         [-0.0015,  0.0250,  0.0363,  ..., -0.0151, -0.0071,  0.0360],\n",
            "         ...,\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402],\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402],\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402]],\n",
            "\n",
            "        [[ 0.0095,  0.0130,  0.0457,  ...,  0.0015, -0.0018,  0.0421],\n",
            "         [ 0.0133,  0.0079,  0.0462,  ...,  0.0029, -0.0158,  0.0450],\n",
            "         [ 0.0068,  0.0080,  0.0471,  ..., -0.0016, -0.0062,  0.0408],\n",
            "         ...,\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402],\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402],\n",
            "         [-0.0064,  0.0071,  0.0378,  ...,  0.0059,  0.0037,  0.0402]]],\n",
            "       device='cuda:0', grad_fn=<CatBackward>)\n",
            "targets:  torch.Size([64, 191])\n",
            "targets_len:  tensor([120, 116,  78,  58,  75,  55,  98,  49,  56,  79, 130, 129,  84,  67,\n",
            "        122,  69,  78, 158,  97,  59, 138,  79,  97, 104, 167,  81,  77, 191,\n",
            "        119, 144,  59, 148,  76,  61, 145, 141,  63, 119,  80,  90, 183,  80,\n",
            "         75,  91, 110,  77, 118,  82,  95, 140,  96, 165,  72, 154,  62,  77,\n",
            "         98,  84, 140, 108, 110,  84,  48,  83], device='cuda:0')\n",
            "target_lens[i]:  tensor(120, device='cuda:0')\n",
            "target_lens[i]:  tensor(116, device='cuda:0')\n",
            "target_lens[i]:  tensor(78, device='cuda:0')\n",
            "target_lens[i]:  tensor(58, device='cuda:0')\n",
            "target_lens[i]:  tensor(75, device='cuda:0')\n",
            "target_lens[i]:  tensor(55, device='cuda:0')\n",
            "target_lens[i]:  tensor(98, device='cuda:0')\n",
            "target_lens[i]:  tensor(49, device='cuda:0')\n",
            "target_lens[i]:  tensor(56, device='cuda:0')\n",
            "target_lens[i]:  tensor(79, device='cuda:0')\n",
            "target_lens[i]:  tensor(130, device='cuda:0')\n",
            "target_lens[i]:  tensor(129, device='cuda:0')\n",
            "target_lens[i]:  tensor(84, device='cuda:0')\n",
            "target_lens[i]:  tensor(67, device='cuda:0')\n",
            "target_lens[i]:  tensor(122, device='cuda:0')\n",
            "target_lens[i]:  tensor(69, device='cuda:0')\n",
            "target_lens[i]:  tensor(78, device='cuda:0')\n",
            "target_lens[i]:  tensor(158, device='cuda:0')\n",
            "target_lens[i]:  tensor(97, device='cuda:0')\n",
            "target_lens[i]:  tensor(59, device='cuda:0')\n",
            "target_lens[i]:  tensor(138, device='cuda:0')\n",
            "target_lens[i]:  tensor(79, device='cuda:0')\n",
            "target_lens[i]:  tensor(97, device='cuda:0')\n",
            "target_lens[i]:  tensor(104, device='cuda:0')\n",
            "target_lens[i]:  tensor(167, device='cuda:0')\n",
            "target_lens[i]:  tensor(81, device='cuda:0')\n",
            "target_lens[i]:  tensor(77, device='cuda:0')\n",
            "target_lens[i]:  tensor(191, device='cuda:0')\n",
            "target_lens[i]:  tensor(119, device='cuda:0')\n",
            "target_lens[i]:  tensor(144, device='cuda:0')\n",
            "target_lens[i]:  tensor(59, device='cuda:0')\n",
            "target_lens[i]:  tensor(148, device='cuda:0')\n",
            "target_lens[i]:  tensor(76, device='cuda:0')\n",
            "target_lens[i]:  tensor(61, device='cuda:0')\n",
            "target_lens[i]:  tensor(145, device='cuda:0')\n",
            "target_lens[i]:  tensor(141, device='cuda:0')\n",
            "target_lens[i]:  tensor(63, device='cuda:0')\n",
            "target_lens[i]:  tensor(119, device='cuda:0')\n",
            "target_lens[i]:  tensor(80, device='cuda:0')\n",
            "target_lens[i]:  tensor(90, device='cuda:0')\n",
            "target_lens[i]:  tensor(183, device='cuda:0')\n",
            "target_lens[i]:  tensor(80, device='cuda:0')\n",
            "target_lens[i]:  tensor(75, device='cuda:0')\n",
            "target_lens[i]:  tensor(91, device='cuda:0')\n",
            "target_lens[i]:  tensor(110, device='cuda:0')\n",
            "target_lens[i]:  tensor(77, device='cuda:0')\n",
            "target_lens[i]:  tensor(118, device='cuda:0')\n",
            "target_lens[i]:  tensor(82, device='cuda:0')\n",
            "target_lens[i]:  tensor(95, device='cuda:0')\n",
            "target_lens[i]:  tensor(140, device='cuda:0')\n",
            "target_lens[i]:  tensor(96, device='cuda:0')\n",
            "target_lens[i]:  tensor(165, device='cuda:0')\n",
            "target_lens[i]:  tensor(72, device='cuda:0')\n",
            "target_lens[i]:  tensor(154, device='cuda:0')\n",
            "target_lens[i]:  tensor(62, device='cuda:0')\n",
            "target_lens[i]:  tensor(77, device='cuda:0')\n",
            "target_lens[i]:  tensor(98, device='cuda:0')\n",
            "target_lens[i]:  tensor(84, device='cuda:0')\n",
            "target_lens[i]:  tensor(140, device='cuda:0')\n",
            "target_lens[i]:  tensor(108, device='cuda:0')\n",
            "target_lens[i]:  tensor(110, device='cuda:0')\n",
            "target_lens[i]:  tensor(84, device='cuda:0')\n",
            "target_lens[i]:  tensor(48, device='cuda:0')\n",
            "target_lens[i]:  tensor(83, device='cuda:0')\n",
            "predicted answer:  torch.Size([64, 191])\n",
            "predicted answer:  tensor([[24.,  9., 24.,  ..., 24., 24., 24.],\n",
            "        [24.,  9.,  9.,  ..., 24., 24., 24.],\n",
            "        [24.,  9.,  9.,  ..., 24., 24., 24.],\n",
            "        ...,\n",
            "        [ 9.,  9.,  9.,  ..., 24., 24., 24.],\n",
            "        [ 9.,  9.,  9.,  ..., 24., 24., 24.],\n",
            "        [ 9., 24.,  9.,  ..., 24., 24., 24.]], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-dfb5319389a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;31m# val()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# test(model, test_loader, epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-9bc47d3431d2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predicted answer: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 932\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mget_enum\u001b[0;34m(reduction)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# TODO: remove once JIT exceptions support control flow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} is not a valid value for reduction\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: None is not a valid value for reduction"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WRPBe1g6TFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}